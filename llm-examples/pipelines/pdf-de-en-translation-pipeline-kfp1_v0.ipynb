{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df4f1a2-5690-41c6-a061-274545e459a3",
   "metadata": {},
   "source": [
    "# About this Jupyter Notebook\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@created: 15.11.2023\\\n",
    "@updated: 31.01.2024\n",
    "\n",
    "**This is the first pipeline prototype to run for the german medical report to english translation for LLM preprocessing.**\n",
    "\n",
    "This notebook defines and runs a kubeflow pipeline with KFP python SDK v1 for BART based de_en translatition models to translate the content of pdf file from german to english.\n",
    "\n",
    "Notice:\n",
    "```\n",
    "urllib3 1.26.16 works with google auth, see any http google auth error, reinstall kfp=1.8.22\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357cf343-6af6-4ba3-b48d-5985633a45e2",
   "metadata": {},
   "source": [
    "## Install KFP Python SDK to build a V1 pipeline\n",
    "* Build KF pipeline with python SDK: https://www.kubeflow.org/docs/components/pipelines/sdk/build-pipeline/\n",
    "* Current KFP python SDK version on pypi.org: https://pypi.org/project/kfp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3e4c76-43fd-460d-9457-20687bc552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792bac4d-9de6-4b2d-975b-ca59b5ecfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip uninstall -y kfp-server-api\n",
    "#!{sys.executable} -m pip install --user --upgrade kfp-server-api==1.8.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e083866-51d7-400f-9cfd-1790675a6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade --user kfp==2.0.0b13\n",
    "#!{sys.executable} -m pip install --upgrade --user kfp==1.8.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acbec4-7f31-4cbf-bc8a-b852271b4884",
   "metadata": {},
   "source": [
    "## Restart the Kernel\n",
    "\n",
    "After the installation of KFP python SDK, the notebook kernel must be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f3811-f3c1-45f2-8476-719a937caadd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting familiar with Jupyter Notebook ENV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5c31cc-0869-48eb-9587-89df6aff7aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current platform python version: 3.8.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print (f\"current platform python version: {python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c90732-087d-46de-9a92-c6eb85e8a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                                                         kf-resource-quota\n",
      "Namespace:                                                    kubeflow-kindfor\n",
      "Resource                                                      Used     Hard\n",
      "--------                                                      ----     ----\n",
      "basic-csi.storageclass.storage.k8s.io/persistentvolumeclaims  4        0\n",
      "basic-csi.storageclass.storage.k8s.io/requests.storage        115Gi    150Gi\n",
      "cpu                                                           2090m    128\n",
      "homedir.storageclass.storage.k8s.io/persistentvolumeclaims    3        15\n",
      "homedir.storageclass.storage.k8s.io/requests.storage          50Gi     150Gi\n",
      "longhorn.storageclass.storage.k8s.io/persistentvolumeclaims   1        15\n",
      "longhorn.storageclass.storage.k8s.io/requests.storage         450Gi    700Gi\n",
      "memory                                                        24966Mi  512Gi\n",
      "requests.nvidia.com/mig-1g.10gb                               0        1\n",
      "requests.nvidia.com/mig-1g.20gb                               0        0\n",
      "requests.nvidia.com/mig-2g.20gb                               0        1\n",
      "requests.nvidia.com/mig-3g.40gb                               1        1\n"
     ]
    }
   ],
   "source": [
    "# run kubectl command line to see the quota in the name space\n",
    "!kubectl describe quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c70007-489a-49d2-baff-eaeaa7a984a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp                       1.8.22\n",
      "kfp-pipeline-spec         0.1.16\n",
      "kfp-server-api            1.8.5\n"
     ]
    }
   ],
   "source": [
    "# examing the kfp python sdk version inside a KubeFlow v1.5.1\n",
    "!{sys.executable} -m pip list | grep kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c986797-6418-4f13-a439-7dfe2e6c09a3",
   "metadata": {},
   "source": [
    "## Setup global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8bcd262-cbbb-4914-90f0-9c6dc85d5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubeflow-kindfor\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "client = kfp.Client()\n",
    "\n",
    "NAMESPACE = client.get_user_namespace()\n",
    "\n",
    "EXPERIMENT_NAME = 'scivias' # Name of the experiment in the KF webapp UI\n",
    "EXPERIMENT_DESC = 'translate germany medical report pdf to english txt file'\n",
    "\n",
    "PREFIX = \"llm\" # for saving the component files\n",
    "\n",
    "MODEL_ROOT = \"/mnt\"\n",
    "MODEL_SUB_PATH = \"core-kind/yinwang\"\n",
    "\n",
    "# FILE_SUB_PATH = f\"{DATA_SUB_PATH}/data/medreports\"\n",
    "# FILE_PATTERN = \"KK-SCIVIAS-*.pdf\"\n",
    "FILE_PREFIX = \"KK-SCIVIAS\"\n",
    "\n",
    "DEFAULT_GEN_MODEL_TYPE = \"7B\"\n",
    "DEFAULT_TRANS_MODEL_TYPE = \"custom\"\n",
    "\n",
    "print(NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f8e8f5-e74d-4128-b6d1-5d0b371a455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings(base_torch_image='harbor-dmz.srv.med.uni-muenchen.de/core-general/ngc:0.0.0', base_torch_simple_image='pytorch/pytorch:2.1.1-cuda12.1-cudnn8-runtime', pandas='pandas==1.5.3', pypdf='pypdf==3.15.5', pyarrow='pyarrow==10.0.0', boto3='boto3==1.29.0')\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "'''\n",
    "cudf 23.2.0 requires pandas<1.6.0dev0,>=1.0, but you have pandas 2.0.3 which is incompatible.\n",
    "dask-cudf 23.2.0 requires pandas<1.6.0dev0,>=1.0, but you have pandas 2.0.3 which is incompatible.\n",
    "'''\n",
    "@dataclass\n",
    "class Settings:\n",
    "    base_torch_image: str = \"harbor-dmz.srv.med.uni-muenchen.de/core-general/ngc:0.0.0\"\n",
    "    base_torch_simple_image: str = \"pytorch/pytorch:2.1.1-cuda12.1-cudnn8-runtime\"\n",
    "    pandas: str = \"pandas==1.5.3\" # < 2.0.3 by cudf and dash-cudf\n",
    "    pypdf: str = \"pypdf==3.15.5\"\n",
    "    pyarrow: str = \"pyarrow==10.0.0\"\n",
    "    boto3: str = \"boto3==1.29.0\"\n",
    "\n",
    "    \n",
    "settings = Settings() \n",
    "print(f\"{settings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b18f24-d264-4352-aaf0-e1ee8adaadfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating KubeFlow component from python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3226c3-b0d9-4c2b-9f48-2927f6433aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import kfp dsl components\n",
    "import kfp.dsl as dsl\n",
    "from functools import partial\n",
    "from kfp.dsl import (\n",
    "    pipeline,\n",
    "    ContainerOp,\n",
    "    PipelineVolume\n",
    ")\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    InputBinaryFile, \n",
    "    OutputBinaryFile,\n",
    "    create_component_from_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a97a7e-1064-432e-b465-d14f560f540b",
   "metadata": {},
   "source": [
    "#### PDF text_translate component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00c4e709-83f6-41e2-bc78-28d6ed129e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}_s3_pdf_translate_component.yaml\",\n",
    "    # base_image=\"python:3.8.18-bullseye\", # settings.base_torch_image, # use pt base image\n",
    "    base_image=settings.base_torch_simple_image, # cpu version, small as pytorch https://hub.docker.com/r/pytorch/pytorch/tags?page=1&name=2.0.1\n",
    "    packages_to_install=[\n",
    "        \"transformers==4.31.0\",\n",
    "        \"sacremoses==0.0.53\",\n",
    "        \"sentencepiece==0.1.99\",\n",
    "        settings.pypdf,\n",
    "        settings.boto3,\n",
    "        # settings.pandas,\n",
    "        # settings.pyarrow,\n",
    "    ],# adding additional libs\n",
    ")\n",
    "def en_translator(\n",
    "    bucket_name: str,\n",
    "    file_prefix: str,\n",
    "    item_max_cap: int,\n",
    "    model_root: str, \n",
    "    model_sub_path: str, \n",
    "    model_type: str,\n",
    "    show_log_txt: bool,\n",
    "    s3_verify_host: bool,\n",
    "    # output_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      item_max_cap: define the max number of file will be processed from the bucket\n",
    "    \"\"\"\n",
    "    import sys, os, boto3\n",
    "    # import pandas as pd\n",
    "    # from pandas import DataFrame, Series\n",
    "    from dataclasses import dataclass\n",
    "    from io import BytesIO\n",
    "    from operator import attrgetter\n",
    "    from pypdf import PdfReader    \n",
    "    import torch, subprocess, re, time\n",
    "    \n",
    "    \n",
    "    class AcceleratorHelper():\n",
    "        @staticmethod\n",
    "        def print_container_info() -> None:\n",
    "            print(\"-\"*10)\n",
    "            print(time.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "            print(f\"python version: {sys.version}\")\n",
    "            print(f\"torch version: {torch.__version__}\")\n",
    "            print(\"-\"*10)\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def nvidia_device_info() -> str:\n",
    "            \"\"\"get the nvidia MIGs device uuid and GPU uuid \n",
    "            \"\"\"\n",
    "            # blocking call\n",
    "            result = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE)\n",
    "            # decode the byte object, returns string with \\n\n",
    "            cmd_out_str = result.stdout.decode('utf-8')\n",
    "            return [line.strip() for line in cmd_out_str.split('\\n') if len(line) > 0]\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def extract_nvidia_device_uuids(input: str):\n",
    "            \"\"\"parse the nvidia devices uuid from the nvidia device info str\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # r'' before the search pattern indicates it is a raw string, \n",
    "                # otherwise \"\" instead of single quote\n",
    "                uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "            except AttributeError:\n",
    "                # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "                uuid = \"\"\n",
    "            return uuid\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def nvidia_device_uuids_filtered_by(is_mig: bool = False, log_output: bool = False) -> str:\n",
    "            \"\"\"get a comma separated str of nvidia MIGs devices\n",
    "            \"\"\"\n",
    "            info_list = AcceleratorHelper.nvidia_device_info()\n",
    "            if is_mig:\n",
    "                # skip the first GPU ID, get the MIGs IDS\n",
    "                uuid_list = [AcceleratorHelper.extract_nvidia_device_uuids(e) for e in info_list[1:]]\n",
    "            else: # all GPU devices\n",
    "                uuid_list = [AcceleratorHelper.extract_nvidia_device_uuids(e) for e in info_list]\n",
    "            if log_output is not None and log_output:\n",
    "                print(uuid_list)\n",
    "\n",
    "            # if multi gpus need to join the device together for pytorch\n",
    "            return \",\".join(uuid_list)\n",
    "\n",
    "\n",
    "        @staticmethod\n",
    "        def init_cuda_torch(uuids: str, data_path: str) -> None:\n",
    "            \"\"\"setup the default env variables for transformers\n",
    "\n",
    "            Args:\n",
    "              uuids: a comma separate str of nvidia gpu/mig uuids\n",
    "            \"\"\"\n",
    "            os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = uuids \n",
    "            os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "            os.environ['XDG_CACHE_HOME']=f\"{data_path}/models\"\n",
    "\n",
    "\n",
    "    class AcceleratorStatus():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Reference: https://stackoverflow.com/questions/58216000/get-total-amount-of-free-gpu-memory-and-available-using-pytorch\n",
    "        # from typing import Tuple\n",
    "        def byte_gb_info(self, byte_mem) -> str:\n",
    "            \"\"\"calculate the byte size to GB size for better human readable\"\"\"\n",
    "            # format the f string float with :.2f to decimal digits\n",
    "            # https://zetcode.com/python/fstring/\n",
    "            return f\"{(byte_mem/1024**3):4f} GB\"\n",
    "\n",
    "\n",
    "        def accelerator_mem_info(self, device_idx: int):\n",
    "            # total\n",
    "            t = torch.cuda.get_device_properties(device_idx).total_memory\n",
    "            # usable\n",
    "            r = torch.cuda.memory_reserved(device_idx)\n",
    "            # allocated\n",
    "            a = torch.cuda.memory_allocated(device_idx)\n",
    "            # still free\n",
    "            f = r-a\n",
    "            # unit = \"GB\"   \n",
    "            print( # \"GPU memory info:\\n\" + \n",
    "                  f\"Physical  memory : {self.byte_gb_info(t)}\\n\" + \n",
    "                  f\"Reserved  memory : {self.byte_gb_info(r)}\\n\" + \n",
    "                  f\"Allocated memory : {self.byte_gb_info(a)}\\n\" + \n",
    "                  f\"Free      memory : {self.byte_gb_info(f)}\")\n",
    "\n",
    "\n",
    "        def accelerator_compute_info(self, device_idx: int) -> None:\n",
    "            name = torch.cuda.get_device_properties(device_idx).name\n",
    "            count = torch.cuda.get_device_properties(device_idx).multi_processor_count\n",
    "            print(f\"Device name      : {name} \\n\" +\n",
    "                  f\"Device idx       : {device_idx} \\n\" +\n",
    "                  f\"No. of processors: {count}\")    \n",
    "\n",
    "\n",
    "        def gpu_usage(self) -> None:        \n",
    "            num_of_gpus = torch.cuda.device_count();\n",
    "            # this shows only the gpu device, not the MIG\n",
    "            print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "            for device_idx in range(torch.cuda.device_count()):\n",
    "                print(\"-\"*20)\n",
    "                self.accelerator_compute_info(device_idx)                 \n",
    "                self.accelerator_mem_info(device_idx)\n",
    "                print(\"-\"*20)\n",
    "    \n",
    "    \n",
    "    @dataclass\n",
    "    class S3AccessConf():\n",
    "        \"\"\"\n",
    "        Examples:\n",
    "            s3_conf = S3AccessConf(\n",
    "                bucket_name = \"xxx\",\n",
    "                access_key_id = os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "                secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "                endpoint = os.environ.get('S3_ENDPOINT'),\n",
    "                verify_host = True\n",
    "            )    \n",
    "        \"\"\"\n",
    "        access_key_id: str\n",
    "        secret_access_key: str\n",
    "        endpoint: str\n",
    "        bucket_name: str\n",
    "        verify_host: bool = True\n",
    "    \n",
    "    \n",
    "    class S3BucketHelper():\n",
    "        def __init__(self, conf: S3AccessConf, file_prefix: str=\"\"):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                file_prefix: s3 bucket objects key prefix used to filter the object\n",
    "\n",
    "            Examples:\n",
    "                S3BucketHelper(S3BucketConfig())\n",
    "            \"\"\"\n",
    "            self.conf = conf      \n",
    "            # session is the resource which can be passed around\n",
    "            # https://github.com/boto/boto3/issues/3197#issuecomment-1180516179\n",
    "            self.session = boto3.session.Session(\n",
    "                aws_access_key_id = self.conf.access_key_id,\n",
    "                aws_secret_access_key = self.conf.secret_access_key\n",
    "            )\n",
    "            self.file_prefix = file_prefix\n",
    "\n",
    "\n",
    "        def _get_s3_resource(self):\n",
    "            # resource is a s3 endpoint \n",
    "            s3 = self.session.resource('s3', endpoint_url = self.conf.endpoint, verify=self.conf.verify_host)\n",
    "            return s3\n",
    "\n",
    "\n",
    "        def get_object_keys(self, limit_count: int=-1, input_attr_key = \"key\") -> map:\n",
    "            \"\"\"\n",
    "            Return:\n",
    "                map generator object\n",
    "            \"\"\"\n",
    "            s3 = self._get_s3_resource()\n",
    "            bucket = s3.Bucket(self.conf.bucket_name)\n",
    "            if limit_count < 0:\n",
    "                # use attrgetter(input_attr_key) to filter object streams and transform the object steam to a stream of the obj.input_attr_key\n",
    "                bucket_items_map = map(attrgetter(input_attr_key), \n",
    "                        bucket.objects.filter(Prefix=self.file_prefix)\n",
    "                       )\n",
    "            else:\n",
    "                bucket_items_map = map(attrgetter(input_attr_key), \n",
    "                        bucket.objects.filter(Prefix=self.file_prefix).limit(limit_count)\n",
    "                       )\n",
    "            return bucket_items_map\n",
    "\n",
    "\n",
    "        def transform_objects(self, s3_keys: map, bytesio_transformer: callable, s3_body_key = \"Body\", output_id_key: str = \"name\", output_content_key: str = \"bytesio\") -> map:\n",
    "            \"\"\"\n",
    "            transforms the selected the s3 object with a given transformer\n",
    "            \"\"\"\n",
    "            # callable https://realpython.com/python-callable-instances/\n",
    "            s3 = self._get_s3_resource()\n",
    "\n",
    "            bytesio_map = map(lambda x: {\n",
    "                output_id_key : x,\n",
    "                output_content_key : BytesIO(s3.Object(self.conf.bucket_name, x).get()[s3_body_key].read())\n",
    "            }, s3_keys)\n",
    "\n",
    "            return map(bytesio_transformer, bytesio_map)\n",
    "\n",
    "\n",
    "        def upload_objects(self, data: map, key_mutater: callable = lambda x:x, input_id_key:str = \"name\", input_body_key: str = \"content\") -> map:\n",
    "            s3 = self._get_s3_resource()\n",
    "            bucket = s3.Bucket(self.conf.bucket_name)\n",
    "            return map(lambda x:\n",
    "                bucket.put_object(Key=key_mutater(x.get(input_id_key)), Body=x.get(input_body_key))\n",
    "                ,data)\n",
    "\n",
    "\n",
    "    class S3PdfObjHelper(S3BucketHelper):\n",
    "            \"\"\"helper class for S3 PDF object specificly\n",
    "            \"\"\"\n",
    "            class DataContract:\n",
    "                name = \"name\"\n",
    "                # reader = \"reader\"\n",
    "                content = \"content\"\n",
    "                pages = \"pages\"\n",
    "                max_token_per_seg = 350\n",
    "                bytesio = \"bytesio\"\n",
    "                key_lead = \"trans2en\"\n",
    "                key_origin_pattern = \"pdf\"\n",
    "                key_new_pattern = \"txt\"\n",
    "\n",
    "\n",
    "            def __init__(self, conf: S3AccessConf, file_prefix: str=\"\"):\n",
    "                super().__init__(conf, file_prefix)\n",
    "\n",
    "\n",
    "            @classmethod\n",
    "            def pdf_reader_transformer(clz, input_dict: dict) -> dict:\n",
    "                \"\"\"\n",
    "                transforms a byte stream data sequence to a PdfReader stream sequence\n",
    "                \"\"\"\n",
    "                return {\n",
    "                    clz.DataContract.name : input_dict.get(clz.DataContract.name),\n",
    "                    clz.DataContract.content : PdfReader(input_dict.get(clz.DataContract.bytesio))\n",
    "                }\n",
    "\n",
    "\n",
    "            @classmethod\n",
    "            def read_pages_transformer(clz, input_dict: dict) -> dict:\n",
    "                \"\"\"\n",
    "                transforms a RdfReader stream sequence to a plain text stream sequence\n",
    "                \"\"\"\n",
    "                return {\n",
    "                    clz.DataContract.name: input_dict.get(clz.DataContract.name),\n",
    "                    clz.DataContract.content: \"\".join([page.extract_text() for page in input_dict.get(clz.DataContract.content).pages])\n",
    "                }\n",
    "\n",
    "\n",
    "            @classmethod\n",
    "            def segment_pages_transformer(clz, input_dict: dict) -> dict:\n",
    "                \"\"\"\n",
    "                transforms a plain text stream sequence to \n",
    "                a segmented plain text stream sequence.\n",
    "                \"\"\"\n",
    "                #  https://stackoverflow.com/questions/13673060/split-string-into-strings-by-length\n",
    "                s = input_dict.get(clz.DataContract.content) # input string\n",
    "                w = clz.DataContract.max_token_per_seg            \n",
    "                return {\n",
    "                    clz.DataContract.name: input_dict.get(clz.DataContract.name),\n",
    "                    clz.DataContract.content: [s[i:i + w] for i in range(0, len(s), w)]\n",
    "                }\n",
    "\n",
    "\n",
    "            @classmethod\n",
    "            def custom_pages_transformer_factory(clz, segment_transformer: callable) -> dict:\n",
    "                \"\"\"\n",
    "                a factory function \n",
    "                Args:\n",
    "                    segment_transformer: a customer transformer function\n",
    "\n",
    "                Returns:\n",
    "                    a transformer function to tranformer a stream of segmented pages\n",
    "                    using the input pages_transformer\n",
    "                \"\"\"\n",
    "                def inner_func(input_dict: dict):\n",
    "                    segment_output = []\n",
    "                    # loop throught the page segments\n",
    "                    for segment in input_dict.get(clz.DataContract.content):\n",
    "                        # apply the custom segment_transformer\n",
    "                        segment_output.append(segment_transformer(segment))\n",
    "                    return {\n",
    "                        clz.DataContract.name: input_dict.get(clz.DataContract.name),\n",
    "                        clz.DataContract.content: ''.join(segment_output)\n",
    "                    }\n",
    "                return inner_func\n",
    "\n",
    "\n",
    "            @classmethod\n",
    "            def s3_key_mutater(clz, old_key: str) -> str:\n",
    "                return f\"{clz.DataContract.key_lead}/{old_key.replace(clz.DataContract.key_origin_pattern, clz.DataContract.key_new_pattern)}\"\n",
    "\n",
    "    \n",
    "    '''helper function'''\n",
    "    # def get_item_batch(items: list, batch_max) -> list:\n",
    "    #     n = len(items)\n",
    "    #     if 1 <= batch_max < n:\n",
    "    #         return items[:batch_max]\n",
    "    #     elif batch_max >= n or batch_max == 0:\n",
    "    #         return items\n",
    "    #     else:\n",
    "    #         return []\n",
    "    \n",
    "    def save_txt_to_feather(txt_list: list, path: str, col_names=[\"text\"]) -> None:\n",
    "        with open(path, \"w\") as file:\n",
    "            ser = Series(txt_list)\n",
    "            df = DataFrame(data=Series(txt_list), columns=col_names)\n",
    "            df.to_feather(path)\n",
    "            \n",
    "            \n",
    "    def log(obj, debug: bool) -> None:\n",
    "        if debug:\n",
    "            print(\"-\"*20)\n",
    "            if isinstance(obj, list):\n",
    "                print(f\"Printing {len(obj)} list elements...\")\n",
    "                for e in obj:\n",
    "                    print(e)\n",
    "            else:\n",
    "                print(\"Printing content...\")\n",
    "                print(obj)        \n",
    "        \n",
    "        \n",
    "    '''Global variable'''\n",
    "    model_map = {\n",
    "       #\"small\": \"google/mt5-small\", # 1.2 GB\n",
    "       #\"base\" : \"google/mt5-base\", # 2.33 GB\n",
    "       #\"large\" : \"google/mt5-large\", # 4.9 GB,\n",
    "       #\"xl\" : \"google/mt5-xl\", # 15 GB\n",
    "       #\"xxl\" : \"google/mt5-xxl\", # 51.7 GB,\n",
    "       \"custom\": \"Helsinki-NLP/opus-mt-de-en\", \n",
    "    }\n",
    "    accelerator_id = 0\n",
    "    model_path = f\"{model_root}/{model_sub_path}\"\n",
    "    log(model_path, debug=show_log_txt)\n",
    "    model_name = model_map.get(model_type, \"custom\")\n",
    "    \n",
    "    \"\"\"Loading BART Translator\"\"\"\n",
    "    # print gpu_usage\n",
    "    gpu_status = AcceleratorStatus()\n",
    "    gpu_status.gpu_usage()\n",
    "    gpu_helper = AcceleratorHelper()\n",
    "    UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)\n",
    "    \n",
    "    gpu_helper.init_cuda_torch(UUIDs, model_path)                         \n",
    "    from transformers import pipeline\n",
    "    import transformers\n",
    "    \n",
    "    generator = pipeline(\n",
    "        \"translation\", \n",
    "        model=model_name,\n",
    "        # device_map=\"auto\",\n",
    "        device=accelerator_id,\n",
    "    )\n",
    "    \n",
    "    def translate_gen(\n",
    "        generator: transformers.pipelines.text2text_generation.TranslationPipeline, \n",
    "        info: AcceleratorStatus,\n",
    "    ):  \n",
    "        \"\"\"\n",
    "        Args:\n",
    "          max_new_tokens: control the maximum length of the generation\n",
    "        \"\"\"\n",
    "\n",
    "        def local(sentences: list, max_length=400, verbose: bool = True) -> list:\n",
    "            \"\"\"single input, no batch input\n",
    "            Args:\n",
    "              sentences:\n",
    "            \"\"\"\n",
    "            start = time.time()\n",
    "\n",
    "            result = generator(\n",
    "                sentences, \n",
    "                max_length=max_length,\n",
    "                # return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            end = time.time()\n",
    "            duration = end - start\n",
    "            if verbose:\n",
    "                print(\"-\"*20)\n",
    "                print(f\"walltime: {duration} in secs.\")\n",
    "                info.gpu_usage()\n",
    "\n",
    "            return result\n",
    "        return local    \n",
    "\n",
    "    translate = translate_gen(generator, gpu_status)\n",
    "    \n",
    "    \"\"\"s3 connection\"\"\"\n",
    "    s3_conf = S3AccessConf(\n",
    "        bucket_name = bucket_name,\n",
    "        access_key_id = os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "        secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "        endpoint = os.environ.get('S3_ENDPOINT'),\n",
    "        verify_host = s3_verify_host\n",
    "    )\n",
    "    bucket_helper = S3PdfObjHelper(conf=s3_conf, file_prefix=file_prefix)\n",
    "   \n",
    "    \"\"\"translate sequence pipeline\"\"\"\n",
    "    # iterable of all s3 obj keys\n",
    "    obj_key_iterable = bucket_helper.get_object_keys(item_max_cap)\n",
    "    # log(list(obj_key_iterable), debug=show_log_txt)\n",
    "    # passing pdf objects from s3 to PdfReader\n",
    "    pdf_reader_iterable = bucket_helper.transform_objects(obj_key_iterable, \n",
    "                                                 bucket_helper.pdf_reader_transformer)\n",
    "    # read pages from PdfReaders\n",
    "    raw_doc_iterable = map(bucket_helper.read_pages_transformer, pdf_reader_iterable)\n",
    "    # split raw doc string as a list of 350 token segments\n",
    "    raw_splitted_segments_iterable = map(bucket_helper.segment_pages_transformer, raw_doc_iterable)\n",
    "    \n",
    "    from functools import partial\n",
    "\n",
    "    @partial(\n",
    "        bucket_helper.custom_pages_transformer_factory,\n",
    "    )\n",
    "    def segment_en_translater(segment: str):\n",
    "        return translate([segment], verbose=False)[0].get('translation_text', '').strip()\n",
    "    \n",
    "    # use the current translater to translate from DE to EN for all document segements and for a sequence of documents\n",
    "    translated_doc_iterable = map(segment_en_translater, raw_splitted_segments_iterable)\n",
    "    \n",
    "    \"\"\"Persist to s3 translated docs\"\"\"\n",
    "    # upload the sequence of translated documents back to s3 storage.\n",
    "    upload_s3_obj_iterable = bucket_helper.upload_objects(translated_doc_iterable, bucket_helper.s3_key_mutater)\n",
    "    \n",
    "    \"\"\"execute the sequence pipeline\"\"\"\n",
    "    # need to use list to trigger the map reactive call for the map generator pipeline\n",
    "    start = time.time()\n",
    "    upload_action_result_list = list(upload_s3_obj_iterable)\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(\"-\"*20)\n",
    "    print(f\"walltime: {duration} in secs.\")\n",
    "\n",
    "    for e in upload_action_result_list:\n",
    "        print(e)\n",
    "    # save to output\n",
    "    # save_txt_to_feather(file_content, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f3b82-2971-4ad2-93fd-edb6e4852fc6",
   "metadata": {},
   "source": [
    "#### translate_bart component\n",
    "\n",
    "While BERT was trained by using a simple token masking technique, BART empowers the BERT encoder by using more challenging kinds of masking mechanisms in its pre-training.\n",
    "\n",
    "* loop with index using enumerate: https://treyhunner.com/2016/04/how-to-loop-with-indexes-in-python/\n",
    "* BART : Generalizing BERT (due to the bidirectional encoder) and GPT2 (with the left to right decoder) : https://www.projectpro.io/article/transformers-bart-model-explained/553\n",
    "* MarianMT on huggingface (BART): https://huggingface.co/docs/transformers/model_doc/marian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70c4081-a5db-4684-8717-67eab3067e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}_trans_component.yaml\",\n",
    "    base_image=\"tensorflow/tensorflow:2.12.0\", # cpu version, small as pytorch https://hub.docker.com/r/pytorch/pytorch/tags?page=1&name=2.0.1\n",
    "    # base_image=\"python:3.8.18\", # settings.base_torch_image, # use pt base image\n",
    "    packages_to_install=[\n",
    "        \"transformers==4.31.0\",\n",
    "        \"sacremoses==0.0.53\",\n",
    "        \"sentencepiece==0.1.99\",\n",
    "        settings.pandas,\n",
    "        settings.pyarrow,\n",
    "        # \"pandas==2.0.3\"\n",
    "        #\"https://download.pytorch.org/whl/cu117/torch==2.0.1+cu117\",\n",
    "    ],# adding additional libs\n",
    ")\n",
    "def bart_translator(\n",
    "    data_root: str, \n",
    "    data_sub_path: str, \n",
    "    model_type: str,\n",
    "    show_log_txt: bool,\n",
    "    # origin_text: str, \n",
    "    input_path: InputPath(\"Dataset\"),\n",
    "    output_path: OutputPath(\"Dataset\")\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      input_path: feather binary encoded text str to be translated from german to english\n",
    "      output_path: feather binary encoded the translated english text\n",
    "    \"\"\"\n",
    "    import subprocess, os, re, sys, time\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame, Series\n",
    "    \n",
    "    class GPUInfoHelper():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "\n",
    "        def byte_gb_info(self, byte_mem) -> str:\n",
    "            \"\"\"calculate the byte size to GB size for better human readable\"\"\"\n",
    "            # format the f string float with :.2f to decimal digits\n",
    "            # https://zetcode.com/python/fstring/\n",
    "            return f\"{(byte_mem/1024**3):4f} GB\"\n",
    "\n",
    "\n",
    "        def accelerator_mem_info(self, device_idx: int):\n",
    "            # total\n",
    "            t = torch.cuda.get_device_properties(device_idx).total_memory\n",
    "            # usable\n",
    "            r = torch.cuda.memory_reserved(device_idx)\n",
    "            # allocated\n",
    "            a = torch.cuda.memory_allocated(device_idx)\n",
    "            # still free\n",
    "            f = r-a  \n",
    "            print( # \"GPU memory info:\\n\" + \n",
    "                  f\"Physical  memory : {self.byte_gb_info(t)}\\n\" + \n",
    "                  f\"Reserved  memory : {self.byte_gb_info(r)}\\n\" + \n",
    "                  f\"Allocated memory : {self.byte_gb_info(a)}\\n\" + \n",
    "                  f\"Free      memory : {self.byte_gb_info(f)}\")\n",
    "\n",
    "\n",
    "        def accelerator_compute_info(self, device_idx: int) -> None:\n",
    "            name = torch.cuda.get_device_properties(device_idx).name\n",
    "            count = torch.cuda.get_device_properties(device_idx).multi_processor_count\n",
    "            print(f\"Device_name      : {name} \\n\" +\n",
    "                  f\"Multi_processor  : {count}\")    \n",
    "\n",
    "\n",
    "        def gpu_usage(self) -> None:        \n",
    "            num_of_gpus = torch.cuda.device_count();\n",
    "            # this shows only the gpu device, not the MIG\n",
    "            print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "            for device_idx in range(torch.cuda.device_count()):\n",
    "                print(\"-\"*20)\n",
    "                self.accelerator_compute_info(device_idx)                 \n",
    "                self.accelerator_mem_info(device_idx)\n",
    "                print(\"-\"*20)\n",
    "     \n",
    "    \n",
    "    # further\n",
    "    def display_container_info():\n",
    "        print(\"-\"*10)\n",
    "        print(f\"python version: {sys.version}\")\n",
    "        # print(f\"torch version: {torch.__version__}\")\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "    \n",
    "    def nvidia_device_uuid(input: str):\n",
    "        \"\"\"parse the nvidia devices uuid from the nvidia device info str\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # r'' before the search pattern indicates it is a raw string, \n",
    "            # otherwise \"\" instead of single quote\n",
    "            uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "        except AttributeError:\n",
    "            # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "            uuid = \"\"\n",
    "        return uuid\n",
    "    \n",
    "    \n",
    "    def nvidia_device_info() -> str:\n",
    "        \"\"\"get the nvidia MIGs device uuid and GPU uuid \n",
    "        \"\"\"\n",
    "        # blocking call\n",
    "        result = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE)\n",
    "        # decode the byte object, returns string with \\n\n",
    "        cmd_out_str = result.stdout.decode('utf-8')\n",
    "        return [line.strip() for line in cmd_out_str.split('\\n') if len(line) > 0]\n",
    "        \n",
    "    \n",
    "    def nvidia_mig_uuids() -> str:\n",
    "        \"\"\"get a comma separated str of nvidia MIGs devices\n",
    "        \"\"\"\n",
    "        info_list = nvidia_device_info()\n",
    "        # skip the first GPU ID, get the MIGs IDS\n",
    "        uuid_list = [nvidia_device_uuid(e) for e in info_list[1:]]\n",
    "        # if multi gpus need to join the device together for pytorch\n",
    "        return \",\".join(uuid_list)\n",
    "    \n",
    "    \n",
    "    def init_cuda_torch(uuids: str, data_path: str) -> None:\n",
    "        \"\"\"setup the default env variables for transformers\n",
    "        \n",
    "        Args:\n",
    "          uuids: a comma separate str of nvidia gpu/mig uuids\n",
    "        \"\"\"\n",
    "        os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = uuids \n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "        init_transformers()\n",
    "        \n",
    "    def init_transformers() -> None:\n",
    "        os.environ['XDG_CACHE_HOME']=f\"{data_path}/models\"\n",
    "        \n",
    "        \n",
    "    def show_folder_files(folder: str) -> None:\n",
    "        print(os.listdir(folder))\n",
    "        \n",
    "    \n",
    "    # https://stackoverflow.com/questions/13673060/split-string-into-strings-by-length\n",
    "    def wrap(s, w):\n",
    "        \"\"\"\n",
    "        split string with length w into a list of strings with length w\n",
    "        Arge:\n",
    "          s: orginial str\n",
    "          w: with of the each split for the string\n",
    "\n",
    "        Return:\n",
    "          a list of string with each element as string of length w\n",
    "        \"\"\"\n",
    "        return [s[i:i + w] for i in range(0, len(s), w)]\n",
    "\n",
    "    \n",
    "    def save_txt_to_feather(txt_list: list, path: str, col_names=[\"text\"]) -> None:\n",
    "        with open(path, \"w\") as file:\n",
    "            ser = Series(txt_list)\n",
    "            df = DataFrame(data=Series(txt_list), columns=col_names)\n",
    "            df.to_feather(path)\n",
    "    \n",
    "    \n",
    "    def load_feather(path: str, col_names=[\"text\"]) -> DataFrame:\n",
    "        with open(path, \"r\") as file:\n",
    "            df = pd.read_feather(path)\n",
    "        if df is not None:\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        \n",
    "    def log_txt(txt: str, debug: bool) -> None:\n",
    "        if debug:\n",
    "            print(\"Reading string content...\")\n",
    "            print(\"-\"*20)\n",
    "            print(txt)  \n",
    "        \n",
    "    \n",
    "    '''Global variable'''\n",
    "    model_map = {\n",
    "       #\"small\": \"google/mt5-small\", # 1.2 GB\n",
    "       #\"base\" : \"google/mt5-base\", # 2.33 GB\n",
    "       #\"large\" : \"google/mt5-large\", # 4.9 GB,\n",
    "       #\"xl\" : \"google/mt5-xl\", # 15 GB\n",
    "       #\"xxl\" : \"google/mt5-xxl\", # 51.7 GB,\n",
    "       \"custom\": \"Helsinki-NLP/opus-mt-de-en\", \n",
    "    }\n",
    "    data_path = f\"{data_root}/{data_sub_path}\"\n",
    "    model_name = model_map.get(model_type, \"custom\")\n",
    "    # nvidia_state = GPUInfoHelper()\n",
    "    split_length = 350 # the token to split for text\n",
    "    \n",
    "    # load feather DataFrame\n",
    "    feather_text_col_name = \"text\"\n",
    "    df = load_feather(input_path)\n",
    "    # get the row series and the column\n",
    "    origin_text = str(df.iloc[0][feather_text_col_name])\n",
    "    print(f\"origin_text has token length: {len(origin_text)}\")\n",
    "    \n",
    "    '''Initialization'''\n",
    "    # UUIDs = nvidia_mig_uuids()\n",
    "    init_transformers()\n",
    "    # init_cuda_torch(UUIDs, data_path)\n",
    "    # import torch\n",
    "    display_container_info()\n",
    "    # print(UUIDs)\n",
    "    \n",
    "    show_folder_files(data_path)\n",
    "    \n",
    "    # transformers must be imported after the init_cuda_torch so that the env variable will be set properly\n",
    "    from transformers import pipeline\n",
    "    import transformers\n",
    "    \n",
    "    print(f\"Loading LLM model {model_name} ...\")\n",
    "    # using the accelerator with id by default, the device_map=\"auto\" doesn't work,\n",
    "    # model is too outdated to use auto accelerator detection. \n",
    "    accelerator_id = 0\n",
    "    generator = pipeline(\n",
    "        \"translation\", \n",
    "        model=model_name,\n",
    "        # device_map=\"auto\",\n",
    "        device=accelerator_id,\n",
    "    )\n",
    "        \n",
    "    def translate_gen(\n",
    "        generator: transformers.pipelines.text2text_generation.TranslationPipeline, \n",
    "        nvidia_state: GPUInfoHelper = None,\n",
    "    ):  \n",
    "        \"\"\"\n",
    "        Args:\n",
    "          max_new_tokens: control the maximum length of the generation\n",
    "        \"\"\"\n",
    "\n",
    "        def local(sentences: list, print_mode: bool = True, max_length=400) -> list:\n",
    "            \"\"\"single input, no batch input\n",
    "            Args:\n",
    "              sentences:\n",
    "            \"\"\"\n",
    "            start = time.time()\n",
    "            result = generator(sentences, max_length=max_length)\n",
    "            end = time.time()\n",
    "            duration = end - start\n",
    "            if print_mode: \n",
    "                print(\"-\"*20)\n",
    "                print(f\"walltime: {duration} in secs.\")\n",
    "                if nvidia_state is not None:\n",
    "                    nvidia_state.gpu_usage()\n",
    "            return result\n",
    "        return local    \n",
    "    \n",
    "    # create the convenient translate function\n",
    "    # translate = translate_gen(generator, nvidia_state)\n",
    "    \n",
    "    translate = translate_gen(generator, None)\n",
    "    # translate input\n",
    "    splitted_content = wrap(origin_text, split_length)\n",
    "    split_count = len(splitted_content)\n",
    "    output = []\n",
    "    for idx, split_text in enumerate(splitted_content, start=1):\n",
    "        print_mode = (idx == 1 or idx == split_count)\n",
    "        print(f\"print_mode: {print_mode}\")\n",
    "        output.append(translate(split_text, print_mode, 1000)[0].get('translation_text', '').strip())\n",
    "    \n",
    "    en_content = ''.join(output)\n",
    "    print(f\"translated en_content text has token length: {len(en_content)}\")\n",
    "    # show content in component logs\n",
    "    log_txt(txt = en_content, debug=show_log_txt)\n",
    "    save_txt_to_feather(en_content, output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83427c5f-3bdb-40d4-8d44-3fef37d4e1c1",
   "metadata": {},
   "source": [
    "### Create llm inference component\n",
    "\n",
    "#### Subprocess call to pass the nvidia-smi output\n",
    "\n",
    "* Python 3.5 subprocess.run https://stackoverflow.com/questions/4760215/running-shell-command-and-capturing-the-output\n",
    "* https://stackoverflow.com/questions/7681715/whats-the-difference-between-subprocess-popen-and-call-how-can-i-use-them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfb109-c492-4e79-aa07-a25886588cfa",
   "metadata": {},
   "source": [
    "#### Issue\n",
    "\n",
    "```console\n",
    "RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n",
    "/usr/local/lib/python3.8/dist-packages/transformer_engine_extensions.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
    "Error: exit status 1\n",
    "```\n",
    "Workaround:\n",
    "* https://github.com/microsoft/TaskMatrix/issues/116\n",
    "\n",
    "\n",
    "* If a None object is returned, and the component shall return as string will receive an error: https://github.com/kubeflow/pipelines/issues/8868"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd5142-0cb7-4a3b-9cd9-e0a9d0e383fd",
   "metadata": {},
   "source": [
    "#### Example of batch Gen\n",
    "\n",
    "* https://github.com/huggingface/transformers/issues/18478#issuecomment-1208049618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf024d03-f8aa-486e-8157-1c3c3cc92068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}_inference_component.yaml\",\n",
    "    base_image=settings.base_torch_image, # use pt base image\n",
    "    packages_to_install=[\n",
    "        # \"transformers==4.32.1\",\n",
    "        \"xformers==0.0.21\",\n",
    "        \"huggingface_hub==0.17.1\", \n",
    "        # \"accelerate==0.21.0\", # bug in accelerate 0.22.0 which runs on cpu only https://discuss.huggingface.co/t/could-not-load-model-meta-llama-llama-2-7b-chat-hf-with-any-of-the-following-classes/47641\n",
    "        settings.pandas,\n",
    "        settings.pyarrow,\n",
    "    ],# adding additional libs\n",
    ")\n",
    "def llm_gen(\n",
    "    data_root: str, \n",
    "    data_sub_path: str, \n",
    "    model_type: str,\n",
    "    prompt_templates: list,\n",
    "    #prompt_context: str,\n",
    "    prompt_placeholder: str,\n",
    "    # prompt: str,\n",
    "    prompt_context_path: InputPath(\"Dataset\"),\n",
    "    show_log_txt: bool=False) -> NamedTuple(\"output\", [(\"answers\", list)]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      prompt_templates: list of prompt string template, which can be extended with prompt_context by the promplt_placeholder\n",
    "      prompt_context_path (prompt_context): additional context string passed as feather binary, which can be injected to the prompts, the path is dropped during the kfp compiling\n",
    "      prompt_placeholder: the special charactor used in the prompts to be replaced by the prompt_context\n",
    "    \"\"\"\n",
    "    from collections import namedtuple \n",
    "    import subprocess\n",
    "    import os, time, sys, re\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame, Series\n",
    "    # https://github.com/huggingface/transformers/issues/23340\n",
    "    # subprocess.call([\"pip\", \"uninstall\", \"-y\", \"transformer-engine\"])\n",
    "    \n",
    "    class GPUInfoHelper():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "\n",
    "        def byte_gb_info(self, byte_mem) -> str:\n",
    "            \"\"\"calculate the byte size to GB size for better human readable\"\"\"\n",
    "            # format the f string float with :.2f to decimal digits\n",
    "            # https://zetcode.com/python/fstring/\n",
    "            return f\"{(byte_mem/1024**3):4f} GB\"\n",
    "\n",
    "\n",
    "        def accelerator_mem_info(self, device_idx: int):\n",
    "            # total\n",
    "            t = torch.cuda.get_device_properties(device_idx).total_memory\n",
    "            # usable\n",
    "            r = torch.cuda.memory_reserved(device_idx)\n",
    "            # allocated\n",
    "            a = torch.cuda.memory_allocated(device_idx)\n",
    "            # still free\n",
    "            f = r-a  \n",
    "            print( # \"GPU memory info:\\n\" + \n",
    "                  f\"Physical  memory : {self.byte_gb_info(t)}\\n\" + \n",
    "                  f\"Reserved  memory : {self.byte_gb_info(r)}\\n\" + \n",
    "                  f\"Allocated memory : {self.byte_gb_info(a)}\\n\" + \n",
    "                  f\"Free      memory : {self.byte_gb_info(f)}\")\n",
    "\n",
    "\n",
    "        def accelerator_compute_info(self, device_idx: int) -> None:\n",
    "            name = torch.cuda.get_device_properties(device_idx).name\n",
    "            count = torch.cuda.get_device_properties(device_idx).multi_processor_count\n",
    "            print(f\"Device_name      : {name} \\n\" +\n",
    "                  f\"Multi_processor  : {count}\")    \n",
    "\n",
    "\n",
    "        def gpu_usage(self) -> None:        \n",
    "            num_of_gpus = torch.cuda.device_count();\n",
    "            # this shows only the gpu device, not the MIG\n",
    "            print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "            for device_idx in range(torch.cuda.device_count()):\n",
    "                print(\"-\"*20)\n",
    "                self.accelerator_compute_info(device_idx)                 \n",
    "                self.accelerator_mem_info(device_idx)\n",
    "                print(\"-\"*20)\n",
    "    \n",
    "    \n",
    "    def display_container_info():\n",
    "        print(\"-\"*10)\n",
    "        print(f\"python version: {sys.version}\")\n",
    "        print(f\"torch version: {torch.__version__}\")\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "    \n",
    "    def nvidia_device_uuid(input: str):\n",
    "        \"\"\"parse the nvidia devices uuid from the nvidia device info str\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # r'' before the search pattern indicates it is a raw string, \n",
    "            # otherwise \"\" instead of single quote\n",
    "            uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "        except AttributeError:\n",
    "            # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "            uuid = \"\"\n",
    "        return uuid\n",
    "    \n",
    "    \n",
    "    def nvidia_device_info() -> str:\n",
    "        \"\"\"get the nvidia MIGs device uuid and GPU uuid \n",
    "        \"\"\"\n",
    "        # blocking call\n",
    "        result = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE)\n",
    "        # decode the byte object, returns string with \\n\n",
    "        cmd_out_str = result.stdout.decode('utf-8')\n",
    "        return [line.strip() for line in cmd_out_str.split('\\n') if len(line) > 0]\n",
    "        \n",
    "    \n",
    "    def nvidia_mig_uuids() -> str:\n",
    "        \"\"\"get a comma separated str of nvidia MIGs devices\n",
    "        \"\"\"\n",
    "        info_list = nvidia_device_info()\n",
    "        # skip the first GPU ID, get the MIGs IDS\n",
    "        uuid_list = [nvidia_device_uuid(e) for e in info_list[1:]]\n",
    "        # if multi gpus need to join the device together for pytorch\n",
    "        return \",\".join(uuid_list)\n",
    "    \n",
    "    \n",
    "    def init_cuda_torch(uuids: str, data_path: str) -> None:\n",
    "        \"\"\"setup the default env variables for transformers\n",
    "        \n",
    "        Args:\n",
    "          uuids: a comma separate str of nvidia gpu/mig uuids\n",
    "        \"\"\"\n",
    "        os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = uuids \n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "        os.environ['XDG_CACHE_HOME']=f\"{data_path}/models\"\n",
    "        \n",
    "        \n",
    "    def show_folder_files(folder: str) -> None:\n",
    "        print(os.listdir(folder))\n",
    "        \n",
    "        \n",
    "    def huggingface_access_token(data_path: str) -> str:\n",
    "        token_file_path = f\"{data_path}/.cache/huggingface/token\"\n",
    "        token = \"\"\n",
    "        with open(token_file_path, \"r\") as file:\n",
    "            token = file.read().replace('\\n', '')\n",
    "        return token\n",
    "    \n",
    "    \n",
    "    def load_feather(path: str, col_names=[\"text\"]) -> DataFrame:\n",
    "        with open(path, \"r\") as file:\n",
    "            df = pd.read_feather(path)\n",
    "        if df is not None:\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        \n",
    "    def log_txt(data, debug: bool) -> None:\n",
    "        if debug:\n",
    "            if isinstance(data, str):\n",
    "                print(\"Reading string content...\")\n",
    "                print(\"-\"*20)\n",
    "                print(data)\n",
    "            elif isinstance(data, list):\n",
    "                for txt in data:\n",
    "                    print(\"Reading string content...\")\n",
    "                    print(\"-\"*20)\n",
    "                    print(txt)\n",
    "    \n",
    "    def prompts_with_context(prompts: list, placeholder, context: str) -> list:\n",
    "        \"\"\"replace the placeholder in prompts with context\n",
    "        \n",
    "        Returns:\n",
    "           new prompt list of str replace the placeholder with context\n",
    "        \"\"\"\n",
    "        return [ prompt.replace(placeholder, context) for prompt in prompts]\n",
    "        \n",
    "        \n",
    "    '''Global variable'''\n",
    "    model_map = {\n",
    "        \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"70B\" : \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        # \"70B\" : \"meta-llama/Llama-2-70b-hf\" \n",
    "    }\n",
    "    data_path = f\"{data_root}/{data_sub_path}\"\n",
    "    model_name = model_map.get(model_type, \"7B\")\n",
    "    nvidia_state = GPUInfoHelper()\n",
    "    # load feather DataFrame\n",
    "    feather_text_col_name = \"text\"\n",
    "    df = load_feather(prompt_context_path)\n",
    "    # get the row series and the column\n",
    "    prompt_context = str(df.iloc[0][feather_text_col_name])\n",
    "    prompts = prompts_with_context(prompts=prompt_templates, placeholder=prompt_placeholder, context=prompt_context)\n",
    "    # clean up memory\n",
    "    del prompt_context\n",
    "    # print str or list[str] content with log_txt\n",
    "    log_txt(data=prompts, debug=show_log_txt)\n",
    "    \n",
    "    '''Initialization'''\n",
    "    UUIDs = nvidia_mig_uuids()\n",
    "    init_cuda_torch(UUIDs, data_path)\n",
    "    import torch\n",
    "    display_container_info()\n",
    "    print(UUIDs)\n",
    "    \n",
    "    show_folder_files(data_path)\n",
    "    \n",
    "    '''Transformers must be imported after the init_cuda_torch to get env set'''\n",
    "    import transformers\n",
    "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "    import transformers\n",
    "    print(f\"transformers version: {transformers.__version__}\")\n",
    "    \n",
    "    def chat_gen(\n",
    "        generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "        tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "        nvidia_state: GPUInfoHelper,\n",
    "    ):    \n",
    "        def local(input: str, print_mode: bool = True) -> list[str]:\n",
    "            start = time.time()\n",
    "            sequences = generator(\n",
    "                input,\n",
    "                do_sample=True,\n",
    "                top_k=10,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                # max_length=200,\n",
    "                max_new_tokens=200,\n",
    "            )\n",
    "            result = []\n",
    "            for seq in sequences:\n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "\n",
    "            end = time.time()\n",
    "            duration = end - start\n",
    "            # returns the result\n",
    "            if print_mode == True:\n",
    "                for s in result:\n",
    "                    print(s)\n",
    "\n",
    "                print(\"-\"*20)\n",
    "                print(f\"walltime: {duration} in secs.\")\n",
    "                nvidia_state.gpu_usage()\n",
    "            return result\n",
    "        \n",
    "        return local\n",
    "    \n",
    "    \n",
    "    token = huggingface_access_token(data_path)\n",
    "    print(f\"XDG_CACHE_HOME: {os.environ['XDG_CACHE_HOME']}\")\n",
    "    print(f\"Loading LLM model {model_name} ...\")\n",
    "    # os.makedirs(\"/tmp/outputs/Output/data\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        #use_auth_token=token,\n",
    "        token=token, #transformers==4.32.1\n",
    "    )\n",
    "    \n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        token=token,\n",
    "        #use_auth_token=token, #transformers==4.32.1\n",
    "    )\n",
    "    # print gpu mem usage after loading the llm\n",
    "    nvidia_state.gpu_usage()\n",
    "    # create convenient chat function\n",
    "    chat = chat_gen(generator, tokenizer, nvidia_state)\n",
    "    \n",
    "    gen_results = []\n",
    "    print(f\"total prompts: {len(prompts)}\")\n",
    "    for prompt in prompts:\n",
    "        print(f\"current prompt token length: {len(prompt)}\")\n",
    "        talk_back_list = chat(prompt)\n",
    "    \n",
    "        if (talk_back_list is not None) and len(talk_back_list) > 0:\n",
    "            # answer_str = talk_back_list[0]\n",
    "            gen_results.append(talk_back_list[0])\n",
    "        else:\n",
    "            # answer_str = \"\"\n",
    "            gen_results.append(\"\")\n",
    "            \n",
    "    #with open(output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "    #    df.to_csv(f, index=False, header=True, encoding=\"utf-8\")\n",
    "    \n",
    "    output = namedtuple('output',['answers']) \n",
    "    return output(gen_results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01f1ed-4564-4302-8fb9-094e1325ebe0",
   "metadata": {},
   "source": [
    "### Create data processing component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165590ad-f3ba-482f-bcdc-16635f5ab69c",
   "metadata": {},
   "source": [
    "### Define Helper Function\n",
    "Difference between 2Gi and 2G:\n",
    "* https://stackoverflow.com/questions/50804915/kubernetes-size-definitions-whats-the-difference-of-gi-and-g/50805048#50805048\n",
    "\n",
    "Set MIG GPU requests:\n",
    "* https://github.com/kubeflow/pipelines/issues/6858#issuecomment-1007511676\n",
    "\n",
    "```python\n",
    "containerOp.add_resource_request(gpu_resource, gpu_req)\n",
    "containerOp.add_resource_limit(gpu_resource, gpu_lim)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb43819c-aeb2-4ab0-a395-54dc3c3bae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pod_resource_transformer(op: ContainerOp, mem_req=\"200Mi\", cpu_req=\"2000m\", mem_lim=\"4000Mi\", cpu_lim='4000m', gpu_req=None, gpu_lim=None, gpu_type:str=\"20gb\"):\n",
    "    \"\"\"\n",
    "    this function helps to set the resource limit for container operators\n",
    "    op.set_memory_limit('1000Mi') = 1GB\n",
    "    op.set_cpu_limit('1000m') = 1 cpu core\n",
    "    \"\"\"\n",
    "    if gpu_type == \"20gb\":\n",
    "        gpu_resource = \"nvidia.com/mig-1g.20gb\"\n",
    "    else:\n",
    "        gpu_resource = \"nvidia.com/mig-1g.10gb\"\n",
    "        \n",
    "    # gpu_resource = \"nvidia.com/mig-2g.20gb\"\n",
    "    new_op = op.set_memory_request(mem_req)\\\n",
    "        .set_memory_limit(mem_lim)\\\n",
    "        .set_cpu_request(cpu_req)\\\n",
    "        .set_cpu_limit(cpu_lim)\n",
    "    if (gpu_req is not None) and (gpu_lim is not None):\n",
    "        new_op.add_resource_request(gpu_resource, gpu_req)\n",
    "        new_op.add_resource_limit(gpu_resource, gpu_lim)\n",
    "    return new_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30253e28-bf1e-4716-a05b-2524952da9b5",
   "metadata": {},
   "source": [
    "## Define Pipeline\n",
    "* Intro Kubeflow pipeline: https://v1-5-branch.kubeflow.org/docs/components/pipelines/introduction/\n",
    "* Kubeflow pipeline SDK v1: https://v1-5-branch.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8202fe0-e1fa-4795-81d3-4441c0a1d201",
   "metadata": {},
   "source": [
    "#### Construct promp list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8363d023-362f-42f0-934d-eb33da3c6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_holder = \"#\"\n",
    "test_prompt='Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n'\n",
    "name_prompt = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {place_holder}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "age_prompt = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{place_holder}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "\n",
    "# processed in parallel output of memory\n",
    "# prompts = [test_prompt, name_prompt, age_prompt]\n",
    "prompts = [age_prompt]\n",
    "# prompts = [test_prompt]\n",
    "# print(len(prompts))\n",
    "# print(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621d9b5-30db-412e-974a-a795b6f06211",
   "metadata": {},
   "source": [
    "#### Define pipeline DAG\n",
    "\n",
    "KFP pipeline v1 sdk https://kubeflow-pipelines.readthedocs.io/en/1.8.22/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f43545d-ddfd-49ad-8f43-fb7600d4b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name = EXPERIMENT_NAME,\n",
    "    description = EXPERIMENT_DESC\n",
    ")\n",
    "def custom_pipeline(\n",
    "    bucket_name: str=\"scivias-medreports\",\n",
    "    s3_secrets: str=\"add-scivias-medreport-secret\", \n",
    "    s3_verify_host: bool = True,\n",
    "    file_prefix: str=\"KK-SCIVIAS\",\n",
    "    item_max_cap: int = 2,\n",
    "    model_root: str = \"/mnt\",\n",
    "    model_sub_path: str = \"core-kind/yinwang\",\n",
    "    trans_model_type: str=\"custom\", \n",
    "    gen_model_type: str=\"7B\",\n",
    "    show_log_txt: bool=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      data_root: the mount path of shared data volume.\n",
    "      data_sub_path: the relative path to the data folder, without leading ./\n",
    "    \"\"\"\n",
    "    \n",
    "    '''local variable'''\n",
    "    no_artifact_cache = \"P0D\"\n",
    "    artifact_cache_today = \"P1D\"\n",
    "    #cache_setting = no_artifact_cache\n",
    "    cache_setting = artifact_cache_today\n",
    "    # prompt = \"how are you buddy?\"\n",
    "    # trans_text = \"Das Haus ist wunderbar.\"\n",
    "    \n",
    "    '''Pipeline Volume'''\n",
    "    # predefined pvc in namespace\n",
    "    shared_volume = PipelineVolume(\"llm-models\")\n",
    "    \n",
    "    '''pipeline'''\n",
    "    translator_task = en_translator(\n",
    "        bucket_name=bucket_name, \n",
    "        file_prefix=file_prefix,\n",
    "        item_max_cap=item_max_cap,\n",
    "        show_log_txt=show_log_txt,\n",
    "        model_root=model_root, \n",
    "        model_sub_path=model_sub_path, \n",
    "        model_type=trans_model_type,\n",
    "        s3_verify_host=s3_verify_host\n",
    "    )\n",
    "\n",
    "\n",
    "    translator_task = pod_resource_transformer(translator_task, mem_req=\"4000Mi\", \n",
    "                                               cpu_req=\"2000m\", mem_lim=\"8000Mi\", \n",
    "                                               cpu_lim=\"4000m\", gpu_req=1, gpu_lim=1, gpu_type=\"10gb\")\n",
    "    translator_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    translator_task.add_pvolumes({model_root: shared_volume})\n",
    "    translator_task.set_display_name(\"S3-PDF de-en translator\")\n",
    "    translator_task.add_pod_label(s3_secrets, \"true\") # s3_serets is the label for poddefault to mount s3 secret\n",
    "    \n",
    "#     translate_task = bart_translator(\n",
    "#         data_root=data_root, \n",
    "#         data_sub_path=data_sub_path,\n",
    "#         model_type=trans_model_type,\n",
    "#         show_log_txt=show_log_txt,\n",
    "#         input=extract_task.outputs[\"output\"]\n",
    "#         # origin_text=extract_task.output\n",
    "#     )\n",
    "#     #translate_task = pod_resource_transformer(translate_task, mem_req=\"24000Mi\", cpu_req=\"1000m\", mem_lim=\"24000Mi\", cpu_lim=\"2000m\", gpu_req=1, gpu_lim=1)\n",
    "#     translate_task = pod_resource_transformer(translate_task, mem_req=\"12000Mi\", cpu_req=\"4000m\", mem_lim=\"12000Mi\", cpu_lim=\"16000m\", gpu_req=1, gpu_lim=1)\n",
    "#     translate_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "#     translate_task.add_pvolumes({data_root: shared_volume})\n",
    "#     translate_task.set_display_name(\"BART de_en translator\")\n",
    "    \n",
    "#     inference_task = llm_gen(\n",
    "#         data_root=data_root, \n",
    "#         data_sub_path=data_sub_path, \n",
    "#         model_type=gen_model_type,\n",
    "#         prompt_templates=prompts,\n",
    "#         prompt_context=translate_task.output,\n",
    "#         prompt_placeholder=place_holder,\n",
    "#         show_log_txt=show_log_txt\n",
    "#     )\n",
    "#     # 200 MB ram and 1 cpu\n",
    "#     inference_task = pod_resource_transformer(inference_task, mem_req=\"24000Mi\", cpu_req=\"1000m\", mem_lim=\"24000Mi\", cpu_lim=\"2000m\", gpu_req=1, gpu_lim=1)\n",
    "#     # set the download caching to be 1day, disable caching with P0D\n",
    "#     inference_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "#     inference_task.add_pvolumes({data_root: shared_volume})\n",
    "#     inference_task.set_display_name(\"LlaMA2 entity extractor\")\n",
    "    \n",
    "#     inference_task.after(translate_task)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e789aa-5ccc-4290-8d9d-b3297128417b",
   "metadata": {},
   "source": [
    "### (optional) pipeline compile step\n",
    "use the following command to compile the pipeline to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beb19bcb-2215-4ca6-93a8-bd65a31f6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFP_VERSION=\"kfp1\"\n",
    "PIPE_LINE_PURPOSE=\"pdf_translation\"\n",
    "PIPE_LINE_FILE_NAME=f\"{PREFIX}_{KFP_VERSION}_{PIPE_LINE_PURPOSE}_pipeline\"\n",
    "kfp.compiler.Compiler().compile(custom_pipeline, f\"{PIPE_LINE_FILE_NAME}.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d198c16-f9ab-4e4c-a2f2-d710bec7aba9",
   "metadata": {},
   "source": [
    "### Create Experiment Run\n",
    "\n",
    "create run label with current data time\n",
    "```python\n",
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "ts = datetime.strftime(datetime.now(ptimezone(\"Europe/Berlin\")), \"%Y-%m-%d %H-%M-%S\")\n",
    "print(ts)\n",
    "```\n",
    "\n",
    "Reference:\n",
    "* https://stackoverflow.com/questions/25837452/python-get-current-time-in-right-timezone/25887393#25887393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08dd9330-1a3f-4fcd-8e53-fa7e4a05de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "\n",
    "def get_local_time_str(target_tz_str: str = \"Europe/Berlin\", format_str: str = \"%Y-%m-%d %H-%M-%S\") -> str:\n",
    "    \"\"\"\n",
    "    this method is created since the local timezone is miss configured on the server\n",
    "    @param: target timezone str default \"Europe/Berlin\"\n",
    "    @param: \"%Y-%m-%d %H-%M-%S\" returns 2022-07-07 12-08-45\n",
    "    \"\"\"\n",
    "    target_tz = ptimezone(target_tz_str) # create timezone, in python3.9 use standard lib ZoneInfo\n",
    "    # utc_dt = datetime.now(datetime.timezone.utc)\n",
    "    target_dt = datetime.now(target_tz)\n",
    "    return datetime.strftime(target_dt, format_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1ed73-9fd8-4e59-855e-690e738d09d9",
   "metadata": {},
   "source": [
    "### Config pipeline run\n",
    "* Setting imagePullSecretes for Pipeline with SDK: https://github.com/kubeflow/pipelines/issues/5843#issuecomment-859799181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dda75a5d-e923-4823-89ee-cc41e49c370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pandas import DataFrame, Series\n",
    "\n",
    "# place_holder = \"#\"\n",
    "# test_prompt='Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n'\n",
    "# name_prompt = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {place_holder}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "# age_prompt = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{place_holder}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "\n",
    "# ser = Series([test_prompt, name_prompt, age_prompt])\n",
    "# df = DataFrame(data=ser, columns=[\"prompt\"])\n",
    "# df.to_json()\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "853837b3-604d-439b-9cea-4b8e226c0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kubernetes import client as k8s_client\n",
    "pipeline_config = dsl.PipelineConf()\n",
    "\n",
    "# pipeline_config.set_image_pull_secrets([k8s_client.V1ObjectReference(name=K8_GIT_SECRET_NAME, namespace=NAME_SPACE)])\n",
    "# pipeline_config.set_image_pull_policy(\"Always\")\n",
    "pipeline_config.set_image_pull_policy(\"IfNotPresent\")\n",
    "\n",
    "# note:\n",
    "#file_idx = 1 has 8 page 14K token, too long for the llm prompt\n",
    "#file_idx = 0 has 4 page 7K token, works for the llm prompt\n",
    "pipeline_args = {\n",
    "    'bucket_name': \"scivias-medreports\",\n",
    "    's3_secrets': \"add-scivias-medreport-secret\",\n",
    "    's3_verify_host': True,\n",
    "    'file_prefix' : FILE_PREFIX,\n",
    "    #'item_max_cap' : 2, # -1 return all, 1 return 1 elemement\n",
    "    'item_max_cap' : -1, # -1 return all, 1 return 1 elemement\n",
    "    'model_root' : MODEL_ROOT,\n",
    "    'model_sub_path' : MODEL_SUB_PATH,\n",
    "    'gen_model_type': DEFAULT_GEN_MODEL_TYPE,\n",
    "    'trans_model_type': DEFAULT_TRANS_MODEL_TYPE,\n",
    "    'show_log_txt': True,\n",
    "}\n",
    "#print(pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5264dc44-9abd-48a2-970e-3ed8693edfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/f5acd22e-7836-41a6-b8cd-a555a7f71f39\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/975faf28-a9d9-4540-ae5b-c7f9661987c5\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunPipelineResult(run_id=975faf28-a9d9-4540-ae5b-c7f9661987c5)\n"
     ]
    }
   ],
   "source": [
    "RUN_NAME = f\"{PREFIX}_{KFP_VERSION}_{PIPE_LINE_PURPOSE} {get_local_time_str()}\"\n",
    "\n",
    "# client = kfp.Client()\n",
    "run = client.create_run_from_pipeline_func(\n",
    "    pipeline_func=custom_pipeline,\n",
    "    arguments = pipeline_args, #{}\n",
    "    run_name = RUN_NAME,\n",
    "    pipeline_conf=pipeline_config,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    ")\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd6d15-c88d-4d5a-9b4e-fb2f3706c2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
