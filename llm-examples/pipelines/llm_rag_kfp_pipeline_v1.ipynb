{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bde0f4-67cd-4a5a-af8c-abf0f89a15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0003cb5d-2144-4d17-9d9c-ca060d3acbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade --user kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01c1e77-1584-4cc6-80ee-95ea349396f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from functools import partial\n",
    "from kfp.dsl import (\n",
    "    pipeline,\n",
    "    ContainerOp,\n",
    "    PipelineVolume\n",
    ")\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    create_component_from_func\n",
    ")\n",
    "\n",
    "EXPERIMENT_NAME = 'llm' # Name of the experiment in the KF webapp UI\n",
    "EXPERIMENT_DESC = 'llm med report pipeline experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7bb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Settings():\n",
    "    llm_base_image: str = 'pytorch/pytorch:2.2.0-cuda11.8-cudnn8-devel'\n",
    "    applyllm_version: str = '0.0.2'\n",
    "    pypdf_version: str = '3.15.5'\n",
    "    accelerate_version: str = '0.26.1'\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7763edd5-3c2e-44c5-afb1-3983a46d0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"custom_registry_component.yaml\",\n",
    "    base_image=settings.llm_base_image, \n",
    "    packages_to_install=[\n",
    "        f\"applyllm=={settings.applyllm_version}\",\n",
    "        f\"pypdf=={settings.pypdf_version}\",\n",
    "        f\"accelerate=={settings.accelerate_version}\"\n",
    "    ], # adding additional libs\n",
    "    # pip_index_urls=[\"https://gitlab.lrz.de/api/v4/projects/150553/packages/pypi/simple\"]\n",
    "    # define my private pypi package registry v2 component decorator\n",
    ")\n",
    "def llm_op(model_root: str):\n",
    "    import os\n",
    "    import applyllm as apl\n",
    "\n",
    "    from applyllm.accelerators import (\n",
    "        AcceleratorHelper,\n",
    "        AcceleratorStatus,\n",
    "        DirectorySetting,\n",
    "        # DIR_MODE_MAP,\n",
    "        TokenHelper as th\n",
    "    )\n",
    "    from applyllm.utils import time_func\n",
    "    from applyllm.pipelines import (\n",
    "        LocalCausalLMConfig,\n",
    "        ModelConfig,\n",
    "        ModelCatalog,\n",
    "    )\n",
    "    import transformers\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from torch import bfloat16\n",
    "\n",
    "\n",
    "    print(f\"applyllm version:     {apl.__version__}\")\n",
    "    print(f\"transformers version: {transformers.__version__}\")\n",
    "    print(f\"torch version:        {torch.__version__}\")\n",
    "\n",
    "    dir_setting = DirectorySetting(home_dir=model_root)\n",
    "\n",
    "    gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "    '''init gpu helper'''\n",
    "    gpu_helper = AcceleratorHelper()\n",
    "    UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)\n",
    "    gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "    print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "    print(os.environ[\"XDG_CACHE_HOME\"])\n",
    "    '''init llm model to be loaded'''\n",
    "    model_map = {\n",
    "        \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "        \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "        \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    }\n",
    "    model_type = \"mistral7B-inst02\"\n",
    "    model_name = model_map.get(model_type, \"7B\")\n",
    "    print(model_name)\n",
    "\n",
    "    token_kwargs = th.gen_token_kwargs(model_type=model_type, dir_setting=dir_setting)\n",
    "\n",
    "    base_lm_config = ModelConfig(\n",
    "        model_config = {\n",
    "            \"pretrained_model_name_or_path\": model_name,\n",
    "            \"device_map\": \"auto\",\n",
    "            # \"max_memory\": f\"{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    lm_model_kwargs = {\n",
    "        \"quantized\": True,\n",
    "        \"model_config\": base_lm_config.get_config(),\n",
    "        \"quantization_config\": {\n",
    "            \"quantization_config\": transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=bfloat16\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "    lm_config = LocalCausalLMConfig(**lm_model_kwargs)\n",
    "\n",
    "    @time_func\n",
    "    def fetch_lm_model():\n",
    "        return AutoModelForCausalLM.from_pretrained(\n",
    "        **lm_config.get_config(),\n",
    "        **token_kwargs,  \n",
    "        )\n",
    "\n",
    "    model = fetch_lm_model()\n",
    "\n",
    "    gpu_status.gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9dc48c-75de-4812-8177-a27b5a414067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_res_limits(task: ContainerOp, mem_req=\"200Mi\", cpu_req=\"2000m\", mem_lim=\"4000Mi\", cpu_lim='4000m', gpu_req=None, gpu_lim=None, gpu_type:str=\"20gb\"):\n",
    "    \"\"\"\n",
    "    this function helps to set the resource limit for container operators\n",
    "    op.set_memory_limit('1000Mi') = 1GB\n",
    "    op.set_cpu_limit('1000m') = 1 cpu core\n",
    "    \"\"\"\n",
    "    if gpu_type == \"20gb\":\n",
    "        gpu_resource = \"nvidia.com/mig-2g.20gb\"\n",
    "        # gpu_resource = \"nvidia.com/mig-1g.20gb\"\n",
    "    else:\n",
    "        gpu_resource = \"nvidia.com/mig-1g.10gb\"\n",
    "        \n",
    "    # gpu_resource = \"nvidia.com/mig-2g.20gb\"\n",
    "    new_op = task.set_memory_request(mem_req)\\\n",
    "        .set_memory_limit(mem_lim)\\\n",
    "        .set_cpu_request(cpu_req)\\\n",
    "        .set_cpu_limit(cpu_lim)\n",
    "    if (gpu_req is not None) and (gpu_lim is not None):\n",
    "        new_op.add_resource_request(gpu_resource, gpu_req)\n",
    "        new_op.add_resource_limit(gpu_resource, gpu_lim)\n",
    "    return new_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2af7a2-4166-4ac5-84fb-93f24b49bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name = EXPERIMENT_NAME,\n",
    "    description = EXPERIMENT_DESC\n",
    ")\n",
    "def llm_pipeline(model_root: str = \"/mnt\"):\n",
    "    '''local variable'''\n",
    "    no_artifact_cache = \"P0D\"\n",
    "    artifact_cache_today = \"P1D\"\n",
    "    # cache_setting = artifact_cache_today\n",
    "    cache_setting = no_artifact_cache\n",
    "\n",
    "    '''Pipeline Volume'''\n",
    "    shared_volume = PipelineVolume(\"llm-models\")\n",
    "    \n",
    "    '''pipeline'''   \n",
    "    llm_task = llm_op(model_root=model_root)\n",
    "    # 200 MB ram and 1 cpu\n",
    "    llm_task = set_res_limits(task=llm_task, mem_req=\"20Gi\", mem_lim=\"40Gi\",\n",
    "                            cpu_req=\"2000m\", cpu_lim=\"10000m\", \n",
    "                            gpu_req=1, gpu_lim=1, gpu_type=\"20gb\")\n",
    "    # set the download caching to be 1day, disable caching with P0D\n",
    "    # download_task.execution_options.caching_strategy.max_cache_staleness = artifact_cache_today\n",
    "    llm_task.add_pvolumes({model_root: shared_volume})\n",
    "    llm_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    llm_task.set_display_name(\"llm op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739fe18f-2384-4bbb-a9bb-7e284b028903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pipeline_path_dir=\"./compiled\"\n",
    "if not os.path.exists(pipeline_path_dir):\n",
    "    os.makedirs(pipeline_path_dir)\n",
    "\n",
    "PIPE_LINE_FILE_NAME=f\"llm_rag_kfp_pipeline\"\n",
    "kfp.compiler.Compiler().compile(llm_pipeline, f\"{pipeline_path_dir}/{PIPE_LINE_FILE_NAME}.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934304c5-326f-4d67-85d5-9920d1683590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "\n",
    "def get_local_time_str(target_tz_str: str = \"Europe/Berlin\", format_str: str = \"%Y-%m-%d %H-%M-%S\") -> str:\n",
    "    \"\"\"\n",
    "    this method is created since the local timezone is miss configured on the server\n",
    "    @param: target timezone str default \"Europe/Berlin\"\n",
    "    @param: \"%Y-%m-%d %H-%M-%S\" returns 2022-07-07 12-08-45\n",
    "    \"\"\"\n",
    "    target_tz = ptimezone(target_tz_str) # create timezone, in python3.9 use standard lib ZoneInfo\n",
    "    # utc_dt = datetime.now(datetime.timezone.utc)\n",
    "    target_dt = datetime.now(target_tz)\n",
    "    return datetime.strftime(target_dt, format_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2118ac78-8984-475f-9c0b-d4c530e60ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kubernetes import client as k8s_client\n",
    "pipeline_config = dsl.PipelineConf()\n",
    "\n",
    "# pipeline_config.set_image_pull_secrets([k8s_client.V1ObjectReference(name=K8_GIT_SECRET_NAME, namespace=NAME_SPACE)])\n",
    "# pipeline_config.set_image_pull_policy(\"Always\")\n",
    "pipeline_config.set_image_pull_policy(\"IfNotPresent\")\n",
    "\n",
    "pipeline_args = {\n",
    "    \"model_root\": \"/mnt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a646a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = f\"{PIPE_LINE_FILE_NAME} {get_local_time_str()}\"\n",
    "\n",
    "client = kfp.Client()\n",
    "NAMESPACE = client.get_user_namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830b1ac0-ae9f-47e5-8896-1f3c1d7672ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/f5eabfa0-38e1-4828-a8ec-09d2adc66e22\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/9405bb18-df37-408f-96f3-19fb50d0423b\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=9405bb18-df37-408f-96f3-19fb50d0423b)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = client.create_run_from_pipeline_func(\n",
    "    pipeline_func=llm_pipeline,\n",
    "    arguments = pipeline_args, #{}\n",
    "    run_name = RUN_NAME,\n",
    "    pipeline_conf=pipeline_config,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    ")\n",
    "\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f38559-6dd4-40ad-91f6-85f3df4387ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192ac26-d24f-41ca-b726-b9122f53fb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3.10",
   "language": "python",
   "name": "llm3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
