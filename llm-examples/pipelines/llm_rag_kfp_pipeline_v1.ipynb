{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bde0f4-67cd-4a5a-af8c-abf0f89a15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0003cb5d-2144-4d17-9d9c-ca060d3acbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade --user kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01c1e77-1584-4cc6-80ee-95ea349396f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from functools import partial\n",
    "from kfp.dsl import (\n",
    "    pipeline,\n",
    "    ContainerOp,\n",
    "    PipelineVolume\n",
    ")\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    create_component_from_func\n",
    ")\n",
    "\n",
    "EXPERIMENT_NAME = 'llm' # Name of the experiment in the KF webapp UI\n",
    "EXPERIMENT_DESC = 'llm med report pipeline experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7bb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Settings():\n",
    "    llm_base_image: str = 'pytorch/pytorch:2.2.0-cuda11.8-cudnn8-devel'\n",
    "    applyllm_version: str = '0.0.2'\n",
    "    pypdf_version: str = '3.15.5'\n",
    "    accelerate_version: str = '0.26.1'\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7763edd5-3c2e-44c5-afb1-3983a46d0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"custom_registry_component.yaml\",\n",
    "    base_image=settings.llm_base_image, \n",
    "    packages_to_install=[\n",
    "        f\"applyllm=={settings.applyllm_version}\",\n",
    "        f\"pypdf=={settings.pypdf_version}\",\n",
    "        f\"accelerate=={settings.accelerate_version}\",\n",
    "    ], # adding additional libs\n",
    "    # pip_index_urls=[\"https://gitlab.lrz.de/api/v4/projects/150553/packages/pypi/simple\"]\n",
    "    # define my private pypi package registry v2 component decorator\n",
    ")\n",
    "def llm_op(model_root: str, \n",
    "           lm_model_type: str, \n",
    "           max_token_length: int, \n",
    "           max_position_embeddings: int,\n",
    "           max_new_tokens: int,\n",
    "           repetition_penalty: float,\n",
    "           temperature: float,\n",
    "           lm_device_map: str,\n",
    "           top_k: int,\n",
    "           top_p: float,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_root: The root directory of the model\n",
    "        lm_model_type: The type of the language model\n",
    "        max_token_length: The maximum token length 4096\n",
    "        max_position_embeddings: The maximum position embeddings 3072\n",
    "        max_new_tokens: The maximum new tokens to be generated 80\n",
    "        repetition_penalty: The repetition penalty 1.15\n",
    "        temperature: The temperature 0.001\n",
    "        lm_device_map: The device map for the language model \"auto\"\n",
    "        top_k: The top k value 3\n",
    "        top_p: The top p value 0.8\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import applyllm as apl\n",
    "\n",
    "    from applyllm.accelerators import (\n",
    "        AcceleratorHelper,\n",
    "        AcceleratorStatus,\n",
    "        DirectorySetting,\n",
    "        # DIR_MODE_MAP,\n",
    "        TokenHelper as th\n",
    "    )\n",
    "    from applyllm.utils import time_func\n",
    "    from applyllm.pipelines import (\n",
    "        LocalCausalLMConfig,\n",
    "        ModelConfig,\n",
    "        ModelCatalog,\n",
    "    )\n",
    "\n",
    "    dir_setting = DirectorySetting(home_dir=model_root)\n",
    "    \n",
    "    # debug code to check the mounted model_root, whether DirectorySetting is working\n",
    "    # print([x[0] for x in os.walk(model_root)])\n",
    "\n",
    "    gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "    '''init gpu helper'''\n",
    "    gpu_helper = AcceleratorHelper()\n",
    "    UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)\n",
    "    # will set the XDG_CACHE_HOME, this line must be called before import transformers\n",
    "    gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "    print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "    print(os.environ[\"XDG_CACHE_HOME\"])\n",
    "    '''init llm model to be loaded'''\n",
    "    model_map = {\n",
    "        \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "        \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "        \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    }\n",
    "    # model_type = \"mistral7B-inst02\"\n",
    "    model_type = lm_model_type\n",
    "    model_name = model_map.get(model_type, \"mistral7B-inst02\")\n",
    "    print(model_name)\n",
    "\n",
    "    import transformers\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from torch import bfloat16\n",
    "    \n",
    "    print(f\"applyllm version:     {apl.__version__}\")\n",
    "    print(f\"transformers version: {transformers.__version__}\")\n",
    "    print(f\"torch version:        {torch.__version__}\")\n",
    "\n",
    "    token_kwargs = th.gen_token_kwargs(model_type=model_type, dir_setting=dir_setting)\n",
    "    \n",
    "    \"\"\"Load CausalLM model\"\"\"\n",
    "    base_lm_config = ModelConfig(\n",
    "        model_config = {\n",
    "            \"pretrained_model_name_or_path\": model_name,\n",
    "            \"device_map\": \"auto\",\n",
    "            # \"max_memory\": f\"{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    lm_model_kwargs = {\n",
    "        \"quantized\": True,\n",
    "        \"model_config\": base_lm_config.get_config(),\n",
    "        \"quantization_config\": {\n",
    "            \"quantization_config\": transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=bfloat16\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "    lm_config = LocalCausalLMConfig(**lm_model_kwargs)\n",
    "\n",
    "    @time_func\n",
    "    def fetch_lm_model():\n",
    "        return AutoModelForCausalLM.from_pretrained(\n",
    "        **lm_config.get_config(),\n",
    "        **token_kwargs,  \n",
    "        )\n",
    "\n",
    "    model = fetch_lm_model()\n",
    "\n",
    "    gpu_status.gpu_usage()\n",
    "    \"\"\"Load CausalLM tokenizer\"\"\"\n",
    "\n",
    "    def config_tokenizer(model_name: str, config: dict, pad_token_id = 2):\n",
    "        if model_name.startswith(ModelCatalog.MISTRAL_FAMILY):\n",
    "            return {**config, \"pad_token_id\": pad_token_id}\n",
    "        else:\n",
    "            return config\n",
    "        \n",
    "    MAX_POSITION_EMBEDDINGS = max_position_embeddings\n",
    "    MAX_LENGTH = max_token_length\n",
    "\n",
    "    model_config= {\n",
    "        \"pretrained_model_name_or_path\": model_name,\n",
    "        \"device\": \"cpu\",\n",
    "        # \"device_map\": \"auto\", # put to GPU if GPU is available\n",
    "        \"max_position_embeddings\": MAX_LENGTH,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "    }\n",
    "    model_config = config_tokenizer(model_name=model_name, config=model_config)\n",
    "    tokenizer_config = ModelConfig(model_config=model_config)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        **tokenizer_config.get_config(), \n",
    "        **token_kwargs,\n",
    "    )\n",
    "\n",
    "    \"\"\"init the transformer pipeline as backend llm for langchain\"\"\"\n",
    "    tp_kwargs = {\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"device_map\": lm_device_map,\n",
    "        \"max_length\": None, # remove the total length of the generated response\n",
    "        \"max_new_tokens\": max_new_tokens, # set the size of new generated token \n",
    "    }\n",
    "\n",
    "    tp_config = ModelConfig(model_config = tp_kwargs)\n",
    "\n",
    "    generator = transformers.pipeline(\n",
    "        **tp_config.get_config(),\n",
    "        **token_kwargs,\n",
    "    )\n",
    "\n",
    "    \"\"\"Huggingface pipeline\"\"\"\n",
    "    from applyllm.pipelines import ModelCatalog, ModelInfo, PromptHelper\n",
    "\n",
    "    # model_info = ModelCatalog.get_model_info(model_name=model_name)\n",
    "    # prompt_helper = PromptHelper(model_info=model_info)\n",
    "\n",
    "    import langchain\n",
    "    from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "    print(f\"langchain.__version__: {langchain.__version__}\")\n",
    "\n",
    "    llm = HuggingFacePipeline(\n",
    "        pipeline=generator \n",
    "    )\n",
    "\n",
    "    llm.model_id = model_name\n",
    "    pipeline_kwargs_config = {\n",
    "        \"device_map\": lm_device_map,\n",
    "        \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "        \"max_new_tokens\": max_new_tokens, # this is not taken by the model ?\n",
    "        \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "        \"temperature\": temperature,\n",
    "        \"repetition_penalty\": repetition_penalty, # 1.15,\n",
    "    }\n",
    "    model_kwargs_config = {\n",
    "        \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "        \"top_k\": top_k, # this param result in trouble with langchain (optional)\n",
    "        \"num_return_sequences\": 1, # (optional)\n",
    "        \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "        \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "        \"max_new_tokens\": max_new_tokens, # this is not taken by the model ?\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p, # 0.95 # alternative to top_k summerized probability while do_sample=True\n",
    "        \"repetition_penalty\": repetition_penalty, # 1.15,\n",
    "        \"trust_remote_code\": True,\n",
    "    }\n",
    "\n",
    "    llm.model_kwargs = config_tokenizer(model_name=model_name, config=model_kwargs_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    llm.pipeline_kwargs = config_tokenizer(model_name=model_name, config=pipeline_kwargs_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(\"HuggingFacePipeline setup done\")\n",
    "    gpu_status.gpu_usage()\n",
    "\n",
    "    \"\"\"LangChain pipeline\"\"\"\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "    from langchain.vectorstores import DocArrayInMemorySearch\n",
    "    from langchain.indexes import VectorstoreIndexCreator\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "    # from langchain.text_splitter import TextSplitter\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain_core.documents.base import Document\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from typing import List\n",
    "    import boto3\n",
    "    from applyllm.io import S3PdfObjHelper, DocMetaInfo, DocCorpusS3\n",
    "\n",
    "    print(boto3.__version__)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9dc48c-75de-4812-8177-a27b5a414067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_res_limits(task: ContainerOp, mem_req=\"200Mi\", cpu_req=\"2000m\", mem_lim=\"4000Mi\", cpu_lim='4000m', gpu_req=None, gpu_lim=None, gpu_type:str=\"20gb\"):\n",
    "    \"\"\"\n",
    "    this function helps to set the resource limit for container operators\n",
    "    op.set_memory_limit('1000Mi') = 1GB\n",
    "    op.set_cpu_limit('1000m') = 1 cpu core\n",
    "    \"\"\"\n",
    "    if gpu_type == \"20gb\":\n",
    "        gpu_resource = \"nvidia.com/mig-2g.20gb\"\n",
    "        # gpu_resource = \"nvidia.com/mig-1g.20gb\"\n",
    "    elif gpu_type == \"40gb\":\n",
    "        gpu_resource = \"nvidia.com/mig-3g.40gb\"\n",
    "    else:\n",
    "        gpu_resource = \"nvidia.com/mig-1g.10gb\"\n",
    "        \n",
    "    # gpu_resource = \"nvidia.com/mig-2g.20gb\"\n",
    "    new_op = task.set_memory_request(mem_req)\\\n",
    "        .set_memory_limit(mem_lim)\\\n",
    "        .set_cpu_request(cpu_req)\\\n",
    "        .set_cpu_limit(cpu_lim)\n",
    "    if (gpu_req is not None) and (gpu_lim is not None):\n",
    "        new_op.add_resource_request(gpu_resource, gpu_req)\n",
    "        new_op.add_resource_limit(gpu_resource, gpu_lim)\n",
    "    return new_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2af7a2-4166-4ac5-84fb-93f24b49bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name = EXPERIMENT_NAME,\n",
    "    description = EXPERIMENT_DESC\n",
    ")\n",
    "def llm_pipeline(\n",
    "        model_root: str = \"/mnt\", \n",
    "        lm_model_type: str = \"mistral7B-inst02\", \n",
    "        max_token_length: int = 4096,\n",
    "        max_position_embeddings: int = 3072,\n",
    "        max_new_tokens: int = 80,\n",
    "        repetition_penalty: float = 1.15,\n",
    "        temperature: float = 0.001,\n",
    "        lm_device_map: str = \"auto\",\n",
    "        top_k: int = 3,\n",
    "        top_p: float = 0.8,\n",
    "        gpu_type: str = \"20gb\"\n",
    "    ):\n",
    "    '''local variable'''\n",
    "    no_artifact_cache = \"P0D\"\n",
    "    artifact_cache_today = \"P1D\"\n",
    "    # cache_setting = artifact_cache_today\n",
    "    cache_setting = no_artifact_cache\n",
    "\n",
    "    '''Pipeline Volume'''\n",
    "    shared_volume = PipelineVolume(\"llm-models\")\n",
    "    \n",
    "    '''pipeline'''   \n",
    "    llm_task = llm_op(\n",
    "        model_root=model_root, \n",
    "        lm_model_type=lm_model_type,\n",
    "        max_token_length=max_token_length,\n",
    "        max_position_embeddings=max_position_embeddings,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        temperature=temperature,\n",
    "        lm_device_map=lm_device_map,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        )\n",
    "    # 200 MB ram and 1 cpu\n",
    "    llm_task = set_res_limits(task=llm_task, mem_req=\"20Gi\", mem_lim=\"40Gi\",\n",
    "                            cpu_req=\"2000m\", cpu_lim=\"10000m\", \n",
    "                            gpu_req=1, gpu_lim=1, gpu_type=gpu_type)\n",
    "    # set the download caching to be 1day, disable caching with P0D\n",
    "    # download_task.execution_options.caching_strategy.max_cache_staleness = artifact_cache_today\n",
    "    llm_task.add_pvolumes({model_root: shared_volume})\n",
    "    llm_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    llm_task.set_display_name(\"llm op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739fe18f-2384-4bbb-a9bb-7e284b028903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pipeline_path_dir=\"./compiled\"\n",
    "if not os.path.exists(pipeline_path_dir):\n",
    "    os.makedirs(pipeline_path_dir)\n",
    "\n",
    "PIPE_LINE_FILE_NAME=f\"llm_rag_kfp_pipeline\"\n",
    "kfp.compiler.Compiler().compile(llm_pipeline, f\"{pipeline_path_dir}/{PIPE_LINE_FILE_NAME}.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934304c5-326f-4d67-85d5-9920d1683590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "\n",
    "def get_local_time_str(target_tz_str: str = \"Europe/Berlin\", format_str: str = \"%Y-%m-%d %H-%M-%S\") -> str:\n",
    "    \"\"\"\n",
    "    this method is created since the local timezone is miss configured on the server\n",
    "    @param: target timezone str default \"Europe/Berlin\"\n",
    "    @param: \"%Y-%m-%d %H-%M-%S\" returns 2022-07-07 12-08-45\n",
    "    \"\"\"\n",
    "    target_tz = ptimezone(target_tz_str) # create timezone, in python3.9 use standard lib ZoneInfo\n",
    "    # utc_dt = datetime.now(datetime.timezone.utc)\n",
    "    target_dt = datetime.now(target_tz)\n",
    "    return datetime.strftime(target_dt, format_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2118ac78-8984-475f-9c0b-d4c530e60ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kubernetes import client as k8s_client\n",
    "pipeline_config = dsl.PipelineConf()\n",
    "\n",
    "# pipeline_config.set_image_pull_secrets([k8s_client.V1ObjectReference(name=K8_GIT_SECRET_NAME, namespace=NAME_SPACE)])\n",
    "# pipeline_config.set_image_pull_policy(\"Always\")\n",
    "pipeline_config.set_image_pull_policy(\"IfNotPresent\")\n",
    "\n",
    "pipeline_args = {\n",
    "    \"model_root\": \"/mnt\",\n",
    "    # \"lm_model_type\": \"llama13B-chat\",\n",
    "    \"lm_model_type\": \"mistral7B-inst02\", # \"llama13B-chat\",\n",
    "    \"max_token_length\": 4096, # for llama2 models max_length is 4096\n",
    "    \"max_position_embeddings\": 3072, # for llama2 models, using chunk size of 3072\n",
    "    \"max_new_tokens\": 80, # the maximum new tokens to be generated by the causalLM \n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"temperature\": 0.001,\n",
    "    \"lm_device_map\": \"auto\",\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 0.8,\n",
    "    \"gpu_type\": \"20gb\", # \"40gb\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a646a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = f\"{PIPE_LINE_FILE_NAME} {get_local_time_str()}\"\n",
    "\n",
    "client = kfp.Client()\n",
    "NAMESPACE = client.get_user_namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830b1ac0-ae9f-47e5-8896-1f3c1d7672ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/f5eabfa0-38e1-4828-a8ec-09d2adc66e22\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/cb4bae2d-e676-447d-89f3-fa9bf21ac0c2\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=cb4bae2d-e676-447d-89f3-fa9bf21ac0c2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = client.create_run_from_pipeline_func(\n",
    "    pipeline_func=llm_pipeline,\n",
    "    arguments = pipeline_args, #{}\n",
    "    run_name = RUN_NAME,\n",
    "    pipeline_conf=pipeline_config,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    ")\n",
    "\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f38559-6dd4-40ad-91f6-85f3df4387ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192ac26-d24f-41ca-b726-b9122f53fb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3.10",
   "language": "python",
   "name": "llm3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
