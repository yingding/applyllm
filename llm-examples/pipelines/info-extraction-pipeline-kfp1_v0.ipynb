{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df4f1a2-5690-41c6-a061-274545e459a3",
   "metadata": {},
   "source": [
    "# About this Jupyter Notebook\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@updated: 08.09.2023\n",
    "\n",
    "This notebook defines and runs a kubeflow pipeline with KFP python SDK v1 for using LlaMA2 and T5 based de_en translatition models to extract information from a non-structured PDF data.\n",
    "\n",
    "The prompt is specially constructed to extract \"patient name\" and \"patient age\" information from a doctor's letter.\n",
    "\n",
    "Notice:\n",
    "```\n",
    "urllib3 1.26.16 works with google auth, see any http google auth error, reinstall kfp=1.8.22\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357cf343-6af6-4ba3-b48d-5985633a45e2",
   "metadata": {},
   "source": [
    "## Install KFP Python SDK to build a V1 pipeline\n",
    "* Build KF pipeline with python SDK: https://www.kubeflow.org/docs/components/pipelines/sdk/build-pipeline/\n",
    "* Current KFP python SDK version on pypi.org: https://pypi.org/project/kfp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3e4c76-43fd-460d-9457-20687bc552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792bac4d-9de6-4b2d-975b-ca59b5ecfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip uninstall -y kfp-server-api\n",
    "#!{sys.executable} -m pip install --user --upgrade kfp-server-api==1.8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e083866-51d7-400f-9cfd-1790675a6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade --user kfp==2.0.0b13\n",
    "#!{sys.executable} -m pip install --upgrade --user kfp==1.8.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acbec4-7f31-4cbf-bc8a-b852271b4884",
   "metadata": {},
   "source": [
    "## Restart the Kernel\n",
    "\n",
    "After the installation of KFP python SDK, the notebook kernel must be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f3811-f3c1-45f2-8476-719a937caadd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting familiar with Jupyter Notebook ENV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5c31cc-0869-48eb-9587-89df6aff7aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current platform python version: 3.8.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print (f\"current platform python version: {python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c90732-087d-46de-9a92-c6eb85e8a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                                                         kf-resource-quota\n",
      "Namespace:                                                    kubeflow-kindfor\n",
      "Resource                                                      Used     Hard\n",
      "--------                                                      ----     ----\n",
      "basic-csi.storageclass.storage.k8s.io/persistentvolumeclaims  5        15\n",
      "basic-csi.storageclass.storage.k8s.io/requests.storage        135Gi    150Gi\n",
      "cpu                                                           2090m    128\n",
      "longhorn.storageclass.storage.k8s.io/persistentvolumeclaims   1        15\n",
      "longhorn.storageclass.storage.k8s.io/requests.storage         250Gi    500Gi\n",
      "memory                                                        24966Mi  512Gi\n",
      "requests.nvidia.com/mig-1g.10gb                               0        2\n",
      "requests.nvidia.com/mig-1g.20gb                               0        1\n",
      "requests.nvidia.com/mig-2g.20gb                               1        1\n"
     ]
    }
   ],
   "source": [
    "# run kubectl command line to see the quota in the name space\n",
    "!kubectl describe quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c70007-489a-49d2-baff-eaeaa7a984a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mkfp                       1.8.22\n",
      "kfp-pipeline-spec         0.1.16\n",
      "kfp-server-api            1.8.5\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# examing the kfp python sdk version inside a KubeFlow v1.5.1\n",
    "!{sys.executable} -m pip list | grep kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c986797-6418-4f13-a439-7dfe2e6c09a3",
   "metadata": {},
   "source": [
    "## Setup global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8bcd262-cbbb-4914-90f0-9c6dc85d5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubeflow-kindfor\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "client = kfp.Client()\n",
    "NAMESPACE = client.get_user_namespace()\n",
    "EXPERIMENT_NAME = 'scivias' # Name of the experiment in the KF webapp UI\n",
    "EXPERIMENT_DESC = 'extract information from doctors letter'\n",
    "PREFIX = \"llm\"\n",
    "DATA_ROOT = \"/mnt\"\n",
    "DATA_SUB_PATH = \"core-kind/yinwang\"\n",
    "FILE_SUB_PATH = f\"{DATA_SUB_PATH}/data/medreports\"\n",
    "FILE_PATTERN = \"KK-SCIVIAS-*.pdf\"\n",
    "DEFAULT_GEN_MODEL_TYPE = \"7B\"\n",
    "DEFAULT_TRANS_MODEL_TYPE = \"custom\"\n",
    "\n",
    "print(NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f8e8f5-e74d-4128-b6d1-5d0b371a455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings(base_torch_image='harbor-dmz.srv.med.uni-muenchen.de/core-general/ngc:0.0.0', pandas='pandas==1.5.3', pypdf='pypdf==3.15.5', pyarrow='pyarrow==10.0.0')\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "'''\n",
    "cudf 23.2.0 requires pandas<1.6.0dev0,>=1.0, but you have pandas 2.0.3 which is incompatible.\n",
    "dask-cudf 23.2.0 requires pandas<1.6.0dev0,>=1.0, but you have pandas 2.0.3 which is incompatible.\n",
    "'''\n",
    "@dataclass\n",
    "class Settings:\n",
    "    base_torch_image: str = \"harbor-dmz.srv.med.uni-muenchen.de/core-general/ngc:0.0.0\"\n",
    "    pandas: str = \"pandas==1.5.3\" # < 2.0.3 by cudf and dash-cudf\n",
    "    pypdf: str = \"pypdf==3.15.5\"\n",
    "    pyarrow: str = \"pyarrow==10.0.0\"\n",
    "\n",
    "    \n",
    "settings = Settings() \n",
    "print(f\"{settings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b18f24-d264-4352-aaf0-e1ee8adaadfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating KubeFlow component from python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3226c3-b0d9-4c2b-9f48-2927f6433aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import kfp dsl components\n",
    "import kfp.dsl as dsl\n",
    "from functools import partial\n",
    "from kfp.dsl import (\n",
    "    pipeline,\n",
    "    ContainerOp,\n",
    "    PipelineVolume\n",
    ")\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    InputBinaryFile, \n",
    "    OutputBinaryFile,\n",
    "    create_component_from_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a97a7e-1064-432e-b465-d14f560f540b",
   "metadata": {},
   "source": [
    "#### PDF text_extractor component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00c4e709-83f6-41e2-bc78-28d6ed129e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}_text_extraction_component.yaml\",\n",
    "    base_image=\"python:3.8.18\", # settings.base_torch_image, # use pt base image\n",
    "    packages_to_install=[\n",
    "        settings.pandas,\n",
    "        settings.pypdf,\n",
    "        settings.pyarrow,\n",
    "        #\"pypdf==3.15.5\",\n",
    "        #\"pyarrow==10.0.0\"\n",
    "    ],# adding additional libs\n",
    ")\n",
    "def text_extractor(\n",
    "    data_root: str, \n",
    "    data_sub_path: str, \n",
    "    file_pattern: str,\n",
    "    file_idx: int,\n",
    "    show_log_txt: bool,\n",
    "    output_path: OutputPath(\"Dataset\")):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      file_idx: the idx of the file to extract text\n",
    "    \"\"\"\n",
    "    import sys, os, glob\n",
    "    from pypdf import PdfReader\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame, Series\n",
    "    \n",
    "    class PDFHelper():\n",
    "        def __init__(self, data_folder: str, file_pattern: str):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "              data_folder: where all the pdf files are\n",
    "              file_pattern: the pdf files match this pattern\n",
    "\n",
    "            Examples:\n",
    "\n",
    "              PDFHelper(data_folder = \"./data/medreports\", \n",
    "                        file_pattern=\"KK-SCIVIAS-*.pdf\")\n",
    "            \"\"\"\n",
    "            self.data_folder = data_folder\n",
    "            self.file_pattern = file_pattern\n",
    "            self.dir_path = f\"{data_folder}/{file_pattern}\"\n",
    "            self.file_path_list = glob.glob(self.dir_path)\n",
    "\n",
    "        # @multifunction(None, str)\n",
    "        def read_pdf(self, input) -> str:\n",
    "            \"\"\"read from the give path the text and returns a raw string. \n",
    "               use print to print the content\n",
    "            \"\"\"\n",
    "            if isinstance(input, str):\n",
    "                file_path = input\n",
    "                reader = PdfReader(file_path)\n",
    "                print(f\"read pages: {len(reader.pages)}\")\n",
    "                content_raw_str = \"\".join([page.extract_text() for page in reader.pages])\n",
    "                return content_raw_str\n",
    "            elif isinstance(input, int):\n",
    "                file_idx = input\n",
    "                if (file_idx < len(self.file_path_list)):\n",
    "                    return self.read_pdf(str(self.file_path_list[file_idx]))\n",
    "                else:\n",
    "                    return \"\"\n",
    "            else:\n",
    "                return \"\"\n",
    "\n",
    "            \n",
    "        # @multifunction(None, str)\n",
    "        def count_token(self, input)-> int:\n",
    "            \"\"\"count the total token in a pdf file\n",
    "            \"\"\"\n",
    "            if isinstance(input, str):\n",
    "                file_path = input\n",
    "                token_size = len(self.read_pdf(file_path))\n",
    "                print(f\"file: {file_path}\\n\" + \n",
    "                  f\"total token: {token_size}\")\n",
    "                return token_size\n",
    "            elif isinstance(input, int):\n",
    "                file_idx = input\n",
    "                if (file_idx < len(self.file_path_list)):\n",
    "                    return self.count_token(str(self.file_path_list[file_idx]))\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "\n",
    "        def read_txt(self, input) -> str:\n",
    "            \"\"\"read from the give path the text and returns a raw string. \n",
    "               use print to print the content\n",
    "            \"\"\"\n",
    "            if isinstance(input, str):\n",
    "                file_path = input\n",
    "                with open(file_path, \"r\") as txt_file:\n",
    "                    content_raw_str = txt_file.read()\n",
    "                return content_raw_str\n",
    "            elif isinstance(input, int):\n",
    "                file_idx = input\n",
    "                if (file_idx < len(self.file_path_list)):\n",
    "                    return self.read_txt(str(self.file_path_list[file_idx]))\n",
    "                else:\n",
    "                    return \"\"\n",
    "            else:\n",
    "                return \"\"\n",
    "    \n",
    "    \n",
    "    '''helper function'''\n",
    "    def show_folder_files(folder: str) -> None:\n",
    "        print(os.listdir(folder))\n",
    "        \n",
    "    \n",
    "    def save_txt_to_feather(txt_list: list, path: str, col_names=[\"text\"]) -> None:\n",
    "        with open(path, \"w\") as file:\n",
    "            ser = Series(txt_list)\n",
    "            df = DataFrame(data=Series(txt_list), columns=col_names)\n",
    "            df.to_feather(path)\n",
    "            \n",
    "            \n",
    "    def log_txt(txt: str, debug: bool) -> None:\n",
    "        if debug:\n",
    "            print(\"Reading string content...\")\n",
    "            print(\"-\"*20)\n",
    "            print(txt)        \n",
    "        \n",
    "        \n",
    "    '''Global variable'''\n",
    "    # file_idx = 1 # which file to read\n",
    "    data_path = f\"{data_root}/{data_sub_path}\"\n",
    "    show_folder_files(data_path)\n",
    "    loader = PDFHelper(data_folder = data_path, file_pattern=file_pattern)\n",
    "    # read only the second file, which is short\n",
    "    file_content = loader.read_pdf(file_idx)\n",
    "    print(f\"all files in directory:\\n{loader.file_path_list}\")\n",
    "    print(f\"current loading file: {loader.file_path_list[file_idx]}\")\n",
    "    # show content in component logs\n",
    "    log_txt(txt = file_content, debug=show_log_txt)\n",
    "    # save to output\n",
    "    save_txt_to_feather(file_content, output_path)\n",
    "        \n",
    "    # return file_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f3b82-2971-4ad2-93fd-edb6e4852fc6",
   "metadata": {},
   "source": [
    "#### translate_bart component\n",
    "\n",
    "While BERT was trained by using a simple token masking technique, BART empowers the BERT encoder by using more challenging kinds of masking mechanisms in its pre-training.\n",
    "\n",
    "* loop with index using enumerate: https://treyhunner.com/2016/04/how-to-loop-with-indexes-in-python/\n",
    "* BART : Generalizing BERT (due to the bidirectional encoder) and GPT2 (with the left to right decoder) : https://www.projectpro.io/article/transformers-bart-model-explained/553\n",
    "* MarianMT on huggingface (BART): https://huggingface.co/docs/transformers/model_doc/marian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70c4081-a5db-4684-8717-67eab3067e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}_trans_component.yaml\",\n",
    "    base_image=\"tensorflow/tensorflow:2.12.0\", # cpu version, small as pytorch https://hub.docker.com/r/pytorch/pytorch/tags?page=1&name=2.0.1\n",
    "    # base_image=\"python:3.8.18\", # settings.base_torch_image, # use pt base image\n",
    "    packages_to_install=[\n",
    "        \"transformers==4.31.0\",\n",
    "        \"sacremoses==0.0.53\",\n",
    "        \"sentencepiece==0.1.99\",\n",
    "        settings.pandas,\n",
    "        settings.pyarrow,\n",
    "        # \"pandas==2.0.3\"\n",
    "        #\"https://download.pytorch.org/whl/cu117/torch==2.0.1+cu117\",\n",
    "    ],# adding additional libs\n",
    ")\n",
    "def bart_translator(\n",
    "    data_root: str, \n",
    "    data_sub_path: str, \n",
    "    model_type: str,\n",
    "    show_log_txt: bool,\n",
    "    # origin_text: str, \n",
    "    input_path: InputPath(\"Dataset\"),\n",
    "    output_path: OutputPath(\"Dataset\")\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      input_path: feather binary encoded text str to be translated from german to english\n",
    "      output_path: feather binary encoded the translated english text\n",
    "    \"\"\"\n",
    "    import subprocess, os, re, sys, time\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame, Series\n",
    "    \n",
    "    class GPUInfoHelper():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "\n",
    "        def byte_gb_info(self, byte_mem) -> str:\n",
    "            \"\"\"calculate the byte size to GB size for better human readable\"\"\"\n",
    "            # format the f string float with :.2f to decimal digits\n",
    "            # https://zetcode.com/python/fstring/\n",
    "            return f\"{(byte_mem/1024**3):4f} GB\"\n",
    "\n",
    "\n",
    "        def accelerator_mem_info(self, device_idx: int):\n",
    "            # total\n",
    "            t = torch.cuda.get_device_properties(device_idx).total_memory\n",
    "            # usable\n",
    "            r = torch.cuda.memory_reserved(device_idx)\n",
    "            # allocated\n",
    "            a = torch.cuda.memory_allocated(device_idx)\n",
    "            # still free\n",
    "            f = r-a  \n",
    "            print( # \"GPU memory info:\\n\" + \n",
    "                  f\"Physical  memory : {self.byte_gb_info(t)}\\n\" + \n",
    "                  f\"Reserved  memory : {self.byte_gb_info(r)}\\n\" + \n",
    "                  f\"Allocated memory : {self.byte_gb_info(a)}\\n\" + \n",
    "                  f\"Free      memory : {self.byte_gb_info(f)}\")\n",
    "\n",
    "\n",
    "        def accelerator_compute_info(self, device_idx: int) -> None:\n",
    "            name = torch.cuda.get_device_properties(device_idx).name\n",
    "            count = torch.cuda.get_device_properties(device_idx).multi_processor_count\n",
    "            print(f\"Device_name      : {name} \\n\" +\n",
    "                  f\"Multi_processor  : {count}\")    \n",
    "\n",
    "\n",
    "        def gpu_usage(self) -> None:        \n",
    "            num_of_gpus = torch.cuda.device_count();\n",
    "            # this shows only the gpu device, not the MIG\n",
    "            print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "            for device_idx in range(torch.cuda.device_count()):\n",
    "                print(\"-\"*20)\n",
    "                self.accelerator_compute_info(device_idx)                 \n",
    "                self.accelerator_mem_info(device_idx)\n",
    "                print(\"-\"*20)\n",
    "     \n",
    "    \n",
    "    # further\n",
    "    def display_container_info():\n",
    "        print(\"-\"*10)\n",
    "        print(f\"python version: {sys.version}\")\n",
    "        # print(f\"torch version: {torch.__version__}\")\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "    \n",
    "    def nvidia_device_uuid(input: str):\n",
    "        \"\"\"parse the nvidia devices uuid from the nvidia device info str\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # r'' before the search pattern indicates it is a raw string, \n",
    "            # otherwise \"\" instead of single quote\n",
    "            uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "        except AttributeError:\n",
    "            # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "            uuid = \"\"\n",
    "        return uuid\n",
    "    \n",
    "    \n",
    "    def nvidia_device_info() -> str:\n",
    "        \"\"\"get the nvidia MIGs device uuid and GPU uuid \n",
    "        \"\"\"\n",
    "        # blocking call\n",
    "        result = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE)\n",
    "        # decode the byte object, returns string with \\n\n",
    "        cmd_out_str = result.stdout.decode('utf-8')\n",
    "        return [line.strip() for line in cmd_out_str.split('\\n') if len(line) > 0]\n",
    "        \n",
    "    \n",
    "    def nvidia_mig_uuids() -> str:\n",
    "        \"\"\"get a comma separated str of nvidia MIGs devices\n",
    "        \"\"\"\n",
    "        info_list = nvidia_device_info()\n",
    "        # skip the first GPU ID, get the MIGs IDS\n",
    "        uuid_list = [nvidia_device_uuid(e) for e in info_list[1:]]\n",
    "        # if multi gpus need to join the device together for pytorch\n",
    "        return \",\".join(uuid_list)\n",
    "    \n",
    "    \n",
    "    def init_cuda_torch(uuids: str, data_path: str) -> None:\n",
    "        \"\"\"setup the default env variables for transformers\n",
    "        \n",
    "        Args:\n",
    "          uuids: a comma separate str of nvidia gpu/mig uuids\n",
    "        \"\"\"\n",
    "        os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = uuids \n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "        init_transformers()\n",
    "        \n",
    "    def init_transformers() -> None:\n",
    "        os.environ['XDG_CACHE_HOME']=f\"{data_path}/models\"\n",
    "        \n",
    "        \n",
    "    def show_folder_files(folder: str) -> None:\n",
    "        print(os.listdir(folder))\n",
    "        \n",
    "    \n",
    "    # https://stackoverflow.com/questions/13673060/split-string-into-strings-by-length\n",
    "    def wrap(s, w):\n",
    "        \"\"\"\n",
    "        split string with length w into a list of strings with length w\n",
    "        Arge:\n",
    "          s: orginial str\n",
    "          w: with of the each split for the string\n",
    "\n",
    "        Return:\n",
    "          a list of string with each element as string of length w\n",
    "        \"\"\"\n",
    "        return [s[i:i + w] for i in range(0, len(s), w)]\n",
    "\n",
    "    \n",
    "    def save_txt_to_feather(txt_list: list, path: str, col_names=[\"text\"]) -> None:\n",
    "        with open(path, \"w\") as file:\n",
    "            ser = Series(txt_list)\n",
    "            df = DataFrame(data=Series(txt_list), columns=col_names)\n",
    "            df.to_feather(path)\n",
    "    \n",
    "    \n",
    "    def load_feather(path: str, col_names=[\"text\"]) -> DataFrame:\n",
    "        with open(path, \"r\") as file:\n",
    "            df = pd.read_feather(path)\n",
    "        if df is not None:\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        \n",
    "    def log_txt(txt: str, debug: bool) -> None:\n",
    "        if debug:\n",
    "            print(\"Reading string content...\")\n",
    "            print(\"-\"*20)\n",
    "            print(txt)  \n",
    "        \n",
    "    \n",
    "    '''Global variable'''\n",
    "    model_map = {\n",
    "       #\"small\": \"google/mt5-small\", # 1.2 GB\n",
    "       #\"base\" : \"google/mt5-base\", # 2.33 GB\n",
    "       #\"large\" : \"google/mt5-large\", # 4.9 GB,\n",
    "       #\"xl\" : \"google/mt5-xl\", # 15 GB\n",
    "       #\"xxl\" : \"google/mt5-xxl\", # 51.7 GB,\n",
    "       \"custom\": \"Helsinki-NLP/opus-mt-de-en\", \n",
    "    }\n",
    "    data_path = f\"{data_root}/{data_sub_path}\"\n",
    "    model_name = model_map.get(model_type, \"custom\")\n",
    "    # nvidia_state = GPUInfoHelper()\n",
    "    split_length = 350 # the token to split for text\n",
    "    \n",
    "    # load feather DataFrame\n",
    "    feather_text_col_name = \"text\"\n",
    "    df = load_feather(input_path)\n",
    "    # get the row series and the column\n",
    "    origin_text = str(df.iloc[0][feather_text_col_name])\n",
    "    print(f\"origin_text has token length: {len(origin_text)}\")\n",
    "    \n",
    "    '''Initialization'''\n",
    "    # UUIDs = nvidia_mig_uuids()\n",
    "    init_transformers()\n",
    "    # init_cuda_torch(UUIDs, data_path)\n",
    "    # import torch\n",
    "    display_container_info()\n",
    "    # print(UUIDs)\n",
    "    \n",
    "    show_folder_files(data_path)\n",
    "    \n",
    "    # transformers must be imported after the init_cuda_torch so that the env variable will be set properly\n",
    "    from transformers import pipeline\n",
    "    import transformers\n",
    "    \n",
    "    print(f\"Loading LLM model {model_name} ...\")\n",
    "    # using the accelerator with id by default, the device_map=\"auto\" doesn't work,\n",
    "    # model is too outdated to use auto accelerator detection. \n",
    "    accelerator_id = 0\n",
    "    generator = pipeline(\n",
    "        \"translation\", \n",
    "        model=model_name,\n",
    "        # device_map=\"auto\",\n",
    "        device=accelerator_id,\n",
    "    )\n",
    "        \n",
    "    def translate_gen(\n",
    "        generator: transformers.pipelines.text2text_generation.TranslationPipeline, \n",
    "        nvidia_state: GPUInfoHelper = None,\n",
    "    ):  \n",
    "        \"\"\"\n",
    "        Args:\n",
    "          max_new_tokens: control the maximum length of the generation\n",
    "        \"\"\"\n",
    "\n",
    "        def local(sentences: list, print_mode: bool = True, max_length=400) -> list:\n",
    "            \"\"\"single input, no batch input\n",
    "            Args:\n",
    "              sentences:\n",
    "            \"\"\"\n",
    "            start = time.time()\n",
    "            result = generator(sentences, max_length=max_length)\n",
    "            end = time.time()\n",
    "            duration = end - start\n",
    "            if print_mode: \n",
    "                print(\"-\"*20)\n",
    "                print(f\"walltime: {duration} in secs.\")\n",
    "                if nvidia_state is not None:\n",
    "                    nvidia_state.gpu_usage()\n",
    "            return result\n",
    "        return local    \n",
    "    \n",
    "    # create the convenient translate function\n",
    "    # translate = translate_gen(generator, nvidia_state)\n",
    "    \n",
    "    translate = translate_gen(generator, None)\n",
    "    # translate input\n",
    "    splitted_content = wrap(origin_text, split_length)\n",
    "    split_count = len(splitted_content)\n",
    "    output = []\n",
    "    for idx, split_text in enumerate(splitted_content, start=1):\n",
    "        print_mode = (idx == 1 or idx == split_count)\n",
    "        print(f\"print_mode: {print_mode}\")\n",
    "        output.append(translate(split_text, print_mode, 1000)[0].get('translation_text', '').strip())\n",
    "    \n",
    "    en_content = ''.join(output)\n",
    "    print(f\"translated en_content text has token length: {len(en_content)}\")\n",
    "    # show content in component logs\n",
    "    log_txt(txt = en_content, debug=show_log_txt)\n",
    "    save_txt_to_feather(en_content, output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83427c5f-3bdb-40d4-8d44-3fef37d4e1c1",
   "metadata": {},
   "source": [
    "### Create llm inference component\n",
    "\n",
    "#### Subprocess call to pass the nvidia-smi output\n",
    "\n",
    "* Python 3.5 subprocess.run https://stackoverflow.com/questions/4760215/running-shell-command-and-capturing-the-output\n",
    "* https://stackoverflow.com/questions/7681715/whats-the-difference-between-subprocess-popen-and-call-how-can-i-use-them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfb109-c492-4e79-aa07-a25886588cfa",
   "metadata": {},
   "source": [
    "#### Issue\n",
    "\n",
    "```console\n",
    "RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n",
    "/usr/local/lib/python3.8/dist-packages/transformer_engine_extensions.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
    "Error: exit status 1\n",
    "```\n",
    "Workaround:\n",
    "* https://github.com/microsoft/TaskMatrix/issues/116\n",
    "\n",
    "\n",
    "* If a None object is returned, and the component shall return as string will receive an error: https://github.com/kubeflow/pipelines/issues/8868"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd5142-0cb7-4a3b-9cd9-e0a9d0e383fd",
   "metadata": {},
   "source": [
    "#### Example of batch Gen\n",
    "\n",
    "* https://github.com/huggingface/transformers/issues/18478#issuecomment-1208049618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf024d03-f8aa-486e-8157-1c3c3cc92068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}_inference_component.yaml\",\n",
    "    base_image=settings.base_torch_image, # use pt base image\n",
    "    packages_to_install=[\n",
    "        # \"transformers==4.32.1\",\n",
    "        \"xformers==0.0.21\",\n",
    "        \"huggingface_hub==0.17.1\", \n",
    "        # \"accelerate==0.21.0\", # bug in accelerate 0.22.0 which runs on cpu only https://discuss.huggingface.co/t/could-not-load-model-meta-llama-llama-2-7b-chat-hf-with-any-of-the-following-classes/47641\n",
    "        settings.pandas,\n",
    "        settings.pyarrow,\n",
    "        #\"https://download.pytorch.org/whl/cu117/torch==2.0.1+cu117\",\n",
    "        #\"https://download.pytorch.org/whl/cu117/torchvision==0.15.2+cu117\",\n",
    "        #\"https://download.pytorch.org/whl/cu117/torchaudio==2.0.2\"\n",
    "    ],# adding additional libs\n",
    ")\n",
    "def llm_gen(\n",
    "    data_root: str, \n",
    "    data_sub_path: str, \n",
    "    model_type: str,\n",
    "    prompt_templates: list,\n",
    "    #prompt_context: str,\n",
    "    prompt_placeholder: str,\n",
    "    # prompt: str,\n",
    "    prompt_context_path: InputPath(\"Dataset\"),\n",
    "    show_log_txt: bool=False) -> NamedTuple(\"output\", [(\"answers\", list)]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      prompt_templates: list of prompt string template, which can be extended with prompt_context by the promplt_placeholder\n",
    "      prompt_context_path (prompt_context): additional context string passed as feather binary, which can be injected to the prompts, the path is dropped during the kfp compiling\n",
    "      prompt_placeholder: the special charactor used in the prompts to be replaced by the prompt_context\n",
    "    \"\"\"\n",
    "    from collections import namedtuple \n",
    "    import subprocess\n",
    "    import os, time, sys, re\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame, Series\n",
    "    # https://github.com/huggingface/transformers/issues/23340\n",
    "    # subprocess.call([\"pip\", \"uninstall\", \"-y\", \"transformer-engine\"])\n",
    "    \n",
    "    class GPUInfoHelper():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "\n",
    "        def byte_gb_info(self, byte_mem) -> str:\n",
    "            \"\"\"calculate the byte size to GB size for better human readable\"\"\"\n",
    "            # format the f string float with :.2f to decimal digits\n",
    "            # https://zetcode.com/python/fstring/\n",
    "            return f\"{(byte_mem/1024**3):4f} GB\"\n",
    "\n",
    "\n",
    "        def accelerator_mem_info(self, device_idx: int):\n",
    "            # total\n",
    "            t = torch.cuda.get_device_properties(device_idx).total_memory\n",
    "            # usable\n",
    "            r = torch.cuda.memory_reserved(device_idx)\n",
    "            # allocated\n",
    "            a = torch.cuda.memory_allocated(device_idx)\n",
    "            # still free\n",
    "            f = r-a  \n",
    "            print( # \"GPU memory info:\\n\" + \n",
    "                  f\"Physical  memory : {self.byte_gb_info(t)}\\n\" + \n",
    "                  f\"Reserved  memory : {self.byte_gb_info(r)}\\n\" + \n",
    "                  f\"Allocated memory : {self.byte_gb_info(a)}\\n\" + \n",
    "                  f\"Free      memory : {self.byte_gb_info(f)}\")\n",
    "\n",
    "\n",
    "        def accelerator_compute_info(self, device_idx: int) -> None:\n",
    "            name = torch.cuda.get_device_properties(device_idx).name\n",
    "            count = torch.cuda.get_device_properties(device_idx).multi_processor_count\n",
    "            print(f\"Device_name      : {name} \\n\" +\n",
    "                  f\"Multi_processor  : {count}\")    \n",
    "\n",
    "\n",
    "        def gpu_usage(self) -> None:        \n",
    "            num_of_gpus = torch.cuda.device_count();\n",
    "            # this shows only the gpu device, not the MIG\n",
    "            print(f\"num_of_gpus: {num_of_gpus}\")\n",
    "            for device_idx in range(torch.cuda.device_count()):\n",
    "                print(\"-\"*20)\n",
    "                self.accelerator_compute_info(device_idx)                 \n",
    "                self.accelerator_mem_info(device_idx)\n",
    "                print(\"-\"*20)\n",
    "    \n",
    "    \n",
    "    def display_container_info():\n",
    "        print(\"-\"*10)\n",
    "        print(f\"python version: {sys.version}\")\n",
    "        print(f\"torch version: {torch.__version__}\")\n",
    "        print(\"-\"*10)\n",
    "    \n",
    "    \n",
    "    def nvidia_device_uuid(input: str):\n",
    "        \"\"\"parse the nvidia devices uuid from the nvidia device info str\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # r'' before the search pattern indicates it is a raw string, \n",
    "            # otherwise \"\" instead of single quote\n",
    "            uuid = re.search(r'UUID\\:\\s(.+?)\\)', input).group(1)\n",
    "        except AttributeError:\n",
    "            # \"UUID\\:\\s\" and \"\\)\" not found\n",
    "            uuid = \"\"\n",
    "        return uuid\n",
    "    \n",
    "    \n",
    "    def nvidia_device_info() -> str:\n",
    "        \"\"\"get the nvidia MIGs device uuid and GPU uuid \n",
    "        \"\"\"\n",
    "        # blocking call\n",
    "        result = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE)\n",
    "        # decode the byte object, returns string with \\n\n",
    "        cmd_out_str = result.stdout.decode('utf-8')\n",
    "        return [line.strip() for line in cmd_out_str.split('\\n') if len(line) > 0]\n",
    "        \n",
    "    \n",
    "    def nvidia_mig_uuids() -> str:\n",
    "        \"\"\"get a comma separated str of nvidia MIGs devices\n",
    "        \"\"\"\n",
    "        info_list = nvidia_device_info()\n",
    "        # skip the first GPU ID, get the MIGs IDS\n",
    "        uuid_list = [nvidia_device_uuid(e) for e in info_list[1:]]\n",
    "        # if multi gpus need to join the device together for pytorch\n",
    "        return \",\".join(uuid_list)\n",
    "    \n",
    "    \n",
    "    def init_cuda_torch(uuids: str, data_path: str) -> None:\n",
    "        \"\"\"setup the default env variables for transformers\n",
    "        \n",
    "        Args:\n",
    "          uuids: a comma separate str of nvidia gpu/mig uuids\n",
    "        \"\"\"\n",
    "        os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = uuids \n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #512\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "        os.environ['XDG_CACHE_HOME']=f\"{data_path}/models\"\n",
    "        \n",
    "        \n",
    "    def show_folder_files(folder: str) -> None:\n",
    "        print(os.listdir(folder))\n",
    "        \n",
    "        \n",
    "    def huggingface_access_token(data_path: str) -> str:\n",
    "        token_file_path = f\"{data_path}/.cache/huggingface/token\"\n",
    "        token = \"\"\n",
    "        with open(token_file_path, \"r\") as file:\n",
    "            token = file.read().replace('\\n', '')\n",
    "        return token\n",
    "    \n",
    "    \n",
    "    def load_feather(path: str, col_names=[\"text\"]) -> DataFrame:\n",
    "        with open(path, \"r\") as file:\n",
    "            df = pd.read_feather(path)\n",
    "        if df is not None:\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        \n",
    "    def log_txt(data, debug: bool) -> None:\n",
    "        if debug:\n",
    "            if isinstance(data, str):\n",
    "                print(\"Reading string content...\")\n",
    "                print(\"-\"*20)\n",
    "                print(data)\n",
    "            elif isinstance(data, list):\n",
    "                for txt in data:\n",
    "                    print(\"Reading string content...\")\n",
    "                    print(\"-\"*20)\n",
    "                    print(txt)\n",
    "    \n",
    "    def prompts_with_context(prompts: list, placeholder, context: str) -> list:\n",
    "        \"\"\"replace the placeholder in prompts with context\n",
    "        \n",
    "        Returns:\n",
    "           new prompt list of str replace the placeholder with context\n",
    "        \"\"\"\n",
    "        return [ prompt.replace(placeholder, context) for prompt in prompts]\n",
    "        \n",
    "        \n",
    "    '''Global variable'''\n",
    "    model_map = {\n",
    "        \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"70B\" : \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        # \"70B\" : \"meta-llama/Llama-2-70b-hf\" \n",
    "    }\n",
    "    data_path = f\"{data_root}/{data_sub_path}\"\n",
    "    model_name = model_map.get(model_type, \"7B\")\n",
    "    nvidia_state = GPUInfoHelper()\n",
    "    # load feather DataFrame\n",
    "    feather_text_col_name = \"text\"\n",
    "    df = load_feather(prompt_context_path)\n",
    "    # get the row series and the column\n",
    "    prompt_context = str(df.iloc[0][feather_text_col_name])\n",
    "    prompts = prompts_with_context(prompts=prompt_templates, placeholder=prompt_placeholder, context=prompt_context)\n",
    "    # clean up memory\n",
    "    del prompt_context\n",
    "    # print str or list[str] content with log_txt\n",
    "    log_txt(data=prompts, debug=show_log_txt)\n",
    "    \n",
    "    '''Initialization'''\n",
    "    UUIDs = nvidia_mig_uuids()\n",
    "    init_cuda_torch(UUIDs, data_path)\n",
    "    import torch\n",
    "    display_container_info()\n",
    "    print(UUIDs)\n",
    "    \n",
    "    show_folder_files(data_path)\n",
    "    \n",
    "    '''Transformers must be imported after the init_cuda_torch to get env set'''\n",
    "    import transformers\n",
    "    from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "    import transformers\n",
    "    print(f\"transformers version: {transformers.__version__}\")\n",
    "    \n",
    "    def chat_gen(\n",
    "        generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "        tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "        nvidia_state: GPUInfoHelper,\n",
    "    ):    \n",
    "        def local(input: str, print_mode: bool = True) -> list[str]:\n",
    "            start = time.time()\n",
    "            sequences = generator(\n",
    "                input,\n",
    "                do_sample=True,\n",
    "                top_k=10,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                # max_length=200,\n",
    "                max_new_tokens=200,\n",
    "            )\n",
    "            result = []\n",
    "            for seq in sequences:\n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "\n",
    "            end = time.time()\n",
    "            duration = end - start\n",
    "            # returns the result\n",
    "            if print_mode == True:\n",
    "                for s in result:\n",
    "                    print(s)\n",
    "\n",
    "                print(\"-\"*20)\n",
    "                print(f\"walltime: {duration} in secs.\")\n",
    "                nvidia_state.gpu_usage()\n",
    "            return result\n",
    "        \n",
    "        return local\n",
    "    \n",
    "    \n",
    "    token = huggingface_access_token(data_path)\n",
    "    print(f\"XDG_CACHE_HOME: {os.environ['XDG_CACHE_HOME']}\")\n",
    "    print(f\"Loading LLM model {model_name} ...\")\n",
    "    # os.makedirs(\"/tmp/outputs/Output/data\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        #use_auth_token=token,\n",
    "        token=token, #transformers==4.32.1\n",
    "    )\n",
    "    \n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        token=token,\n",
    "        #use_auth_token=token, #transformers==4.32.1\n",
    "    )\n",
    "    # print gpu mem usage after loading the llm\n",
    "    nvidia_state.gpu_usage()\n",
    "    # create convenient chat function\n",
    "    chat = chat_gen(generator, tokenizer, nvidia_state)\n",
    "    \n",
    "    gen_results = []\n",
    "    print(f\"total prompts: {len(prompts)}\")\n",
    "    for prompt in prompts:\n",
    "        print(f\"current prompt token length: {len(prompt)}\")\n",
    "        talk_back_list = chat(prompt)\n",
    "    \n",
    "        if (talk_back_list is not None) and len(talk_back_list) > 0:\n",
    "            # answer_str = talk_back_list[0]\n",
    "            gen_results.append(talk_back_list[0])\n",
    "        else:\n",
    "            # answer_str = \"\"\n",
    "            gen_results.append(\"\")\n",
    "            \n",
    "    #with open(output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "    #    df.to_csv(f, index=False, header=True, encoding=\"utf-8\")\n",
    "    \n",
    "    output = namedtuple('output',['answers']) \n",
    "    return output(gen_results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01f1ed-4564-4302-8fb9-094e1325ebe0",
   "metadata": {},
   "source": [
    "### Create data processing component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165590ad-f3ba-482f-bcdc-16635f5ab69c",
   "metadata": {},
   "source": [
    "### Define Helper Function\n",
    "Difference between 2Gi and 2G:\n",
    "* https://stackoverflow.com/questions/50804915/kubernetes-size-definitions-whats-the-difference-of-gi-and-g/50805048#50805048\n",
    "\n",
    "Set MIG GPU requests:\n",
    "* https://github.com/kubeflow/pipelines/issues/6858#issuecomment-1007511676\n",
    "\n",
    "```python\n",
    "containerOp.add_resource_request(gpu_resource, gpu_req)\n",
    "containerOp.add_resource_limit(gpu_resource, gpu_lim)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb43819c-aeb2-4ab0-a395-54dc3c3bae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pod_resource_transformer(op: ContainerOp, mem_req=\"200Mi\", cpu_req=\"2000m\", mem_lim=\"4000Mi\", cpu_lim='4000m', gpu_req=None, gpu_lim=None):\n",
    "    \"\"\"\n",
    "    this function helps to set the resource limit for container operators\n",
    "    op.set_memory_limit('1000Mi') = 1GB\n",
    "    op.set_cpu_limit('1000m') = 1 cpu core\n",
    "    \"\"\"\n",
    "    gpu_resource = \"nvidia.com/mig-1g.20gb\"\n",
    "    # gpu_resource = \"nvidia.com/mig-2g.20gb\"\n",
    "    new_op = op.set_memory_request(mem_req)\\\n",
    "        .set_memory_limit(mem_lim)\\\n",
    "        .set_cpu_request(cpu_req)\\\n",
    "        .set_cpu_limit(cpu_lim)\n",
    "    if (gpu_req is not None) and (gpu_lim is not None):\n",
    "        new_op.add_resource_request(gpu_resource, gpu_req)\n",
    "        new_op.add_resource_limit(gpu_resource, gpu_lim)\n",
    "    return new_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30253e28-bf1e-4716-a05b-2524952da9b5",
   "metadata": {},
   "source": [
    "## Define Pipeline\n",
    "* Intro Kubeflow pipeline: https://v1-5-branch.kubeflow.org/docs/components/pipelines/introduction/\n",
    "* Kubeflow pipeline SDK v1: https://v1-5-branch.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8202fe0-e1fa-4795-81d3-4441c0a1d201",
   "metadata": {},
   "source": [
    "#### Construct promp list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8363d023-362f-42f0-934d-eb33da3c6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_holder = \"#\"\n",
    "test_prompt='Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n'\n",
    "name_prompt = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {place_holder}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "age_prompt = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{place_holder}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "\n",
    "# processed in parallel output of memory\n",
    "# prompts = [test_prompt, name_prompt, age_prompt]\n",
    "prompts = [age_prompt]\n",
    "# prompts = [test_prompt]\n",
    "# print(len(prompts))\n",
    "# print(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621d9b5-30db-412e-974a-a795b6f06211",
   "metadata": {},
   "source": [
    "#### Define pipeline DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f43545d-ddfd-49ad-8f43-fb7600d4b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name = EXPERIMENT_NAME,\n",
    "    description = EXPERIMENT_DESC\n",
    ")\n",
    "def custom_pipeline(\n",
    "    data_root: str= \"/mnt\", \n",
    "    data_sub_path: str=\"core-kind/yinwang\",\n",
    "    file_sub_path: str=\"core-kind/yinwang/data/medreports\",\n",
    "    file_pattern: str=\"KK-SCIVIAS-*.pdf\",\n",
    "    file_idx: int = 1,\n",
    "    trans_model_type: str=\"custom\", \n",
    "    gen_model_type: str=\"7B\",\n",
    "    show_log_txt: bool=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      data_root: the mount path of shared data volume.\n",
    "      data_sub_path: the relative path to the data folder, without leading ./\n",
    "    \"\"\"\n",
    "    \n",
    "    '''local variable'''\n",
    "    no_artifact_cache = \"P0D\"\n",
    "    artifact_cache_today = \"P1D\"\n",
    "    #cache_setting = no_artifact_cache\n",
    "    cache_setting = artifact_cache_today\n",
    "    # prompt = \"how are you buddy?\"\n",
    "    # trans_text = \"Das Haus ist wunderbar.\"\n",
    "    \n",
    "    '''Pipeline Volume'''\n",
    "    # predefined pvc in namespace\n",
    "    shared_volume = PipelineVolume(\"llm-models\")\n",
    "    \n",
    "    '''pipeline'''\n",
    "    extract_task = text_extractor(\n",
    "        data_root=data_root, \n",
    "        data_sub_path=file_sub_path,\n",
    "        file_pattern=file_pattern,\n",
    "        file_idx=file_idx,\n",
    "        show_log_txt=show_log_txt\n",
    "    )\n",
    "\n",
    "    extract_task = pod_resource_transformer(extract_task, mem_req=\"4000Mi\", cpu_req=\"2000m\", mem_lim=\"8000Mi\", cpu_lim=\"4000m\")\n",
    "    extract_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    extract_task.add_pvolumes({data_root: shared_volume})\n",
    "    extract_task.set_display_name(\"PDF-text extractor\")    \n",
    "    \n",
    "    translate_task = bart_translator(\n",
    "        data_root=data_root, \n",
    "        data_sub_path=data_sub_path,\n",
    "        model_type=trans_model_type,\n",
    "        show_log_txt=show_log_txt,\n",
    "        input=extract_task.outputs[\"output\"]\n",
    "        # origin_text=extract_task.output\n",
    "    )\n",
    "    #translate_task = pod_resource_transformer(translate_task, mem_req=\"24000Mi\", cpu_req=\"1000m\", mem_lim=\"24000Mi\", cpu_lim=\"2000m\", gpu_req=1, gpu_lim=1)\n",
    "    translate_task = pod_resource_transformer(translate_task, mem_req=\"12000Mi\", cpu_req=\"4000m\", mem_lim=\"12000Mi\", cpu_lim=\"16000m\", gpu_req=1, gpu_lim=1)\n",
    "    translate_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    translate_task.add_pvolumes({data_root: shared_volume})\n",
    "    translate_task.set_display_name(\"BART de_en translator\")\n",
    "    \n",
    "    inference_task = llm_gen(\n",
    "        data_root=data_root, \n",
    "        data_sub_path=data_sub_path, \n",
    "        model_type=gen_model_type,\n",
    "        prompt_templates=prompts,\n",
    "        prompt_context=translate_task.output,\n",
    "        prompt_placeholder=place_holder,\n",
    "        show_log_txt=show_log_txt\n",
    "    )\n",
    "    # 200 MB ram and 1 cpu\n",
    "    inference_task = pod_resource_transformer(inference_task, mem_req=\"24000Mi\", cpu_req=\"1000m\", mem_lim=\"24000Mi\", cpu_lim=\"2000m\", gpu_req=1, gpu_lim=1)\n",
    "    # set the download caching to be 1day, disable caching with P0D\n",
    "    inference_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    inference_task.add_pvolumes({data_root: shared_volume})\n",
    "    inference_task.set_display_name(\"LlaMA2 entity extractor\")\n",
    "    \n",
    "    inference_task.after(translate_task)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e789aa-5ccc-4290-8d9d-b3297128417b",
   "metadata": {},
   "source": [
    "### (optional) pipeline compile step\n",
    "use the following command to compile the pipeline to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beb19bcb-2215-4ca6-93a8-bd65a31f6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPE_LINE_FILE_NAME=f\"{PREFIX}_kfp1_info_extraction_pipeline\"\n",
    "kfp.compiler.Compiler().compile(custom_pipeline, f\"{PIPE_LINE_FILE_NAME}.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d198c16-f9ab-4e4c-a2f2-d710bec7aba9",
   "metadata": {},
   "source": [
    "### Create Experiment Run\n",
    "\n",
    "create run label with current data time\n",
    "```python\n",
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "ts = datetime.strftime(datetime.now(ptimezone(\"Europe/Berlin\")), \"%Y-%m-%d %H-%M-%S\")\n",
    "print(ts)\n",
    "```\n",
    "\n",
    "Reference:\n",
    "* https://stackoverflow.com/questions/25837452/python-get-current-time-in-right-timezone/25887393#25887393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08dd9330-1a3f-4fcd-8e53-fa7e4a05de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "\n",
    "def get_local_time_str(target_tz_str: str = \"Europe/Berlin\", format_str: str = \"%Y-%m-%d %H-%M-%S\") -> str:\n",
    "    \"\"\"\n",
    "    this method is created since the local timezone is miss configured on the server\n",
    "    @param: target timezone str default \"Europe/Berlin\"\n",
    "    @param: \"%Y-%m-%d %H-%M-%S\" returns 2022-07-07 12-08-45\n",
    "    \"\"\"\n",
    "    target_tz = ptimezone(target_tz_str) # create timezone, in python3.9 use standard lib ZoneInfo\n",
    "    # utc_dt = datetime.now(datetime.timezone.utc)\n",
    "    target_dt = datetime.now(target_tz)\n",
    "    return datetime.strftime(target_dt, format_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1ed73-9fd8-4e59-855e-690e738d09d9",
   "metadata": {},
   "source": [
    "### Config pipeline run\n",
    "* Setting imagePullSecretes for Pipeline with SDK: https://github.com/kubeflow/pipelines/issues/5843#issuecomment-859799181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dda75a5d-e923-4823-89ee-cc41e49c370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pandas import DataFrame, Series\n",
    "\n",
    "# place_holder = \"#\"\n",
    "# test_prompt='Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n'\n",
    "# name_prompt = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {place_holder}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "# age_prompt = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{place_holder}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "\n",
    "# ser = Series([test_prompt, name_prompt, age_prompt])\n",
    "# df = DataFrame(data=ser, columns=[\"prompt\"])\n",
    "# df.to_json()\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "853837b3-604d-439b-9cea-4b8e226c0ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_root': '/mnt', 'data_sub_path': 'core-kind/yinwang', 'file_sub_path': 'core-kind/yinwang/data/medreports', 'file_pattern': 'KK-SCIVIAS-*.pdf', 'file_idx': 0, 'gen_model_type': '7B', 'trans_model_type': 'custom', 'show_log_txt': True}\n"
     ]
    }
   ],
   "source": [
    "# from kubernetes import client as k8s_client\n",
    "pipeline_config = dsl.PipelineConf()\n",
    "\n",
    "# pipeline_config.set_image_pull_secrets([k8s_client.V1ObjectReference(name=K8_GIT_SECRET_NAME, namespace=NAME_SPACE)])\n",
    "# pipeline_config.set_image_pull_policy(\"Always\")\n",
    "pipeline_config.set_image_pull_policy(\"IfNotPresent\")\n",
    "\n",
    "# note:\n",
    "#file_idx = 1 has 8 page 14K token, too long for the llm prompt\n",
    "#file_idx = 0 has 4 page 7K token, works for the llm prompt\n",
    "pipeline_args = {\n",
    "    'data_root' : DATA_ROOT,\n",
    "    'data_sub_path' : DATA_SUB_PATH,\n",
    "    'file_sub_path' : FILE_SUB_PATH,\n",
    "    'file_pattern' : FILE_PATTERN,\n",
    "    'file_idx' : 0, # 1\n",
    "    'gen_model_type': DEFAULT_GEN_MODEL_TYPE,\n",
    "    'trans_model_type': DEFAULT_TRANS_MODEL_TYPE,\n",
    "    'show_log_txt': True,\n",
    "}\n",
    "print(pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5264dc44-9abd-48a2-970e-3ed8693edfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/f5acd22e-7836-41a6-b8cd-a555a7f71f39\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/78fa3d87-dab9-42f2-a6ab-cbee1a5d59c6\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=78fa3d87-dab9-42f2-a6ab-cbee1a5d59c6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_NAME = f\"{PREFIX}_extract_info_kfp1 {get_local_time_str()}\"\n",
    "\n",
    "# client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=custom_pipeline,\n",
    "    arguments = pipeline_args, #{}\n",
    "    run_name = RUN_NAME,\n",
    "    pipeline_conf=pipeline_config,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd6d15-c88d-4d5a-9b4e-fb2f3706c2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
