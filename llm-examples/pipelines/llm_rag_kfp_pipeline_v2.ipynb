{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bde0f4-67cd-4a5a-af8c-abf0f89a15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0003cb5d-2144-4d17-9d9c-ca060d3acbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade --user kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01c1e77-1584-4cc6-80ee-95ea349396f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from functools import partial\n",
    "from kfp.dsl import (\n",
    "    pipeline,\n",
    "    ContainerOp,\n",
    "    PipelineVolume\n",
    ")\n",
    "from kfp.components import (\n",
    "    # InputPath,\n",
    "    # OutputPath,\n",
    "    create_component_from_func\n",
    ")\n",
    "\n",
    "EXPERIMENT_NAME = 'llm' # Name of the experiment in the KF webapp UI\n",
    "EXPERIMENT_DESC = 'llm med report pipeline experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7bb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Settings():\n",
    "    llm_base_image: str = 'pytorch/pytorch:2.2.0-cuda11.8-cudnn8-devel'\n",
    "    # s3_base_image: str = 'python:3.10.13-slim-bullseye'\n",
    "    # use a runtime pytorch image to speed up the pip install process, since the applyllm has too many dependencies\n",
    "    # TODO: to seperate applyllm-io an applyllm package\n",
    "    s3_base_image: str = 'pytorch/pytorch:2.2.0-cuda11.8-cudnn8-runtime'\n",
    "    applyllm_version: str = '0.0.3'\n",
    "    pypdf_version: str = '3.15.5'\n",
    "    accelerate_version: str = '0.26.1'\n",
    "    unstructured_version: str = '0.11.0'\n",
    "    sentence_transformers_version: str = '2.2.2'\n",
    "    docarray_version: str = '0.39.1'\n",
    "    pydantic_version: str = '1.10.13'\n",
    "    boto3_version: str = '1.34.14'\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b30ee2b-796a-4aaa-b2a9-c472a0b12b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PIPELINE_PATH_DIR = \"./compiled\"\n",
    "if not os.path.exists(PIPELINE_PATH_DIR):\n",
    "    os.makedirs(PIPELINE_PATH_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d62514-8a95-4e2e-bf27-cd1197f36d1b",
   "metadata": {},
   "source": [
    "#### Load S3 text data objects with prefix and count limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afeb4af-5681-49a5-973e-5eb1aaa9fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PIPELINE_PATH_DIR}/s3_text_report_load_component.yaml\",\n",
    "    base_image=settings.s3_base_image, \n",
    "    packages_to_install=[\n",
    "        f\"applyllm=={settings.applyllm_version}\",\n",
    "        f\"boto3=={settings.boto3_version}\",\n",
    "    ],\n",
    ")\n",
    "def file_op(bucket_name: str, s3_file_prefix: str, s3_file_max_count: int, verify_host: bool = True) -> list:\n",
    "    # print(f\"{bucket_name}, {s3_file_prefix}, {s3_file_max_count}\")\n",
    "    import os\n",
    "    from applyllm.io import (\n",
    "        S3AccessConf,\n",
    "        S3BucketHelper,\n",
    "    )\n",
    "    s3_conf = S3AccessConf(\n",
    "        access_key_id = os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "        secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "        endpoint = os.environ.get('S3_ENDPOINT'),\n",
    "        bucket_name = bucket_name,\n",
    "        verify_host = verify_host,\n",
    "    )\n",
    "    s3_text_reports_helper = S3BucketHelper(conf=s3_conf, file_prefix=s3_file_prefix)\n",
    "    target_s3_obj_list = list(s3_text_reports_helper.get_object_keys(limit_count=s3_file_max_count))\n",
    "    return target_s3_obj_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3a500",
   "metadata": {},
   "source": [
    "#### Get run id and run name within the kubeflow pipeline component\n",
    "* https://stackoverflow.com/questions/63473716/how-to-obtain-the-kubeflow-pipeline-run-name-from-within-a-component/67616896#67616896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7763edd5-3c2e-44c5-afb1-3983a46d0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PIPELINE_PATH_DIR}/llm_entity_extraction_component.yaml\",\n",
    "    base_image=settings.llm_base_image, \n",
    "    packages_to_install=[\n",
    "        f\"applyllm=={settings.applyllm_version}\",\n",
    "        f\"pypdf=={settings.pypdf_version}\",\n",
    "        f\"accelerate=={settings.accelerate_version}\",\n",
    "        f\"unstructured=={settings.unstructured_version}\",\n",
    "        f\"sentence-transformers=={settings.sentence_transformers_version}\", # for langchain vectorestore embedding model\n",
    "        f\"docarray=={settings.docarray_version}\",\n",
    "        f\"pydantic=={settings.pydantic_version}\", # must be a version less than 2.x\n",
    "    ], # adding additional libs\n",
    "    # pip_index_urls=[\"https://gitlab.lrz.de/api/v4/projects/150553/packages/pypi/simple\"]\n",
    "    # define my private pypi package registry v2 component decorator\n",
    ")\n",
    "def llm_op(model_root: str, \n",
    "           lm_model_type: str, \n",
    "           max_token_length: int, \n",
    "           max_position_embeddings: int,\n",
    "           max_new_tokens: int,\n",
    "           repetition_penalty: float,\n",
    "           temperature: float,\n",
    "           lm_device_map: str,\n",
    "           top_k: int,\n",
    "           top_p: float,\n",
    "           bucket_name: str,\n",
    "           # s3_file_prefix: str,\n",
    "           # s3_file_key: str,\n",
    "           s3_file_key_list: list,\n",
    "           embed_model_vendor: str,\n",
    "           retriever_k: int,\n",
    "           user_query1: str,\n",
    "           user_query1_parser_question: str,\n",
    "           user_query1_target_var: str,\n",
    "           user_query1_target_var_desc: str,\n",
    "           user_query2: str,\n",
    "           user_query2_vars: list,\n",
    "           user_query2_parser_question: str,\n",
    "           user_query2_target_var: str,\n",
    "           user_query2_target_var_desc: str,\n",
    "           sql_target_table: str,\n",
    "           run_name: str,\n",
    "           verbose: bool = False,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_root: The root directory of the model\n",
    "        lm_model_type: The type of the language model\n",
    "        max_token_length: The maximum token length 4096\n",
    "        max_position_embeddings: The maximum position embeddings 3072\n",
    "        max_new_tokens: The maximum new tokens to be generated 80\n",
    "        repetition_penalty: The repetition penalty 1.15\n",
    "        temperature: The temperature 0.001\n",
    "        lm_device_map: The device map for the language model \"auto\"\n",
    "        top_k: The top k value 3\n",
    "        top_p: The top p value 0.8\n",
    "        bucket_name: The name of the bucket \"scivias-medreports\"\n",
    "        s3_file_key: The key of the s3 file \"trans2en/KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\"\n",
    "        s3_file_key_list: list, The key of the s3 files to be processed [\"trans2en/KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\"]\n",
    "        embed_model_vendor: The embedding model vendor \"baai\" or \"sentence-transformers\"\n",
    "        retriever_k: The retriever k value 3\n",
    "        user_query1: The user query \"What is the name of the patient? (Remember to include 'The name of the patient is' in your answer.)\"\n",
    "        user_query1_parser_question: The user query parser question \"retrieve one: patient name\"\n",
    "        user_query1_target_var: The user query target variable \"patient_name\"\n",
    "        user_query1_target_var_desc: The user query target variable description \"patient name\"\n",
    "        user_query2: \"What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer.)\"\n",
    "        user_query2_vars: The user query 2 variables [\"patient_name\"]\n",
    "        user_query2_parser_question: The user query 2 parser question \"retrieve one: patient age as number\"\n",
    "        user_query2_target_var: The user query 2 target variable \"patient_age\"\n",
    "        user_query2_target_var_desc: The user query 2 target variable description \"patient age\"\n",
    "        sql_target_table: The target table for the sql \"llm_med_report_info\"\n",
    "        run_name: The kubeflow pipeline run name \"llm-med-report-pipeline\"\n",
    "        verbose: The verbose flag False\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import applyllm as apl\n",
    "\n",
    "    from applyllm.accelerators import (\n",
    "        AcceleratorHelper,\n",
    "        AcceleratorStatus,\n",
    "        DirectorySetting,\n",
    "        # DIR_MODE_MAP,\n",
    "        TokenHelper as th\n",
    "    )\n",
    "    from applyllm.utils import time_func\n",
    "    from applyllm.pipelines import (\n",
    "        LocalCausalLMConfig,\n",
    "        ModelConfig,\n",
    "        ModelCatalog,\n",
    "    )\n",
    "\n",
    "    dir_setting = DirectorySetting(home_dir=model_root)\n",
    "    \n",
    "    # debug code to check the mounted model_root, whether DirectorySetting is working\n",
    "    # print([x[0] for x in os.walk(model_root)])\n",
    "\n",
    "    gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "    '''init gpu helper'''\n",
    "    gpu_helper = AcceleratorHelper()\n",
    "    UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)\n",
    "    # will set the XDG_CACHE_HOME, this line must be called before import transformers\n",
    "    gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "    print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "    print(os.environ[\"XDG_CACHE_HOME\"])\n",
    "    '''init llm model to be loaded'''\n",
    "    model_map = {\n",
    "        \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "        \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "        \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    }\n",
    "    # model_type = \"mistral7B-inst02\"\n",
    "    model_type = lm_model_type\n",
    "    model_name = model_map.get(model_type, \"mistral7B-inst02\")\n",
    "    print(model_name)\n",
    "\n",
    "    import transformers\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from torch import bfloat16\n",
    "    \n",
    "    print(f\"applyllm version:     {apl.__version__}\")\n",
    "    print(f\"transformers version: {transformers.__version__}\")\n",
    "    print(f\"torch version:        {torch.__version__}\")\n",
    "\n",
    "    token_kwargs = th.gen_token_kwargs(model_type=model_type, dir_setting=dir_setting)\n",
    "    \n",
    "    \"\"\"Load CausalLM model\"\"\"\n",
    "    base_lm_config = ModelConfig(\n",
    "        model_config = {\n",
    "            \"pretrained_model_name_or_path\": model_name,\n",
    "            \"device_map\": \"auto\",\n",
    "            # \"max_memory\": f\"{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    lm_model_kwargs = {\n",
    "        \"quantized\": True,\n",
    "        \"model_config\": base_lm_config.get_config(),\n",
    "        \"quantization_config\": {\n",
    "            \"quantization_config\": transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=bfloat16\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "    lm_config = LocalCausalLMConfig(**lm_model_kwargs)\n",
    "\n",
    "    @time_func\n",
    "    def fetch_lm_model():\n",
    "        return AutoModelForCausalLM.from_pretrained(\n",
    "        **lm_config.get_config(),\n",
    "        **token_kwargs,  \n",
    "        )\n",
    "\n",
    "    model = fetch_lm_model()\n",
    "\n",
    "    gpu_status.gpu_usage()\n",
    "    \"\"\"Load CausalLM tokenizer\"\"\"\n",
    "\n",
    "    def config_tokenizer(model_name: str, config: dict, pad_token_id = 2):\n",
    "        if model_name.startswith(ModelCatalog.MISTRAL_FAMILY):\n",
    "            return {**config, \"pad_token_id\": pad_token_id}\n",
    "        else:\n",
    "            return config\n",
    "        \n",
    "    MAX_POSITION_EMBEDDINGS = max_position_embeddings\n",
    "    MAX_LENGTH = max_token_length\n",
    "\n",
    "    model_config= {\n",
    "        \"pretrained_model_name_or_path\": model_name,\n",
    "        \"device\": \"cpu\",\n",
    "        # \"device_map\": \"auto\", # put to GPU if GPU is available\n",
    "        \"max_position_embeddings\": MAX_LENGTH,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "    }\n",
    "    model_config = config_tokenizer(model_name=model_name, config=model_config)\n",
    "    tokenizer_config = ModelConfig(model_config=model_config)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        **tokenizer_config.get_config(), \n",
    "        **token_kwargs,\n",
    "    )\n",
    "\n",
    "    \"\"\"init the transformer pipeline as backend llm for langchain\"\"\"\n",
    "    tp_kwargs = {\n",
    "        \"task\": \"text-generation\",\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"device_map\": lm_device_map,\n",
    "        \"max_length\": None, # remove the total length of the generated response\n",
    "        \"max_new_tokens\": max_new_tokens, # set the size of new generated token \n",
    "    }\n",
    "\n",
    "    tp_config = ModelConfig(model_config = tp_kwargs)\n",
    "\n",
    "    generator = transformers.pipeline(\n",
    "        **tp_config.get_config(),\n",
    "        **token_kwargs,\n",
    "    )\n",
    "\n",
    "    \"\"\"Huggingface pipeline\"\"\"\n",
    "    from applyllm.pipelines import ModelCatalog, ModelInfo, PromptHelper\n",
    "\n",
    "    model_info = ModelCatalog.get_model_info(model_name=model_name)\n",
    "    prompt_helper = PromptHelper(model_info=model_info)\n",
    "\n",
    "    import langchain\n",
    "    from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "    print(f\"langchain.__version__: {langchain.__version__}\")\n",
    "\n",
    "    llm = HuggingFacePipeline(\n",
    "        pipeline=generator \n",
    "    )\n",
    "\n",
    "    llm.model_id = model_name\n",
    "    pipeline_kwargs_config = {\n",
    "        \"device_map\": lm_device_map,\n",
    "        \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "        \"max_new_tokens\": max_new_tokens, # this is not taken by the model ?\n",
    "        \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "        \"temperature\": temperature,\n",
    "        \"repetition_penalty\": repetition_penalty, # 1.15,\n",
    "    }\n",
    "    model_kwargs_config = {\n",
    "        \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "        \"top_k\": top_k, # this param result in trouble with langchain (optional)\n",
    "        \"num_return_sequences\": 1, # (optional)\n",
    "        \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "        \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "        \"max_new_tokens\": max_new_tokens, # this is not taken by the model ?\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p, # 0.95 # alternative to top_k summerized probability while do_sample=True\n",
    "        \"repetition_penalty\": repetition_penalty, # 1.15,\n",
    "        \"trust_remote_code\": True,\n",
    "    }\n",
    "\n",
    "    llm.model_kwargs = config_tokenizer(model_name=model_name, config=model_kwargs_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    llm.pipeline_kwargs = config_tokenizer(model_name=model_name, config=pipeline_kwargs_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(\"HuggingFacePipeline setup done\")\n",
    "    gpu_status.gpu_usage()\n",
    "\n",
    "    \"\"\"LangChain pipeline\"\"\"\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "    from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "    from langchain.indexes import VectorstoreIndexCreator\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "    # from langchain.text_splitter import TextSplitter\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain_core.documents.base import Document\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from typing import List\n",
    "    import boto3\n",
    "    from applyllm.io import S3PdfObjHelper, DocMetaInfo, DocCorpusS3\n",
    "\n",
    "    print(boto3.__version__)\n",
    "\n",
    "    bucket_name = bucket_name\n",
    "    # file_prefix = s3_file_prefix\n",
    "    # PREFIX = f\"{S3PdfObjHelper.DataContract.key_lead}/{file_prefix}\"\n",
    "    access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "    secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "    s3_endpoint = os.environ.get('S3_ENDPOINT')\n",
    "    # VERIFY = False\n",
    "    VERIFY = True\n",
    "\n",
    "    # print(PREFIX)\n",
    "\n",
    "    def get_single_file_loader(key: str):\n",
    "        return S3FileLoader(bucket=bucket_name, \n",
    "                        key = key,\n",
    "                        aws_access_key_id=access_key_id,\n",
    "                        aws_secret_access_key=secret_access_key,\n",
    "                        endpoint_url=s3_endpoint,\n",
    "                        verify = VERIFY,\n",
    "                        use_ssl = True)\n",
    "    \n",
    "    @time_func\n",
    "    def fetch_s3_object(key: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        a list of LangChain Document object\n",
    "        \"\"\"\n",
    "        loader = get_single_file_loader(key)\n",
    "        return loader.load()\n",
    "\n",
    "\n",
    "    \"\"\"RAG Setting\"\"\"\n",
    "    from applyllm.utils import token_size\n",
    "\n",
    "    CHUNK_SIZE = (MAX_POSITION_EMBEDDINGS // 1000) * 1000\n",
    "    model_config = {\n",
    "        # Set a really small chunk size, just to show.\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"chunk_overlap\": 20,\n",
    "        \"length_function\": token_size, # len,\n",
    "        \"is_separator_regex\": False,\n",
    "    }\n",
    "\n",
    "    splitter_config = ModelConfig(model_config=model_config)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        **splitter_config.get_config()\n",
    "    )\n",
    "\n",
    "    embed_model_map = {\n",
    "        \"sentence-transformers\": \"sentence-transformers/all-MiniLM-L12-v2\", # 384\n",
    "        \"baai\" : \"BAAI/bge-base-en-v1.5\" # 768 embedding dims\n",
    "    }\n",
    "\n",
    "    embed_model_vendor = embed_model_vendor\n",
    "    embed_model_name = embed_model_map[embed_model_vendor]\n",
    "    model_config = {\n",
    "        \"model_name\" : embed_model_name,\n",
    "        \"model_kwargs\": {'device': 'cpu'},\n",
    "        \"encode_kwargs\": {'normalize_embeddings': True}\n",
    "    }\n",
    "    embed_config = ModelConfig(model_config=model_config)\n",
    "\n",
    "    # is downloaded at \"{MODEL_CACHE_DIR}/models/torch/sentence_transformer\" folder\n",
    "    embed_model = HuggingFaceEmbeddings(\n",
    "        **embed_config.get_config()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    from sqlalchemy import String, Integer, MetaData, BigInteger\n",
    "    from sqlalchemy.orm import DeclarativeBase\n",
    "    from sqlalchemy.orm import mapped_column\n",
    "    from sqlalchemy.orm import Mapped\n",
    "    from typing import Optional\n",
    "    from sqlalchemy.dialects.postgresql import insert as pgsql_upsert\n",
    "    from sqlalchemy.orm import sessionmaker\n",
    "    from sqlalchemy.orm import Session\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.chains import LLMChain, TransformChain\n",
    "    from langchain.output_parsers import ResponseSchema\n",
    "    from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "   \n",
    "    class Base(DeclarativeBase):\n",
    "        pass\n",
    "\n",
    "    class ReportInfo(Base):\n",
    "        __tablename__ = sql_target_table\n",
    "        \n",
    "        id: Mapped[str] = mapped_column(String(50), primary_key=True, autoincrement=False)\n",
    "        age: Mapped[int] = mapped_column(Integer)\n",
    "        llm_model: Mapped[str] = mapped_column(String(100))\n",
    "        embed_model: Mapped[str] = mapped_column(String(100))\n",
    "        run_name: Mapped[str] = mapped_column(String(100))\n",
    "        timestamp: Mapped[int] = mapped_column(BigInteger)\n",
    "        # timestamp: Mapped[int] = mapped_column(Integer)\n",
    "        \n",
    "        def __repr__(self) -> str:\n",
    "            # !r calls repr(self.id) \n",
    "            # https://stackoverflow.com/questions/44800801/in-python-format-f-string-strings-what-does-r-mean\n",
    "            return f\"ReportInfo(id={self.id!r}, age={self.age!r}), llm_model={self.llm_model!r}, embed_model={self.embed_model!r}, run_name={self.run_name!r}, timestamp={self.timestamp!r}\"\n",
    "        \n",
    "        def to_dict(self) -> dict:\n",
    "            return {\n",
    "                \"id\": self.id,\n",
    "                \"age\": self.age,\n",
    "                \"llm_model\": self.llm_model,\n",
    "                \"embed_model\": self.embed_model,\n",
    "                \"run_name\": self.run_name,\n",
    "                \"timestamp\": self.timestamp,\n",
    "            }\n",
    "        \n",
    "    \n",
    "    from applyllm.io import (\n",
    "        SqlDBHelperFactory,\n",
    "    )\n",
    "    from applyllm.pipelines import StructuredOutputParserHelper as ParserHelper\n",
    "\n",
    "    db_config = SqlDBHelperFactory.get_db_config_from_env(\n",
    "        port_key=\"SCIVIAS_ANALYTICS_DB_PORT\"\n",
    "    )\n",
    "    sync_engine = SqlDBHelperFactory.get_sync_engine(db_config=db_config, verbose=verbose)\n",
    "\n",
    "    def build_sql_stmt(id: str, age: int, llm_model: str, embed_model: str, run_name: str, timestamp: int):\n",
    "        stmt = pgsql_upsert(ReportInfo).values(\n",
    "            [ \n",
    "                {\"id\": id, \"age\": age, \"llm_model\": llm_model, \"embed_model\": embed_model, \"run_name\": run_name, \"timestamp\": timestamp}\n",
    "            ]\n",
    "        )\n",
    "        stmt = stmt.on_conflict_do_update(constraint = ReportInfo.__table__.primary_key, set_=stmt.excluded)\n",
    "        return stmt\n",
    "    \n",
    "    def llm_experiment(index, sync_engine, llm_model: str, embed_model: str):\n",
    "        # build in memory retriever index\n",
    "        retriever = index.vectorstore.as_retriever(search_kwargs={'k': retriever_k})\n",
    "\n",
    "        \"\"\"Prompts\"\"\"\n",
    "        query_template = \"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Only return the helpful answer below and nothing else.\n",
    "        Helpful answer:\n",
    "        \"\"\"\n",
    "\n",
    "        map_template = prompt_helper.gen_prompt(query_template)\n",
    "\n",
    "        map_prompt_template = PromptTemplate.from_template(map_template)\n",
    "\n",
    "        reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "        Always answer as helpfully as possible using the context text provided.\n",
    "        Always summarise the context text provided.\n",
    "        Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "        If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "        If you don't know the answer to a question, please don't share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "        CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "        Question: {question}/n/n\n",
    "\n",
    "        Only return the summarised answer below and nothing else.\n",
    "        Summarised answer:\n",
    "        [/INST]\"\"\"\n",
    "\n",
    "        reduce_prompt_template = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "        refine_init_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "        Always answer as helpfully as possible using the context text provided.\n",
    "        Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "        If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "        Context:/n/n {context_str}/n/n/n\n",
    "\n",
    "        Question: {question}/n/n\n",
    "\n",
    "        Only return the helpful answer below and nothing else.\n",
    "        Helpful answer:\n",
    "        [/INST]\"\"\"\n",
    "\n",
    "        init_prompt_template = PromptTemplate.from_template(refine_init_template)\n",
    "\n",
    "        chain_type = \"map_reduce\"\n",
    "\n",
    "        def build_qa_chain(chain_type: str, retriever, map_prompt_template, reduce_prompt_template, init_prompt_template):\n",
    "            qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=llm,\n",
    "                chain_type=chain_type,\n",
    "                retriever=retriever,\n",
    "                # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "                # chain_type_kwargs={\"map_prompt\": map_prompt_template},\n",
    "                return_source_documents=True,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "            if chain_type == \"map_reduce\":\n",
    "                qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template\n",
    "                qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "                # set the token max from 3000 to 4000\n",
    "                qa_chain.combine_documents_chain.reduce_documents_chain.token_max = MAX_POSITION_EMBEDDINGS   \n",
    "            if chain_type == \"refine\":\n",
    "                # pass\n",
    "                qa_chain.combine_documents_chain.initial_llm_chain.prompt = init_prompt_template\n",
    "            return qa_chain\n",
    "        \n",
    "        qa_chain = build_qa_chain(chain_type, retriever, map_prompt_template, reduce_prompt_template, init_prompt_template)\n",
    "        \n",
    "        query = user_query1\n",
    "        response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "\n",
    "        parser_query_template = \"\"\"<s>[INST] You are a helpful, respectful and honest assistant.\n",
    "        Always answer as helpfully as possible using the context text provided.\n",
    "        Your answers should only answer the question once and not have any text after the answer is done.\n",
    "        If you don't know the answer to a question, please don't share false information. Just return \"</s>\"\n",
    "\n",
    "        CONTEXT:\n",
    "        {text}\n",
    "\n",
    "        Question: \n",
    "        {question}\n",
    "\n",
    "        {format_instructions}\n",
    "        [/INST]\"\"\"\n",
    "        \n",
    "        name_schema = ResponseSchema(name=user_query1_target_var, description=user_query1_target_var_desc)\n",
    "\n",
    "        def build_parser_and_chain(llm, schema: ResponseSchema, parser_query_template: str, input_variables: List[str] = [\"text\",\"questions\"]):\n",
    "            output_parser = StructuredOutputParser.from_response_schemas([schema])\n",
    "            format_instructions = output_parser.get_format_instructions()\n",
    "            prompt_template = PromptTemplate(\n",
    "                template=parser_query_template,\n",
    "                input_variables=input_variables,\n",
    "                partial_variables={\"format_instructions\": format_instructions},\n",
    "            )\n",
    "            return output_parser, LLMChain(prompt=prompt_template, llm=llm)\n",
    "        \n",
    "        \n",
    "        user_query1_parser_chain_input = response['result'].strip()\n",
    "\n",
    "        query1_output_parser, parser_chain = build_parser_and_chain(llm=llm, schema=name_schema, parser_query_template=parser_query_template, input_variables=[\"text\",\"question\"])\n",
    "        dict_response = parser_chain.invoke(input={\"text\":user_query1_parser_chain_input, \"question\":user_query1_parser_question})\n",
    "        \n",
    "        user_query1_target_value = ParserHelper.parse_response_dict(\n",
    "            parser_response=dict_response,\n",
    "            output_parser=query1_output_parser,\n",
    "            text_key=\"text\"\n",
    "        ).get(user_query1_target_var, \"\").strip()\n",
    "\n",
    "        llm_results = {user_query1_target_var: user_query1_target_value}\n",
    "\n",
    "        \"\"\"Read User Query 2\"\"\"\n",
    "\n",
    "        query = user_query2\n",
    "        if len(user_query2_vars) > 0:\n",
    "            prompt_template = PromptTemplate.from_template(user_query2)\n",
    "            kwargs = {var: llm_results.get(var, \"\") for var in user_query2_vars}\n",
    "            # print(kwargs)\n",
    "            query = prompt_template.format(**kwargs)\n",
    "        \n",
    "        # print(query)\n",
    "\n",
    "        chain_type = \"map_reduce\"\n",
    "        # chain_type = \"stuff\"\n",
    "        # chain_type = \"refine\" \n",
    "        qa_chain = build_qa_chain(chain_type, retriever, map_prompt_template, reduce_prompt_template, init_prompt_template)\n",
    "        response = qa_chain.invoke({\"query\": query})\n",
    "        \n",
    "        age_schema = ResponseSchema(name=user_query2_target_var, description=user_query2_target_var_desc)\n",
    "        query2_output_parser, parser_chain = build_parser_and_chain(llm=llm, schema=age_schema, parser_query_template=parser_query_template, input_variables=[\"text\",\"question\"])\n",
    "        \n",
    "        user_query2_parser_chain_input = response['result'].strip()\n",
    "        dict_response = parser_chain.invoke(input={\"text\":user_query2_parser_chain_input, \"question\":user_query2_parser_question})\n",
    "        \n",
    "        patient_age_obj = ParserHelper.parse_response_dict(\n",
    "            parser_response=dict_response,\n",
    "            output_parser=query2_output_parser,\n",
    "            text_key=\"text\",\n",
    "            verbose=False\n",
    "        ).get(user_query2_target_var, \"\")\n",
    "\n",
    "        try:\n",
    "            if isinstance(patient_age_obj, str):\n",
    "                patient_age_obj = patient_age_obj.strip()\n",
    "                patient_age = int(patient_age_obj)\n",
    "            if isinstance(patient_age_obj, int):\n",
    "                patient_age = patient_age_obj\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            patient_age = -1\n",
    "\n",
    "        llm_results[user_query2_target_var] = patient_age\n",
    "\n",
    "        id = CUR_DOC_INFO.name.split(\".\")[0]\n",
    "        \n",
    "        now = datetime.utcnow()\n",
    "        print(f\"timestamp to save result for {id} is {now}\")\n",
    "        current_time = int(now.timestamp())\n",
    "        stmt = build_sql_stmt(id=id, age=llm_results[user_query2_target_var], \n",
    "                              llm_model=llm_model, embed_model=embed_model, run_name=run_name, timestamp=current_time)\n",
    "        with Session(sync_engine) as session:    \n",
    "            # sqlalchemy.engine.result.ScalarResult\n",
    "            result = session.scalars(stmt.returning(ReportInfo))\n",
    "            # result.all() returns a list of ReportInfo objects which is bound to the session\n",
    "            # unpacking the ReportInfo objects to dictionary to use it outside session as copy\n",
    "            # elements = [e.to_dict() for e in result.all()]    \n",
    "            session.commit()\n",
    "    \n",
    "    \n",
    "    \"\"\"create target sql table if not exists\"\"\"\n",
    "    table_objects = [ReportInfo.__table__]\n",
    "    Base.metadata.create_all(sync_engine, tables=table_objects, checkfirst=True)\n",
    "\n",
    "    for s3_file_key in s3_file_key_list:\n",
    "        \n",
    "        data = fetch_s3_object(key=s3_file_key)\n",
    "\n",
    "        \"\"\"Fetch S3 text file corpra\"\"\"\n",
    "        s3_corpus = DocCorpusS3(data)\n",
    "        if verbose:\n",
    "            print(\"--- Max Length Doc Info ---\")\n",
    "            print(s3_corpus.max_doc_meta)\n",
    "            print(\"--- Min Length Doc Info ---\")\n",
    "            print(s3_corpus.min_doc_meta)\n",
    "\n",
    "        file_idx = 0\n",
    "        show_content = False\n",
    "        CUR_DOC, CUR_DOC_INFO = s3_corpus.get_s3_obj_info(file_idx, show_content=show_content)\n",
    "        # print(CUR_DOC)\n",
    "        print(CUR_DOC_INFO)\n",
    "\n",
    "        # RAG one document\n",
    "        index = VectorstoreIndexCreator(\n",
    "            vectorstore_cls=DocArrayInMemorySearch,\n",
    "            embedding=embed_model,\n",
    "            text_splitter=text_splitter,\n",
    "            ).from_documents([data[file_idx]])        \n",
    "        \n",
    "        llm_experiment(index=index, sync_engine=sync_engine, llm_model=model_name, embed_model=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d9dc48c-75de-4812-8177-a27b5a414067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_res_limits(task: ContainerOp, mem_req=\"200Mi\", cpu_req=\"2000m\", mem_lim=\"4000Mi\", cpu_lim='4000m', gpu_req=None, gpu_lim=None, gpu_type:str=\"20gb\"):\n",
    "    \"\"\"\n",
    "    this function helps to set the resource limit for container operators\n",
    "    op.set_memory_limit('1000Mi') = 1GB\n",
    "    op.set_cpu_limit('1000m') = 1 cpu core\n",
    "    \"\"\"\n",
    "    if gpu_type == \"20gb\":\n",
    "        gpu_resource = \"nvidia.com/mig-2g.20gb\"\n",
    "        # gpu_resource = \"nvidia.com/mig-1g.20gb\"\n",
    "    elif gpu_type == \"40gb\":\n",
    "        gpu_resource = \"nvidia.com/mig-3g.40gb\"\n",
    "    else:\n",
    "        gpu_resource = \"nvidia.com/mig-1g.10gb\"\n",
    "        \n",
    "    # gpu_resource = \"nvidia.com/mig-2g.20gb\"\n",
    "    new_op = task.set_memory_request(mem_req)\\\n",
    "        .set_memory_limit(mem_lim)\\\n",
    "        .set_cpu_request(cpu_req)\\\n",
    "        .set_cpu_limit(cpu_lim)\n",
    "    if (gpu_req is not None) and (gpu_lim is not None) and (gpu_type is not None):\n",
    "        new_op.add_resource_request(gpu_resource, gpu_req)\n",
    "        new_op.add_resource_limit(gpu_resource, gpu_lim)\n",
    "    return new_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2af7a2-4166-4ac5-84fb-93f24b49bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name = EXPERIMENT_NAME,\n",
    "    description = EXPERIMENT_DESC\n",
    ")\n",
    "def llm_pipeline(\n",
    "        model_root: str = \"/mnt\", \n",
    "        lm_model_type: str = \"mistral7B-inst02\", \n",
    "        max_token_length: int = 4096,\n",
    "        max_position_embeddings: int = 3072,\n",
    "        max_new_tokens: int = 80,\n",
    "        repetition_penalty: float = 1.15,\n",
    "        temperature: float = 0.001,\n",
    "        lm_device_map: str = \"auto\",\n",
    "        top_k: int = 3,\n",
    "        top_p: float = 0.8,\n",
    "        bucket_name: str = \"scivias-medreports\",\n",
    "        s3_file_prefix: str = \"KK-SCIVIAS\",\n",
    "        s3_file_max_count: int = -1, # unlimited\n",
    "        verify_host: bool = True,\n",
    "        # s3_file_key: str = \"trans2en/KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\",\n",
    "        s3_secrets: str=\"add-scivias-medreport-secret\",\n",
    "        sql_secrets: str=\"add-scivias-postgresdb-secret\",\n",
    "        embed_model_vendor: str = \"baai\",\n",
    "        retriever_k: int = 3,\n",
    "        user_query1: str = \"What is the name of the patient? (Remember to include 'The name of the patient is' in your answer.)\",\n",
    "        user_query1_parser_question: str = \"retrieve one: patient name\",\n",
    "        user_query1_target_var: str = \"patient_name\",\n",
    "        user_query1_target_var_desc: str = \"patient name\",\n",
    "        user_query2: str = \"What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer.)\",\n",
    "        user_query2_vars: list = [\"patient_name\"],\n",
    "        user_query2_parser_question: str = \"retrieve one: patient age as number\",\n",
    "        user_query2_target_var: str = \"patient_age\",\n",
    "        user_query2_target_var_desc: str = \"patient age\",\n",
    "        sql_target_table: str = \"llm_med_report_info\",\n",
    "        run_name: str = \"llm_rag_kfp_pipeline\",\n",
    "        verbose: bool = False,\n",
    "        gpu_type: str = \"20gb\"\n",
    "    ):\n",
    "    '''local variable'''\n",
    "    no_artifact_cache = \"P0D\"\n",
    "    artifact_cache_today = \"P1D\"\n",
    "    # cache_setting = artifact_cache_today\n",
    "    cache_setting = no_artifact_cache\n",
    "\n",
    "    '''Pipeline Volume'''\n",
    "    shared_volume = PipelineVolume(\"llm-models\")\n",
    "    \n",
    "    '''pipeline'''\n",
    "    file_task = file_op(\n",
    "        bucket_name=bucket_name,\n",
    "        s3_file_prefix=s3_file_prefix, \n",
    "        s3_file_max_count=s3_file_max_count,\n",
    "        verify_host=verify_host,\n",
    "    )\n",
    "    file_task = set_res_limits(task=file_task, mem_req=\"1Gi\", mem_lim=\"4Gi\",\n",
    "                            cpu_req=\"2000m\", cpu_lim=\"10000m\", \n",
    "                            gpu_req=0, gpu_lim=0, gpu_type=None)\n",
    "    file_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    file_task.add_pod_label(s3_secrets, \"true\") # s3_serets is the label for poddefault\n",
    "    file_task.set_display_name(\"load reports op\")\n",
    "    \n",
    "    llm_task = llm_op(\n",
    "        model_root=model_root, \n",
    "        lm_model_type=lm_model_type,\n",
    "        max_token_length=max_token_length,\n",
    "        max_position_embeddings=max_position_embeddings,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        temperature=temperature,\n",
    "        lm_device_map=lm_device_map,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        bucket_name=bucket_name,\n",
    "        s3_file_key_list=file_task.output, # s3_file_key_list selected from file task output\n",
    "        embed_model_vendor=embed_model_vendor,\n",
    "        retriever_k=retriever_k,\n",
    "        user_query1=user_query1,\n",
    "        user_query1_parser_question=user_query1_parser_question,\n",
    "        user_query1_target_var=user_query1_target_var,\n",
    "        user_query1_target_var_desc=user_query1_target_var_desc,\n",
    "        user_query2=user_query2,\n",
    "        user_query2_vars=user_query2_vars,\n",
    "        user_query2_parser_question=user_query2_parser_question,\n",
    "        user_query2_target_var=user_query2_target_var,\n",
    "        user_query2_target_var_desc=user_query2_target_var_desc,\n",
    "        sql_target_table=sql_target_table,\n",
    "        run_name=run_name,\n",
    "        verbose=verbose,\n",
    "        )\n",
    "    llm_task = set_res_limits(task=llm_task, mem_req=\"20Gi\", mem_lim=\"40Gi\",\n",
    "                            cpu_req=\"2000m\", cpu_lim=\"10000m\", \n",
    "                            gpu_req=1, gpu_lim=1, gpu_type=gpu_type)\n",
    "    llm_task.add_pvolumes({model_root: shared_volume})\n",
    "    llm_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    llm_task.add_pod_label(s3_secrets, \"true\") \n",
    "    llm_task.add_pod_label(sql_secrets, \"true\")\n",
    "    llm_task.set_display_name(\"llm op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739fe18f-2384-4bbb-a9bb-7e284b028903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# pipeline_path_dir=\"./compiled\"\n",
    "# if not os.path.exists(pipeline_path_dir):\n",
    "#     os.makedirs(pipeline_path_dir)\n",
    "\n",
    "PIPE_LINE_FILE_NAME=f\"llm_rag_kfp_pipeline\"\n",
    "kfp.compiler.Compiler().compile(llm_pipeline, f\"{PIPELINE_PATH_DIR}/{PIPE_LINE_FILE_NAME}.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "934304c5-326f-4d67-85d5-9920d1683590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "\n",
    "def get_local_time_str(target_tz_str: str = \"Europe/Berlin\", format_str: str = \"%Y-%m-%d %H-%M-%S\") -> str:\n",
    "    \"\"\"\n",
    "    this method is created since the local timezone is miss configured on the server\n",
    "    @param: target timezone str default \"Europe/Berlin\"\n",
    "    @param: \"%Y-%m-%d %H-%M-%S\" returns 2022-07-07 12-08-45\n",
    "    \"\"\"\n",
    "    target_tz = ptimezone(target_tz_str) # create timezone, in python3.9 use standard lib ZoneInfo\n",
    "    # utc_dt = datetime.now(datetime.timezone.utc)\n",
    "    target_dt = datetime.now(target_tz)\n",
    "    return datetime.strftime(target_dt, format_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e7e8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = f\"{PIPE_LINE_FILE_NAME} {get_local_time_str()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2118ac78-8984-475f-9c0b-d4c530e60ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kubernetes import client as k8s_client\n",
    "pipeline_config = dsl.PipelineConf()\n",
    "\n",
    "# pipeline_config.set_image_pull_secrets([k8s_client.V1ObjectReference(name=K8_GIT_SECRET_NAME, namespace=NAME_SPACE)])\n",
    "# pipeline_config.set_image_pull_policy(\"Always\")\n",
    "pipeline_config.set_image_pull_policy(\"IfNotPresent\")\n",
    "\n",
    "pipeline_args = {\n",
    "    \"model_root\": \"/mnt\",\n",
    "    # \"lm_model_type\": \"llama13B-chat\",\n",
    "    # \"lm_model_type\": \"mistral7B-inst02\",\n",
    "    \"lm_model_type\": \"mixtral8x7B-inst01\", # \"mistral7B-inst02\", # \"llama13B-chat\",\n",
    "    \"max_token_length\": 4096, # for llama2 models max_length is 4096\n",
    "    \"max_position_embeddings\": 3072, # for llama2 models, using chunk size of 3072\n",
    "    \"max_new_tokens\": 80, # the maximum new tokens to be generated by the causalLM \n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"temperature\": 0.001,\n",
    "    \"lm_device_map\": \"auto\",\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 0.8,\n",
    "    \"bucket_name\": \"scivias-medreports\",\n",
    "    \"s3_file_prefix\": \"trans2en/KK-SCIVIAS\", # used for filter the s3 file inside the trans2en folder\n",
    "    \"s3_file_max_count\": 1, # -1 for unlimited file match the prefix\n",
    "    \"verify_host\": True,\n",
    "    # \"s3_file_key\": \"trans2en/KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\",\n",
    "    # \"s3_file_key\": \"trans2en/KK-SCIVIAS-00008^0053673565^2019-04-29^KIIID.txt\",\n",
    "    \"s3_secrets\": \"add-scivias-medreport-secret\",\n",
    "    \"sql_secrets\": \"add-scivias-postgresdb-secret\",\n",
    "    \"embed_model_vendor\": \"baai\", # \"sentence-transformers\"\n",
    "    \"retriever_k\": 3, # the number of docs as context to be retrieved by RAG for RetrievalQA chain\n",
    "    \"user_query1\": \"What is the name of the patient? (Remember to include 'The name of the patient is' in your answer.)\",\n",
    "    \"user_query1_parser_question\": \"retrieve one: patient name\",\n",
    "    \"user_query1_target_var\": \"patient_name\",\n",
    "    \"user_query1_target_var_desc\": \"patient name\",\n",
    "    \"user_query2\": \"What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer.)\",\n",
    "    \"user_query2_vars\": [\"patient_name\"],\n",
    "    \"user_query2_parser_question\": \"retrieve one: patient age as number\",\n",
    "    \"user_query2_target_var\": \"patient_age\",\n",
    "    \"user_query2_target_var_desc\": \"patient age\",\n",
    "    \"sql_target_table\": \"llm_med_report_info\",\n",
    "    \"run_name\": RUN_NAME,\n",
    "    \"verbose\": False,\n",
    "    \"gpu_type\": \"40gb\", # \"20gb\", # \"40gb\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a646a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "NAMESPACE = client.get_user_namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "830b1ac0-ae9f-47e5-8896-1f3c1d7672ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/f5eabfa0-38e1-4828-a8ec-09d2adc66e22\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/003c2c51-4a83-45a6-8bf3-13c210198fce\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=003c2c51-4a83-45a6-8bf3-13c210198fce)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = client.create_run_from_pipeline_func(\n",
    "    pipeline_func=llm_pipeline,\n",
    "    arguments = pipeline_args, #{}\n",
    "    run_name = RUN_NAME,\n",
    "    pipeline_conf=pipeline_config,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    ")\n",
    "\n",
    "run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
