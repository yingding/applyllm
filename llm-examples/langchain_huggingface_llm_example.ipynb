{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4946d3e4-c86a-497c-a0e4-21c89b93799d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@created: 24.11.2023\\\n",
    "@updated: 27.11.2023\\\n",
    "@version: 1\n",
    "\n",
    "This notebook comprises of examples to use transformer, pytorch, llama2, langchain to achive entity extraction with engineered prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a922d0b-5532-4379-bf78-43f1a9724f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade --user jupyterlab==3.4.3 # for the KF 1.7.0 release\n",
    "# !{sys.executable} -m pip install --upgrade --user jupyterlab==3.6.6 # custom upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73cab26f-c8a7-4d87-8f13-2c5c094f6a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/jovyan/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mName: jupyterlab\n",
      "Version: 3.6.6\n",
      "Summary: JupyterLab computational environment\n",
      "Home-page: https://jupyter.org\n",
      "Author: Jupyter Development Team\n",
      "Author-email: jupyter@googlegroups.com\n",
      "License: \n",
      "Location: /home/jovyan/.local/lib/python3.8/site-packages\n",
      "Requires: ipython, jinja2, jupyter-core, jupyter-server, jupyter-server-ydoc, jupyter-ydoc, jupyterlab-server, nbclassic, notebook, packaging, tomli, tornado\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip show jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install --user --upgrade kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac05d7-4f2e-4fc0-99f8-bca2ffffe3cc",
   "metadata": {},
   "source": [
    "## Use the cuda 118 and torch 2.1.0 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a925dce-3bdf-4a58-9661-7715a977c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608244d5-b5e5-4740-b44e-81ea0d67ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda 11.7 version\n",
    "# !{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48662e9c-04da-455f-a80d-bd8136ee5f2e",
   "metadata": {},
   "source": [
    "## Additional technical informaiton\n",
    "#### Useful installation for KF notebook 1.7.0 cu111 drivers\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade transformers==4.31.0\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==1.10.2+cu111 fastai==2.7.12 fastcore==1.5.29 fastdownload==0.0.7 torchvision==0.11.3+cu111 --extra-index-url https://download.pytorch.org/whl/cu111\n",
    "#!{sys.executable} -m pip install --user --upgrade accelerate==0.20.3\n",
    "```\n",
    "cuda118\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==2.0.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "`xformers==0.0.21` need `torch==2.0.1`\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade xformers==0.0.21 torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "```\n",
    "\n",
    "show js loading with ipywidgets\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0 comm==0.1.4 jupyterlab-widgets==3.0.8 widgetsnbextension==4.0.8\n",
    "```\n",
    "\n",
    "uninstall\n",
    "```shell\n",
    "#!{sys.executable} -m pip uninstall accelerator transformers xformers torch -y \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "#### Basics of GPU\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "#### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "#### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "The following python command shall show the available MIG memory\n",
    "```shell\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)\n",
    "```\n",
    "The first tuple shows the availabe MIG cuda memory, if it goes to zero, and no process is attached,\n",
    "this means a cuda process is hang.\n",
    "```console\n",
    "(20748107776, 20937965568)\n",
    "19.32318115234375\n",
    "19.5\n",
    "```\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "```shell\n",
    "nvidia-smi # find out the PID something like 830333\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2594fcbd-b0a4-42e0-8ce8-e524423d7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from util.accelerator_utils import AcceleratorStatus, AcceleratorHelper\n",
    "\n",
    "# data volume mounted in kubeflow notebook\n",
    "MODEL_ROOT=\"/home/jovyan/llm-models\"\n",
    "MODEL_SUB_PATH = \"core-kind/yinwang\"\n",
    "# the cache dir for huggingface models\n",
    "MODEL_CACHE_DIR = f\"{MODEL_ROOT}/{MODEL_SUB_PATH}\"\n",
    "\n",
    "gpu_status = AcceleratorStatus()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a268056f-65d5-4e2c-a73a-75570990bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_helper = AcceleratorHelper()\n",
    "# dynamically fetch attached accelerator devices\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebd3f3cf-f74e-4377-964d-a5c0c1b667bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-9579f618-ddae-5958-9285-3207382f0b36\n",
      "/home/jovyan/llm-models/core-kind/yinwang/models\n"
     ]
    }
   ],
   "source": [
    "# init all the cuda torch env and model download cache directory\n",
    "gpu_helper.init_cuda_torch(UUIDs, MODEL_CACHE_DIR)\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(os.environ[\"XDG_CACHE_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "2.1.0+cu118\n"
     ]
    }
   ],
   "source": [
    "model_map = {\n",
    "        \"7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"13B\" : \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        \"70B\" : \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        # \"70B\" : \"meta-llama/Llama-2-70b-hf\" \n",
    "}\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the huggingface hub token\n",
    "\"\"\"\n",
    "token_sub_path = \".cache/huggingface/token\"\n",
    "token_file_path = f\"{MODEL_CACHE_DIR}/{token_sub_path}\"\n",
    "# stripe the leading and tailing EOL chars\n",
    "# https://stackoverflow.com/questions/275018/how-can-i-remove-a-trailing-newline/275025#275025\n",
    "with open (token_file_path, \"r\") as file:\n",
    "    # file read add a new line to the token, remove it.\n",
    "    # token = file.read().replace('\\n', '')    \n",
    "    token = file.read().strip()\n",
    "\n",
    "# print the raw string to see if there is new line in the token\n",
    "# print(r'{}'.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "# model_type = \"13B\"\n",
    "model_type = \"7B\"\n",
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    token=token, #transformer>=4.32.1\n",
    "    device_map=\"auto\", # put to GPU\n",
    "    # use_auth_token=token, #transformer==4.31.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd687825-b00c-4909-95f9-373e8f1bdc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "# not loading to the GPU with accelerator\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b07f9052-1ad4-41ff-af51-3bd4394e4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # will call the AutoModelForCausalLM automatically\n",
    "# generator = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_name,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     token=token, #transformer>=4.32.1\n",
    "#     #use_auth_token=token, #transformer==4.31.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c825194-473c-4f05-a1e4-9feb6b2475e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ecd282d41c41fdb07eae993ddc5560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# in Transformer 4.32.1 need to use \"token\" parameter\n",
    "# in Transformer 4.30.x need to use \"use_auth_token\" parameter\n",
    "# with torch.no_grad():\n",
    "# need to load the TextGenerationPipeline with max_new_tokens, since the HuggingFacePipeline reuse it, can not reload it later.\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    # model=model,\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer, # optional\n",
    "    torch_dtype=torch.bfloat16, # torch.float16, torch.float32\n",
    "    device_map=\"auto\",\n",
    "    # max_length=MAX_LENGTH,\n",
    "    max_length=None, # remove the total length of the generated response\n",
    "    max_new_tokens=100, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "    token=token, #transformer>=4.32.1\n",
    "    #use_auth_token=token, #transformer==4.31.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24bfaf57-d2e0-464d-bd8e-16eb98a6c4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 12.615234 GB\n",
      "Allocated memory : 12.613792 GB\n",
      "Free      memory : 0.001442 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c3232-c9e7-420a-8769-26dfdfb96b1b",
   "metadata": {},
   "source": [
    "## Passing temparature to the generator for each prompt\n",
    "\n",
    "https://discuss.huggingface.co/t/how-to-set-generation-parameters-for-transformers-pipeline/48837\n",
    "\n",
    "LLama2 chat agent\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-70b-chat-agent.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "267a6e3f-7456-4e36-af4d-7ca1ed807e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.1, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.1  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f775ca5a-1974-4837-8821-b556266cee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set DEBUG to false to remove all the llm answer outputs\n",
    "# DEBUG=True\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "897b8269-5855-4d49-8b2a-f028dcffb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_answer(answer: list)-> None:\n",
    "#     if DEBUG:\n",
    "#         print(\"-\"*10)\n",
    "#         print(answer[0])\n",
    "#         print(\"-\"*10)\n",
    "#         print(answer[0].split(\"\\n\")[-1])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d64be04-c528-426b-91f1-47b71996d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# def free_memory_gen(\n",
    "#     generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "#     tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     def local():\n",
    "#         l_generator = generator\n",
    "#         l_tokenizer = tokenizer\n",
    "#         #l_generator.cpu()\n",
    "#         #l_tokenizer.cpu()\n",
    "#         # model.cpu()\n",
    "        \n",
    "#         del l_tokenizer, l_generator\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         #for device_idx in range(torch.cuda.device_count()):\n",
    "#         #    print(device_idx)\n",
    "#         #    device = torch.device(f\"cuda:{device_idx}\")\n",
    "#         #    device.reset()\n",
    "#     return local    \n",
    "\n",
    "# free_memory = free_memory_gen(generator, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bfad65b-f23b-4e99-b724-57dd530b99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chain of thoughts prompting\n",
    "\n",
    "# testing prompt\n",
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: First, the cafeteria had 23 apples. Then, they used 20 to make lunch, leaving 3 apples. Finally, they bought 6 more, so they have 3 + 6 = 9 apples. The answer is 9.\n",
      "--------------------\n",
      "walltime: 1.9146647453308105 in secs.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 12.955078 GB\n",
      "Allocated memory : 12.621727 GB\n",
      "Free      memory : 0.333351 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])\n",
    "    \n",
    "# note: the expected answer is 9    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a433d07-366b-437f-a164-d7a84947b65e",
   "metadata": {},
   "source": [
    "## Huggingface with Local LLM\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/huggingface_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec0ecea0-3c65-4fd4-81c1-9847a6b2e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade langchain==0.0.340\n",
    "#!{sys.executable} -m pip install --user --upgrade langchain==0.0.313 \n",
    "\n",
    "# HuggingFacePipeline broken above version 0.0.313\n",
    "# HuggingFacePipeline works in version 0.0.312\n",
    "# !{sys.executable} -m pip install --user --upgrade langchain==0.0.312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ce7c3e4-0137-4605-9bdb-3d3ceb7f42c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.354\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "# till 0.0.350\n",
    "# from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "# from 0.0.354\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a04de-b5ea-4722-bd40-04ae88f4ed91",
   "metadata": {},
   "source": [
    "### Init a HuggingFacePipeline with pipeline_kwargs\n",
    "\n",
    "https://github.com/langchain-ai/langchain/issues/8280#issuecomment-1652085694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ba778fa-e2ad-42bf-bc58-6a1814f24395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_id  = \"TheBloke/wizardLM-7B-HF\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_id,\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={\"trust_remote_code\": True},\n",
    "#     pipeline_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"device_map\": \"auto\",\n",
    "#         \"max_new_tokens\": 1200,\n",
    "#         \"temperature\": 0.3,\n",
    "#         \"top_p\": 0.95,\n",
    "#         \"repetition_penalty\": 1.15,\n",
    "#     },\n",
    "# )\n",
    "# print(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9894f6e1-3867-4e08-b2a9-85f89455ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "print(llm.pipeline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcfdd2fa-8ed4-4f0f-bb97-c4ec02bf8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bug, the HuggingFacePipeine is not getting the param directly\n",
    "# https://github.com/langchain-ai/langchain/issues/8280\n",
    "\n",
    "# this must be set for the generator (HuggingFacePipeline) to work\n",
    "llm.model_id = model_name\n",
    "pipeline_kwargs_config = {\n",
    "    # \"do_sample\": True, # also making trouble with langchain\n",
    "    # \"top_k\": 10, # this param result in trouble with langchain\n",
    "    # \"num_return_sequences\": 1, #optional\n",
    "    # \"eos_token_id\": tokenizer.eos_token_id, # also making trouble\n",
    "    \"device_map\": \"auto\",\n",
    "    \"max_length\": 200,\n",
    "    \"max_new_tokens\": 80, # this is not taken by the model ?\n",
    "    \"temperature\": 0.1,\n",
    "    # \"top_p\": 0.95, # what is this?\n",
    "    \"repetition_penalty\": 1.15, # 1.15,\n",
    "}\n",
    "# llm.model_kwargs = {\"trust_remote_code\": True}\n",
    "llm.pipeline_kwargs = pipeline_kwargs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a400b78d-5f21-4ce8-a7ca-4388bdee6b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 12.955078 GB\n",
      "Allocated memory : 12.621727 GB\n",
      "Free      memory : 0.333351 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93b51595-0f2f-4076-b52d-94aaabdee9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "None\n",
      "{'device_map': 'auto', 'max_length': 200, 'max_new_tokens': 80, 'temperature': 0.1, 'repetition_penalty': 1.15}\n"
     ]
    }
   ],
   "source": [
    "print(llm.pipeline.model.name_or_path)\n",
    "print(llm.model_id)\n",
    "print(llm.model_kwargs)\n",
    "print(llm.pipeline_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccd4f4-bb44-4057-b60b-09bd668ff6d8",
   "metadata": {},
   "source": [
    "## Sequential Doc Chain\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2d1f291-f13b-493d-98a8-780321585b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7faeb14a5cd0>, model_id='meta-llama/Llama-2-7b-chat-hf', pipeline_kwargs={'device_map': 'auto', 'max_length': 200, 'max_new_tokens': 80, 'temperature': 0.1, 'repetition_penalty': 1.15})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89503a14-894e-4b45-94c0-636c896c0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 2 µs, total: 8 µs\n",
      "Wall time: 16.2 µs\n",
      "A: The cafeteria started with 23 apples. They used 20 to make lunch, so they have 23 - 20 = 3 apples left. Then they bought 6 more, so they have 3 + 6 = 9 apples. The answer is 9.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 12.955078 GB\n",
      "Allocated memory : 12.621727 GB\n",
      "Free      memory : 0.333351 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for _ in range(1): # is here a CPU bottleneck? for some reason, if called twice, the model lost the context, will hallucinate.\n",
    "    print(llm(inputs[0]))\n",
    "    gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "095866e7-777d-4f6d-a359-498bb5306a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c03817-da5b-445d-b3cc-ec7cd8a29cf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading documents from s3 bucket source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec43b44c-db33-48ff-bc1e-07da28287bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fd27939-7b6e-4a5c-884f-c4813825c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861587f9-f41e-4ce3-903b-ec50b17f4cce",
   "metadata": {},
   "source": [
    "#### zero shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b5316fa-82d8-4f47-98fa-9506916a1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name\n",
    "input=f\"Can you tell me the name of the patient from the folowing doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddd6b804-67bd-4c54-a7c2-0e5ef14aa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(input)\n",
    "# 6810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "780a0f12-7d99-4aba-9c8a-5fb1e28f456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c299015-c5b7-425b-8a79-ba155491c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age\n",
    "input=f\"Can you tell me the age of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fcf966a-76c1-4e7d-8a6d-0b356b2b86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d4d472f-80db-4d73-ac73-f673bc5fe279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnosis\n",
    "input=f\"Can you tell me the diagnosis of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51d7927d-9cba-4cbf-a05c-4fcaf886c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa11f-0eb2-4dac-aa70-194cfe8e577d",
   "metadata": {},
   "source": [
    "#### Chain-of-thoughts prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc858b23-c973-4dbc-9b4c-24131f90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name prompt\n",
    "input = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {context}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "#print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7881162a-6514-4f31-a64c-60718b2f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e61eea4-3731-4ba5-8c1b-e392cc6fe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "input = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{context}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "# print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca31ec3d-1912-4a07-aab0-c84151783112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "#len(input)\n",
    "# > 6913 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80fbeb59-1fe4-4ab0-b004-05b2fe2d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a401151-af8b-46f4-8ef2-0177ae075f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose prompt\n",
    "input=f\"Context:\\nPatient: Fried is a 34-year-old patient, Diagnoses: Influenza (J09.X2) \\nQuestion:\\nWhat diagnoses has the patient? \\nAnswer:\\nFried is a patient, 34 year-old, has diagnoses Influenza (J09.X2). The answers is Influenza (J09.X2)\\nContext:\\n{context}\\nQuestion:\\nWhat diagnoses has the patient?\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efd94444-a458-40dd-b95d-2be26f9ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "374b6e3c-3d58-4cf6-b834-1d066519e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "024d74a3-4ef3-4379-9075-a3944812244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
