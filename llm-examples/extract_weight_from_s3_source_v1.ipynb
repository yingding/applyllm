{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4946d3e4-c86a-497c-a0e4-21c89b93799d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@created: 28.02.2024\\\n",
    "@updated: 28.02.2024\\\n",
    "@version: 1\n",
    "\n",
    "This notebook contains llm inference pipeline (using transformer, pytorch, llama2, langchain) performed on an annotated properitory medical report dataset to extract weight information, the dataset is accessed from a s3 compatible storage.\n",
    "\n",
    "This notebook also demonstrate the use of `applyllm` PyPI lib for building a LLM RAG inference system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install --user --upgrade kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678ff5b4-48df-432f-a44b-eb82920ac42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade --no-cache-dir -i https://test.pypi.org/simple/ applyllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7b06b",
   "metadata": {},
   "source": [
    "### Libraries used in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2594fcbd-b0a4-42e0-8ce8-e524423d7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.14\n",
      "applyllm.__version__ : 0.0.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import applyllm\n",
    "\n",
    "print(python_version())\n",
    "print(f\"applyllm.__version__ : {applyllm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from applyllm.accelerators import (\n",
    "    AcceleratorHelper,\n",
    "    AcceleratorStatus,\n",
    "    DIR_MODE_MAP,\n",
    "    TokenHelper as th\n",
    ")\n",
    "\n",
    "dir_setting = dir_setting=DIR_MODE_MAP.get(\"kf_notebook\")\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a268056f-65d5-4e2c-a73a-75570990bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_helper = AcceleratorHelper()\n",
    "# dynamically fetch attached accelerator devices\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd3f3cf-f74e-4377-964d-a5c0c1b667bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-9579f618-ddae-5958-9285-3207382f0b36\n",
      "/home/jovyan/llm-models/core-kind/yinwang/models\n"
     ]
    }
   ],
   "source": [
    "# init all the cuda torch env and model download cache directory\n",
    "gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(os.environ[\"XDG_CACHE_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02ef10e-ae83-43ec-ad9e-b3bb80ea7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\":    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\":    \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    \"llama3-8B-inst\":   \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"llama3-70B-inst\":  \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    \"gemma7b-it\": \"google/gemma-7b-it\",\n",
    "    \"gemma7b\":    \"google/gemma-7b\",\n",
    "    \"gemma2b-it\": \"google/gemma-2b-it\",\n",
    "    \"gemma2b\":    \"google/gemma-2b\",\n",
    "    \"gemma7b-it-1.1\": \"google/gemma-1.1-7b-it\",\n",
    "    \"gemma2b-it-1.1\": \"google/gemma-1.1-2b-it\",\n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-inst02\"\n",
    "default_dir_mode = \"mac_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"mixtral8x7B-01\"\n",
    "model_type = \"mixtral8x7B-inst01\"\n",
    "# model_type = \"llama3-70B-inst\"\n",
    "# model_type = \"llama3-8B-inst\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, default_model_type)\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.39.3\n",
      "2.2.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "# from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "token_kwargs = th.gen_token_kwargs(model_type=model_type, dir_setting=dir_setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451058b-fdec-4165-a3a7-1883b36dbdc7",
   "metadata": {},
   "source": [
    "## Following this approach to load OSS LLM model with bitsandbytes quantization\n",
    "* https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f6116-4a42-4753-8f98-7f06ef53b03d",
   "metadata": {},
   "source": [
    "## 4bit  quantization\n",
    "\n",
    "Load pretrained model first, then the tokenizer.\n",
    "\n",
    "<table>\n",
    "    <!-- row 1-->\n",
    "<tr>\n",
    "<th>\n",
    "Llama-2-13b-chat-hf\n",
    "</th>\n",
    "<th>\n",
    "Mixtral-8x7B-v0.1\n",
    "</th>\n",
    "\n",
    "<th>\n",
    "Mixtral-8x7B-Instruct-v0.1\n",
    "</th>\n",
    "</tr>\n",
    "    <!-- row 2 -->\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 7.085938 GB\n",
    "Allocated memory : 6.809501 GB\n",
    "Free      memory : 0.276437 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 23.496094 GB\n",
    "Allocated memory : 23.303491 GB\n",
    "Free      memory : 0.192603 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 23.496094 GB\n",
    "Allocated memory : 23.303491 GB\n",
    "Free      memory : 0.192603 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9f2f7ed-e2b8-4f73-810e-7371df4bb44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalCausalLMConfig(quantized=True, model_config={'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'device_map': 'auto'}, quantization_config={'quantization_config': BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": true,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from applyllm.utils import time_func\n",
    "from applyllm.pipelines import (\n",
    "    LocalCausalLMConfig,\n",
    "    ModelConfig,\n",
    "    ModelCatalog,\n",
    ")\n",
    "\n",
    "#cuda_max_memory = {\n",
    "#   0: \"40GB\", # GPU device 0\n",
    "#   \"cpu\": \"160GB\", \n",
    "#} \n",
    "\n",
    "base_lm_config = ModelConfig(\n",
    "    model_config = {\n",
    "        \"pretrained_model_name_or_path\": model_name,\n",
    "        \"device_map\": \"auto\",\n",
    "        # \"max_memory\": cuda_max_memory,\n",
    "        # \"offload_state_dict\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "kwargs = {\n",
    "    \"quantized\": True,\n",
    "    \"model_config\": base_lm_config.get_config(),\n",
    "    \"quantization_config\": {\n",
    "        \"quantization_config\": transformers.BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "    }\n",
    "}\n",
    "lm_config = LocalCausalLMConfig(**kwargs)\n",
    "\n",
    "lm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ade2aa-df28-4a47-9dcc-3aef43c42826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e1c8deafe8411681d72050f73f99c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: fetch_model() python function\n",
      "walltime: 223.96892833709717 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "@time_func\n",
    "def fetch_model():\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "      **lm_config.get_config(),\n",
    "      **token_kwargs,  \n",
    "    )\n",
    "\n",
    "model = fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72c75b9-5d20-458e-9996-647105f0db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.527344 GB\n",
      "Allocated memory : 23.302026 GB\n",
      "Free      memory : 0.225318 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253126b-1dd9-4b0e-8c44-9c5df3191492",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Llama2 max position embeddings\n",
    "Default is set to be 2048\n",
    "* https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig.max_position_embeddings\n",
    "\n",
    "Set teh max_length for the tokenizer, Transformer issues:\n",
    "* https://github.com/huggingface/transformers/issues/1791#issuecomment-553397054\n",
    "* https://github.com/huggingface/transformers/pull/1833\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3bcda04-20d9-4469-b44e-c6fa391f03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_tokenizer(model_name: str, config: dict, pad_token_id = 2):\n",
    "    if model_name.startswith(ModelCatalog.MISTRAL_FAMILY):\n",
    "        return {**config, \"pad_token_id\": pad_token_id}\n",
    "    else:\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(model_config={'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'device': 'cpu', 'max_position_embeddings': 4096, 'max_length': 4096, 'pad_token_id': 2})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_POSITION_EMBEDDINGS = 3072\n",
    "MAX_LENGTH = 4096\n",
    "\n",
    "model_config= {\n",
    "    \"pretrained_model_name_or_path\": model_name,\n",
    "    \"device\": \"cpu\",\n",
    "    # \"device_map\": \"auto\", # put to GPU if GPU is available\n",
    "    \"max_position_embeddings\": MAX_LENGTH,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "}\n",
    "model_config = config_tokenizer(model_name=model_name, config=model_config)\n",
    "\n",
    "\n",
    "tokenizer_config = ModelConfig(model_config=model_config)\n",
    "tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc864b9a-eac7-4335-bbdc-a711b8b664b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-Instruct-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    **tokenizer_config.get_config(), \n",
    "    **token_kwargs,\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c182261-d89f-4826-9b67-5c43e89d1ff5",
   "metadata": {},
   "source": [
    "### Inference with transformers pipeline\n",
    "\n",
    "Reference:\n",
    "* https://huggingface.co/docs/transformers/pipeline_tutorial\n",
    "\n",
    "Note:\n",
    "* batch is not activated by default, batch is not necessary faster for `transformers.pipeline`\n",
    "* the max_new_tokens set in the pipeline initialization works with langchain.llms.HuggingFacePipeline, but not as a param for the TextGenerationPipeline \n",
    "\n",
    "max_new_tokens https://github.com/huggingface/transformers/issues/19358"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4a410-34b6-4b43-9bb1-3f72678b4f74",
   "metadata": {},
   "source": [
    "#### Return only generated text as LLM output\n",
    "new transformers.pipeline update\n",
    "```python\n",
    "\"return_full_text\": False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bacab17-1610-4cbe-8340-5c17e55ccd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 200 # 80\n",
    "\n",
    "tp_kwargs = {\n",
    "    \"task\": \"text-generation\",\n",
    "    \"model\": model,\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"device_map\": \"auto\",\n",
    "    \"max_length\": None, # remove the total length of the generated response\n",
    "    \"max_new_tokens\": MAX_NEW_TOKENS, # set 200 instead of 80, since it may cut off the json response. set the size of new generated token \n",
    "    \"return_full_text\": False, # return only the generated text, not the input text with the generated text\n",
    "}\n",
    "\n",
    "tp_config = ModelConfig(model_config = tp_kwargs)\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    **tp_config.get_config(),\n",
    "    **token_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77b83ae3-e8b1-4be6-b2e3-7819a9b5cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_generation.TextGenerationPipeline"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a433d07-366b-437f-a164-d7a84947b65e",
   "metadata": {},
   "source": [
    "### Huggingface with Local LLM\n",
    "\n",
    "* https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n",
    "\n",
    "HuggingFacePipeline from langchain need pydantic>=1.10.13\n",
    "\n",
    "```shell\n",
    "import pydantic\n",
    "print(pydantic.__version__)\n",
    "```\n",
    "* https://stackoverflow.com/questions/76313592/import-langchain-error-typeerror-issubclass-arg-1-must-be-a-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1b6fdb8-aef0-4129-81c4-fbf57f6b126c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelInfo(model_family='mistralai', inst_msg_begin='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n', inst_msg_end='[/INST]')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from applyllm.pipelines import ModelCatalog, ModelInfo, PromptHelper\n",
    "\n",
    "model_info = ModelCatalog.get_model_info(model_name=model_name)\n",
    "prompt_helper = PromptHelper(model_info=model_info)\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afb42d96-a471-4e01-8b47-3daf91a0ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pydantic.__version__: 1.10.13\n",
      "langchain.__version__: 0.1.16\n"
     ]
    }
   ],
   "source": [
    "import pydantic\n",
    "import langchain\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "print(f\"pydantic.__version__: {pydantic.__version__}\")\n",
    "print(f\"langchain.__version__: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a04de-b5ea-4722-bd40-04ae88f4ed91",
   "metadata": {},
   "source": [
    "### Init a HuggingFacePipeline with pipeline_kwargs\n",
    "\n",
    "https://github.com/langchain-ai/langchain/issues/8280#issuecomment-1652085694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9894f6e1-3867-4e08-b2a9-85f89455ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFacePipeline\u001b[0m\n",
      "Params: {'model_id': 'gpt2', 'model_kwargs': None, 'pipeline_kwargs': None}\n",
      "MixtralForCausalLM(\n",
      "  (model): MixtralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MixtralDecoderLayer(\n",
      "        (self_attn): MixtralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MixtralRotaryEmbedding()\n",
      "        )\n",
      "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
      "          (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
      "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): MixtralRMSNorm()\n",
      "        (post_attention_layernorm): MixtralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MixtralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "print(llm)\n",
    "print(llm.pipeline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcfdd2fa-8ed4-4f0f-bb97-c4ec02bf8e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 200, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'pad_token_id': 2, 'trust_remote_code': True}\n",
      "{'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 200, 'eos_token_id': 2, 'temperature': 0.01, 'repetition_penalty': 1.15, 'pad_token_id': 2}\n"
     ]
    }
   ],
   "source": [
    "# there is a bug, the HuggingFacePipeine is not getting the param directly\n",
    "# https://github.com/langchain-ai/langchain/issues/8280\n",
    "\n",
    "# this must be set for the generator (HuggingFacePipeline) to work\n",
    "llm.model_id = model_name\n",
    "pipeline_kwargs_config = {\n",
    "    \"device_map\": \"auto\",\n",
    "    \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": MAX_NEW_TOKENS, # this is not taken by the model ?\n",
    "    \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    \"temperature\": 0.01,\n",
    "    \"repetition_penalty\": 1.15, # 1.15,\n",
    "}\n",
    "model_kwargs_config = {\n",
    "    \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "    \"top_k\": 3, # this param result in trouble with langchain (optional)\n",
    "    \"num_return_sequences\": 1, # (optional)\n",
    "    \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": MAX_NEW_TOKENS, # this is not taken by the model ?\n",
    "    \"temperature\": 0.01,\n",
    "    \"top_p\": 0.8, # 0.95 # alternative to top_k summerized probability while do_sample=True\n",
    "    \"repetition_penalty\": 1.15, # 1.15,\n",
    "}\n",
    "\n",
    "llm.model_kwargs = config_tokenizer(model_name=model_name, config=model_kwargs_config, pad_token_id=tokenizer.eos_token_id)\n",
    "llm.model_kwargs[\"trust_remote_code\"] = True\n",
    "llm.pipeline_kwargs = config_tokenizer(model_name=model_name, config=pipeline_kwargs_config, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(llm.model_kwargs)\n",
    "print(llm.pipeline_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a400b78d-5f21-4ce8-a7ca-4388bdee6b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.527344 GB\n",
      "Allocated memory : 23.302026 GB\n",
      "Free      memory : 0.225318 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87d496-94e0-4c78-9788-e4771d01bf94",
   "metadata": {},
   "source": [
    "## Sequential Doc Chain\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bef3ff8d-b12b-42a3-bc75-ba07d3793633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.84\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "# from langchain.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "# from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "# from langchain.text_splitter import TextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List\n",
    "import boto3\n",
    "from applyllm.io import S3PdfObjHelper, DocMetaInfo, DocCorpusS3\n",
    "\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa49be-65ee-4245-9274-bdb505652e22",
   "metadata": {},
   "source": [
    "## Loading S3 objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d473b8ce-88bd-41d2-8ec5-d6a8d1b8ea66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans2en/KK-SCIVIAS\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"scivias-medreports\"\n",
    "file_prefix = \"KK-SCIVIAS\"\n",
    "PREFIX = f\"{S3PdfObjHelper.DataContract.key_lead}/{file_prefix}\"\n",
    "access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT')\n",
    "# VERIFY = False\n",
    "VERIFY = True\n",
    "print(PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6ee22-e1ab-4a23-9637-71246f1f538d",
   "metadata": {},
   "source": [
    "### Loading one text file\n",
    "* https://python.langchain.com/docs/integrations/document_loaders/aws_s3_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c551f269-4fb7-42cc-a07d-091a0b54d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_file_loader(key: str):\n",
    "    return S3FileLoader(bucket=bucket_name, \n",
    "                      key = key,\n",
    "                      aws_access_key_id=access_key_id,\n",
    "                      aws_secret_access_key=secret_access_key,\n",
    "                      endpoint_url=s3_endpoint,\n",
    "                      verify = VERIFY,\n",
    "                      use_ssl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63373df4-6ceb-4e25-84ce-ea452c2ea6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "@time_func\n",
    "def fetch_s3_object(key: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      a list of LangChain Document object\n",
    "    \"\"\"\n",
    "    loader = get_single_file_loader(key)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd094390",
   "metadata": {},
   "source": [
    "### Loading all data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47ae132f-626c-43fe-9317-ac667cb9ce10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prefix_files_loader(prefix: str):\n",
    "    return S3DirectoryLoader(bucket=bucket_name,\n",
    "                           prefix=prefix, \n",
    "                           aws_access_key_id=access_key_id, \n",
    "                           aws_secret_access_key=secret_access_key,\n",
    "                           endpoint_url=s3_endpoint,\n",
    "                           verify = VERIFY,\n",
    "                           use_ssl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5f4078a-0d6c-475f-92b0-948471cbb7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@time_func\n",
    "def fetch_s3_corpus(prefix: str) -> List[Document]:\n",
    "    loader = get_prefix_files_loader(prefix=prefix)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab833503-08c3-47d0-85e0-b40d1c352ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SINGLE_DOC = True\n",
    "\n",
    "# missing closing }\\n```\n",
    "# TEST_S3_DOC_KEY = \"trans2en/KK-SCIVIAS-00400^0054947100^2021-08-05^KIIID.txt\"\n",
    "# TEST_S3_DOC_KEY = \"trans2en/KK-SCIVIAS-00401^0052906626^2017-12-18^KIIID.txt\"\n",
    "TEST_S3_DOC_KEY = \"trans2en/KK-SCIVIAS-00403^0049215191^2011-06-30^KIIMUKO.txt\"\n",
    "\n",
    "# TEST_S3_DOC_KEY = \"trans2en/KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ac82f1c-fbc7-4dad-a221-f46d6ed5796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: fetch_s3_object() python function\n",
      "walltime: 3.9831676483154297 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "if TEST_SINGLE_DOC:\n",
    "    data = fetch_s3_object(key=TEST_S3_DOC_KEY)\n",
    "else:\n",
    "    data = fetch_s3_corpus(pefix=PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b06520a-948d-4f76-99d0-5931a33660e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Max Length Doc Info ---\n",
      "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00403^0049215191^2011-06-30^KIIMUKO.txt\n",
      "name:KK-SCIVIAS-00403^0049215191^2011-06-30^KIIMUKO.txt\n",
      "tokens:1734\n",
      "characters:13048\n",
      "--- Min Length Doc Info ---\n",
      "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00403^0049215191^2011-06-30^KIIMUKO.txt\n",
      "name:KK-SCIVIAS-00403^0049215191^2011-06-30^KIIMUKO.txt\n",
      "tokens:1734\n",
      "characters:13048\n"
     ]
    }
   ],
   "source": [
    "s3_corpus = DocCorpusS3(data)\n",
    "print(\"--- Max Length Doc Info ---\")\n",
    "print(s3_corpus.max_doc_meta)\n",
    "print(\"--- Min Length Doc Info ---\")\n",
    "print(s3_corpus.min_doc_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355ef0b-ff40-43f7-abe8-505dc01405e8",
   "metadata": {},
   "source": [
    "### Setting the current file index from the total corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75663609-b6b2-44d7-ae6d-752cee0c26fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TEST_SINGLE_DOC:\n",
    "    file_idx = 0 # ID 0003 has weight 43.2 kg\n",
    "else:\n",
    "    file_idx = 0 # ID 0003 has weight 43.2 kg\n",
    "    # file_idx = 1\n",
    "    # file_idx = idx_of_max_token\n",
    "\n",
    "show_content = False\n",
    "# show_content = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08ff4c4c-7dab-4088-bd83-61511e65ec94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total objects: 1\n",
      "====================\n",
      "s3 key     :s3://scivias-medreports/trans2en/KK-SCIVIAS-00403^0049215191^2011-06-30^KIIMUKO.txt\n",
      "obj name   :KK-SCIVIAS-00403^0049215191^2011-06-30^KIIMUKO.txt\n",
      "token size :1734\n",
      "char. size :13048\n"
     ]
    }
   ],
   "source": [
    "# the s3_corpus can be single doc corpus or multiple doc corpus\n",
    "# for testing purpose, just get one doc using file_idx\n",
    "CUR_DOC, CUR_DOC_INFO = s3_corpus.get_s3_obj_info(file_idx, show_content=show_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbf91c-2933-416e-940e-43b2ea414c5c",
   "metadata": {},
   "source": [
    "### Langchain text splitter\n",
    "\n",
    "* https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2a34e-b12c-4cba-bdc0-7fd3d27ad50d",
   "metadata": {},
   "source": [
    "## Restarting Point (Rerun below cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b65bcf1f-813f-481a-9e50-d9ebea07483a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(model_config={'chunk_size': 3000, 'chunk_overlap': 20, 'length_function': <function token_size at 0x7f31da610160>, 'is_separator_regex': False})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from applyllm.utils import token_size\n",
    "\n",
    "CHUNK_SIZE = (MAX_POSITION_EMBEDDINGS // 1000) * 1000\n",
    "model_config = {\n",
    "    # Set a really small chunk size, just to show.\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"chunk_overlap\": 20,\n",
    "    \"length_function\": token_size, # len,\n",
    "    \"is_separator_regex\": False,\n",
    "}\n",
    "\n",
    "splitter_config = ModelConfig(model_config=model_config)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    **splitter_config.get_config()\n",
    ")\n",
    "\n",
    "splitter_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "433b8ab8-07a8-4956-bd12-8d9352e590ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "len:    13048\n",
      "tokens: 1734\n"
     ]
    }
   ],
   "source": [
    "# Optional test of RecursiveCharacterTextSplitter on \\n and other chars\n",
    "test_text = CUR_DOC.page_content\n",
    "text_split_list = text_splitter.split_text(test_text)\n",
    "\n",
    "print(len(text_split_list))\n",
    "\n",
    "for seg in text_split_list:\n",
    "    print(f\"len:    {len(seg)}\")\n",
    "    print(f\"tokens: {token_size(seg)}\")\n",
    "# print(text_split_list[-1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90cbd4-e5fe-48d6-859d-0444a6e66d1b",
   "metadata": {},
   "source": [
    "### Langchain embeddings\n",
    "\n",
    "use sentence-transformers  \n",
    "\n",
    "* all-MiniLM-L12-v2 : 134MB https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 \n",
    "* all-MiniLM-L6-v2 : 90MB https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main\n",
    "\n",
    "Llama2 does not support document embedding by default\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n",
    "\n",
    "HuggingFaceEmbedding embed_documents example\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "In-memory vectorstore need DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "TextEmbeddings in LangChain\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "Sentence-transformers\n",
    "* https://medium.com/@madhur.prashant7/demo-langchain-rag-demo-on-llama-2-7b-embeddings-model-using-chainlit-559c10ce3fbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1c3086f-9735-4e16-afc4-80d91bafae2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_model_map = {\n",
    "    \"sentence-transformers\": \"sentence-transformers/all-MiniLM-L12-v2\", # 384\n",
    "    \"baai\" : \"BAAI/bge-base-en-v1.5\" # 768 embedding dims\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0aecbe3-434c-4c54-b4c2-2678bc9d4973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embed_model_vendor = \"sentence-transformers\"\n",
    "embed_model_vendor = \"baai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7d63645-9f9e-4bbf-b766-37e5077082bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_model_name = embed_model_map[embed_model_vendor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5df4e30-6ded-4be1-a6e3-20af0c674ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(model_config={'model_name': 'BAAI/bge-base-en-v1.5', 'model_kwargs': {'device': 'cpu'}, 'encode_kwargs': {'normalize_embeddings': True}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_kwargs = {'device': 'cpu'}\n",
    "# model_kwargs = {'device_map': \"auto\",}\n",
    "# encode_kwargs = {'normalize_embeddings': False}\n",
    "# encode_kwargs = {'normalize_embeddings': True} # for the cosin similarity search\n",
    "\n",
    "model_config = {\n",
    "    \"model_name\" : embed_model_name,\n",
    "    \"model_kwargs\": {'device': 'cpu'},\n",
    "    \"encode_kwargs\": {'normalize_embeddings': True}\n",
    "}\n",
    "embed_config = ModelConfig(model_config=model_config)\n",
    "\n",
    "# is downloaded at \"{MODEL_CACHE_DIR}/models/torch/sentence_transformer\" folder\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    **embed_config.get_config()\n",
    ")\n",
    "\n",
    "embed_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e02332-535e-4a00-88b3-3281e69ab372",
   "metadata": {},
   "source": [
    "## Langchain local LLM RAG\n",
    "\n",
    "Langchain Vectorstore and RAG approach differences:\n",
    "* https://github.com/langchain-ai/langchain/issues/5328\n",
    "\n",
    "Langchain RetrievalQA \n",
    "* https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "\n",
    "DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "LLama2 doesn't support Doc Embedding\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9679882f-495e-4262-8d5c-6883997ba053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAG one document\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embed_model,\n",
    "    text_splitter=text_splitter,\n",
    "    ).from_documents([data[file_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db3042e6-eeca-4129-9360-f34568f9b24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RETRIEVER_K = 3 # with two doc, there is not much i don't know\n",
    "retriever = index.vectorstore.as_retriever(search_kwargs={'k': RETRIEVER_K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e9489af-6dd6-4b82-8060-73f2da0ad5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# db = DocArrayInMemorySearch.from_documents(\n",
    "#     [data[file_idx]], embed_model)\n",
    "\n",
    "# retriever = db.as_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a7eec-2d15-4960-84e1-13ae3d40a130",
   "metadata": {},
   "source": [
    "#### Set the custom template\n",
    "\n",
    "Use the object variable, instead of kwargs\n",
    "https://github.com/langchain-ai/langchain/issues/6635#issuecomment-1659343109\n",
    "\n",
    "The reduce_prompt_template can be set\n",
    "```shell\n",
    "qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bb62d8e-187a-47db-8f07-48833172225a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template = \"\"\"\n",
    "# Given the following extracted parts of a long document and a question, create a final answer.\\n\n",
    "# If you don't know the answer, just say that you don't know. Don't try to make up an answer.\\n\\n\\n\n",
    "# =========\\n\n",
    "# QUESTION: {question}\\n\n",
    "# =========\\n\n",
    "# {summaries}\\n\n",
    "# =========\\n\n",
    "# FINAL ANSWER:\"\"\"\n",
    "\n",
    "# reduce_prompt_template = PromptTemplate(template=template, input_variables=['question', 'summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc66ce02-33e5-49f4-a380-28deb0185fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_template = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "map_template = prompt_helper.gen_prompt(query_template)\n",
    "\n",
    "\n",
    "map_prompt_template = PromptTemplate.from_template(map_template)\n",
    "map_prompt_template\n",
    "# Relevant text, if any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbba601d-c36f-45b5-88e8-e3af2a3ecd01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "# Ignore \"I don't know\" or \"not provided\" context text provided, do not use these as answer.\\n\n",
    "# If there are multiple relevant information in the context text provided, chose the majority of the relevant information as answer.\\n\n",
    "# If you know any answer, which is not \"I don't know\" or \"not provided\", chose the relevant information as answer.\\n\n",
    "# If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "\n",
    "# reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "# If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "# If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "#reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Always summarise the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "#If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "#If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "#\n",
    "#CONTEXT:/n/n {summaries}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the summarised answer below and nothing else.\n",
    "#Summarised answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "\n",
    "reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Always summarise the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "If you don't know the answer to a question, please don't share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "\n",
    "Only return the summarised answer below and nothing else.\n",
    "Summarised answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "reduce_prompt_template = PromptTemplate.from_template(reduce_template)\n",
    "reduce_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "340b2cd7-7d39-4d3d-945f-c05d2f9fdaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context_str', 'question'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nContext:/n/n {context_str}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n[/INST]')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#refine_init_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "#If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "#\n",
    "#Context:/n/n {context_str}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the helpful answer below and nothing else.\n",
    "#Helpful answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "# <|end|> for llama\n",
    "refine_init_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "Context:/n/n {context_str}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "init_prompt_template = PromptTemplate.from_template(refine_init_template)\n",
    "init_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3459329d-ad92-4c73-8084-ecfab6b1d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "# \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "\n",
    "# https://github.com/langchain-ai/langchain/issues/4613\n",
    "\n",
    "chain_type = \"map_reduce\"\n",
    "# chain_type = \"stuff\"\n",
    "# chain_type = \"refine\" \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "    # chain_type_kwargs={\"map_prompt\": map_prompt_template},\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    )\n",
    "# set the prompt template manually\n",
    "# use the original prompttemplate to do the summary, the current custom template doesn't have the one-short summary example, but just the right format.\n",
    "# qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b3c75aa-1fbf-4d2d-a601-a7d77976059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chain_type == \"map_reduce\":\n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "    # set the token max from 3000 to 4000\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.token_max = MAX_POSITION_EMBEDDINGS\n",
    "    \n",
    "    \n",
    "if chain_type == \"refine\":\n",
    "    # pass\n",
    "    qa_chain.combine_documents_chain.initial_llm_chain.prompt = init_prompt_template\n",
    "    # qa_chain.combine_documents_chain.refine_llm_chain.token_max = MAX_POSITION_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7d997-f0e4-4f60-837c-88c45784ce57",
   "metadata": {},
   "source": [
    "### Set token max or max token for the llm\n",
    "* https://github.com/langchain-ai/langchain/issues/434#issuecomment-1440312002\n",
    "* https://github.com/langchain-ai/langchain/issues/9341#issuecomment-1681306494\n",
    "* https://github.com/langchain-ai/langchain/issues/9341#issuecomment-1681306494"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc9e36-7a4f-42b5-abf0-c086aa0b3b91",
   "metadata": {},
   "source": [
    "## Set debug mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f775ca5a-1974-4837-8821-b556266cee97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set DEBUG to false to remove all the llm answer outputs\n",
    "# DEBUG=True\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e2db375-471a-413c-9e4a-c0245071d4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f3175003c70>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 200, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'pad_token_id': 2, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 200, 'eos_token_id': 2, 'temperature': 0.01, 'repetition_penalty': 1.15, 'pad_token_id': 2})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f3175003c70>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 200, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'pad_token_id': 2, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 200, 'eos_token_id': 2, 'temperature': 0.01, 'repetition_penalty': 1.15, 'pad_token_id': 2})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7f280fb5fa30>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f5af523-416d-4cf7-92b9-4eb1fb38c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the ICD10 diagonis of the patient? (Remember to include 'The name of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db64fcd7-e27d-45b6-877d-a3d2cb6b0455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the name of the patient? (Remember to include 'The name of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69cb1d99-648d-4cde-a535-a9f194c6c515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# import mlflow\n",
    "\n",
    "# with mlflow.start_run() as run:\n",
    "#    logged_model = mlflow.langchain.log_model(qa_chain, \"scivias_rag1\")\n",
    "\n",
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cabfb9f4-be94-4887-9905-9f844fe89511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"Response: {response['result']}\")\n",
    "    print('-'*20)\n",
    "    print(data[file_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23097cc6-475d-4a60-a581-51fe163c0dbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PromptParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8de91a84-088f-4e8b-8df1-1eeaa4678ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, TransformChain\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "# for LLama2\n",
    "#query_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "#If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "#\n",
    "#CONTEXT:/n/n {text}/n/n/n\n",
    "#\n",
    "#Question: {question}/n\n",
    "#{format_instructions}\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "# Mixtral gives leading extra text\n",
    "#parser_query_template = \"\"\"<s>[INST] You are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "#If you don't know the answer to a question, please don't share false information. Just return \"</s>\"\n",
    "#\n",
    "#CONTEXT:\n",
    "#{text}\n",
    "#\n",
    "#Question: \n",
    "#{question}\n",
    "#\n",
    "#{format_instructions}\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "# no text about explain, no leading text such as \"sure ..., ```json ... ```\"\n",
    "parser_query_template = \"\"\"<s>[INST] You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\n",
    "If you don't know the answer to a question, please don't share false information. Just return \"</s>\"\n",
    "\n",
    "CONTEXT:\n",
    "{text}\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "{format_instructions}\n",
    "[/INST]\"\"\"\n",
    "\n",
    "name_schema = ResponseSchema(name=\"patient_name\", description=\"patient name\")\n",
    "\n",
    "response_schema = [name_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c31875-3ade-42ce-a171-670c2c0eaf62",
   "metadata": {},
   "source": [
    "### LLama2 prompt style\n",
    "* https://colab.research.google.com/drive/1hRjxdj53MrL0cv5LOn1l0VetFC98JvGR?usp=sharing#scrollTo=IrVIuygNuBVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f6cca58-88a5-42b1-bb95-fa64748c84b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Default LLaMA-2 prompt style\n",
    "# B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "# B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "# def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "#     SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "#     prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "#     return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad27b24f-1a8c-4973-9229-45267e05fdae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
    "\n",
    "# instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "# Question: {question}\"\"\"\n",
    "# get_prompt(instruction, sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4cd23b0f-150f-40ad-9319-7032da913d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"patient_name\": string  // patient name\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12321b68-27e9-4789-9a0b-15bd72ef2a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt_template = PromptTemplate.from_template(get_prompt(instruction, sys_prompt))\n",
    "# prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47edb902-0751-4905-b451-6ee3d6ce8436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question', 'text'], partial_variables={'format_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"patient_name\": string  // patient name\\n}\\n```'}, template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\nCONTEXT:\\n{text}\\n\\nQuestion: \\n{question}\\n\\n{format_instructions}\\n[/INST]')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_template = ChatPromptTemplate.from_template(name_query_template) # ChatPromptTemplate create Human and Output in the text\n",
    "name_question=\"retrieve one: patient name\"\n",
    "\n",
    "# prompt_template = PromptTemplate.from_template(query_template)\n",
    "prompt_template = PromptTemplate(\n",
    "    template=parser_query_template,\n",
    "    input_variables=[\"text\",\"questions\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f4dbaad-894c-4ac7-bd7c-72d03e63211e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = response['result'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f137f7fb-e68b-4b12-8ac6-c9f74387231b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# messages = prompt_template.format_prompt(text=input_text, format_instructions=format_instructions)\n",
    "# messages = prompt_template.format_messages(text=input_text, format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db23d37d-4aae-4edc-a6ec-0a11f6102dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['question', 'text'], partial_variables={'format_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"patient_name\": string  // patient name\\n}\\n```'}, template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\nCONTEXT:\\n{text}\\n\\nQuestion: \\n{question}\\n\\n{format_instructions}\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f3175003c70>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 200, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'pad_token_id': 2, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 200, 'eos_token_id': 2, 'temperature': 0.01, 'repetition_penalty': 1.15, 'pad_token_id': 2}))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51c67d20-ce3b-4256-bb4f-0af283a8089d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True \n",
    "# parser_response = chain.invoke(input={\"text\":input_text, \"format_instructions\":format_instructions, \"question\":name_question}, temperature=0.001)\n",
    "dict_response = chain.invoke(input={\"text\":input_text, \"question\":name_question})\n",
    "# parser_response = chain.run(text=input_text, format_instructions=format_instructions, question=name_question, temperature=0.001)\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5460a479-a110-47d2-b2c7-de358b8f5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from applyllm.pipelines import StructuredOutputParserHelper as ParserHelper\n",
    "\n",
    "patient_name = ParserHelper.parse_response_dict(\n",
    "        parser_response=dict_response,\n",
    "        output_parser=output_parser,\n",
    "        text_key=\"text\"\n",
    "    ).get(\"patient_name\", \"\").strip()\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"patient_name is: {patient_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286bd31-8bb2-4e65-8a2b-23be2ac5f3a8",
   "metadata": {},
   "source": [
    "### Weight question\n",
    "\n",
    "PromptTemplate\n",
    "* https://www.comet.com/site/blog/introduction-to-prompt-templates-in-langchain/\n",
    "* https://stackoverflow.com/questions/77316112/langchain-how-do-input-variables-work-in-particular-how-is-context-replaced\n",
    "\n",
    "Structured Parser uses partial_variables in langchain:\n",
    "* https://python.langchain.com/docs/modules/model_io/output_parsers/types/structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dbc5db03-faaa-4870-9a66-6e617a629902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_name=f\"{patient_name}\" if patient_name is not None else \"\"\n",
    "# query = f\"What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer.)\"\n",
    "entity_query_template = \"\"\"What is the weight of the patient {patient_name} in kilogram? (Remember to include 'The weight of the patient is' in your answer)\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(entity_query_template)\n",
    "query = prompt_template.format(patient_name=patient_name) \n",
    "\n",
    "if DEBUG:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "69a41314-cb3f-45ff-9eac-7a8f2d6de58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import logging\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "def get_ml_logging_info(exp_name: str = \"scivias-med-reports\", run_surfix: str = \"weight\") -> Tuple[str, str]:\n",
    "    # Set the run name to time string\n",
    "    run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_name = f\"{run_name}_{run_surfix}\"\n",
    "    search_pattern = f\"name = '{exp_name}'\"\n",
    "    experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "    \n",
    "    if len(experiments) < 1:\n",
    "        exp_id = mlflow.create_experiment(name=exp_name)\n",
    "        print(f\"experiment with string id {exp_id} is created.\")\n",
    "    else:\n",
    "        exp_id = experiments[0].experiment_id\n",
    "        # experiment_id = experiments.experiment_id[0]\n",
    "        print(f\"experiment with string id {exp_id} is reused.\")\n",
    "    return exp_id, run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bcd9a56f-1c2e-4fb9-ac58-8db56a7a0861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment with string id 1 is reused.\n"
     ]
    }
   ],
   "source": [
    "'''mlflow log run start'''\n",
    "exp_id, run_name = get_ml_logging_info()\n",
    "mlflow.end_run()\n",
    "mlflow.set_experiment(experiment_id=exp_id)\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "chain_type = \"map_reduce\"\n",
    "# chain_type = \"stuff\"\n",
    "# chain_type = \"refine\" \n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "    # chain_type_kwargs={\"map_prompt\": map_prompt_template},\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3bd801cb-4e39-424e-8310-9b013f740a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if chain_type == \"map_reduce\":\n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "    # set the token max from 3000 to 4000\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.token_max = MAX_POSITION_EMBEDDINGS\n",
    "    \n",
    "    \n",
    "if chain_type == \"refine\":\n",
    "    # pass\n",
    "    qa_chain.combine_documents_chain.initial_llm_chain.prompt = init_prompt_template\n",
    "    # qa_chain.combine_documents_chain.refine_llm_chain.token_max = MAX_POSITION_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b52cd2df-08f5-45bc-9f5c-9f94926fe4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f3175003c70>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 200, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'pad_token_id': 2, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 200, 'eos_token_id': 2, 'temperature': 0.01, 'repetition_penalty': 1.15, 'pad_token_id': 2})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f3175003c70>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 200, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'pad_token_id': 2, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 200, 'eos_token_id': 2, 'temperature': 0.01, 'repetition_penalty': 1.15, 'pad_token_id': 2})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7f280fb5fa30>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9713d2c7-fcfe-4934-b0c1-b1ae9e7df4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qa_chain.combine_documents_chain.initial_llm_chain.prompt\n",
    "# qa_chain.combine_documents_chain.refine_llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a3bd4924-a9b3-4099-9b13-d3555faa8f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f3175003c70>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 200, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'pad_token_id': 2, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 200, 'eos_token_id': 2, 'temperature': 0.01, 'repetition_penalty': 1.15, 'pad_token_id': 2})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f3175003c70>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 200, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'pad_token_id': 2, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 200, 'eos_token_id': 2, 'temperature': 0.01, 'repetition_penalty': 1.15, 'pad_token_id': 2})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7f280fb5fa30>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3bbe2371-ed99-4cc0-9875-29c2d3bd5c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mlflow.log_param(\"map_prompt\", map_prompt_template.template)\n",
    "mlflow.log_param(\"user_query\", entity_query_template)\n",
    "mlflow.log_param(\"doc_name\", CUR_DOC_INFO.name)\n",
    "mlflow.log_param(\"doc_source\", CUR_DOC_INFO.source)\n",
    "mlflow.log_param(\"llm_model\", model_name)\n",
    "\n",
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe0b73af-3d35-4426-a272-1d63a8afdb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"Response: {response['result']}\")\n",
    "    print('-'*20)\n",
    "    print(data[file_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e7ef2daa-235d-4fd8-afee-180e3446812a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the string type\n",
    "entity_schema = ResponseSchema(name=\"patient_weight\", description=\"patient weight\")\n",
    "# age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\", type=\"int\")\n",
    "\n",
    "# response_schema = [age_schema]\n",
    "entity_output_parser = StructuredOutputParser.from_response_schemas([entity_schema])\n",
    "# age_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6584cd99-239b-476f-8932-05235dfeab48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"patient_weight\": string  // patient weight\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "entity_question=\"retrieve one: patient weight as number in kilograms\"\n",
    "\n",
    "format_instructions = entity_output_parser.get_format_instructions()\n",
    "chain.prompt.partial_variables[\"format_instructions\"] = format_instructions\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aea79c9e-923e-43b7-bdaa-abc17e2aadcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39761501-39b9-4cd2-81df-27be8c9bfd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "# parser_response = chain.invoke(input={\"text\":input_text, \"format_instructions\":format_instructions, \"question\":age_question}, temperature=0.001)\n",
    "# parser_response = chain.run(text=input_text, format_instructions=format_instructions, question=age_question, temperature=0.001)\n",
    "dict_response = chain.invoke(input={\"text\":input_text, \"question\":entity_question})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "147d412a-2fa3-485b-bd9d-7550f43a89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param(\"response_json\", dict_response.get(\"text\", \"\"))\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c11d626-8be5-405f-8421-9b8ec30b3ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_entity_float_obj = ParserHelper.parse_response_dict(\n",
    "        parser_response=dict_response,\n",
    "        output_parser=entity_output_parser,\n",
    "        text_key=\"text\",\n",
    "        verbose=DEBUG,\n",
    "    ).get(\"patient_weight\", \"\")\n",
    "\n",
    "try:\n",
    "    if isinstance(patient_entity_float_obj, str):\n",
    "        patient_entity_float_obj = patient_entity_float_obj.strip()\n",
    "        patient_entity_float = float(patient_entity_float_obj)\n",
    "    if isinstance(patient_entity_float_obj, float):\n",
    "        patient_entity_float = patient_entity_float_obj\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    patient_entity_float = -1.0\n",
    "\n",
    "if DEBUG:\n",
    "    # print(f\"str response: {dict_response}\")\n",
    "    print(f\"patient_weight is: {patient_entity_float}\")\n",
    "    print(f\"pateint_weight has type: {type(patient_entity_float)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74bf5f-fdad-4843-96c3-95844d7d91fa",
   "metadata": {},
   "source": [
    "### (optional) Additional Read\n",
    "\n",
    "GPT4All\n",
    "* https://python.langchain.com/docs/integrations/llms/gpt4all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3.10",
   "language": "python",
   "name": "llm3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
