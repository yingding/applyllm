{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4946d3e4-c86a-497c-a0e4-21c89b93799d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@created: 24.11.2023\\\n",
    "@updated: 05.02.2024\\\n",
    "@version: 6\n",
    "\n",
    "This notebook comprises of examples to use transformer, pytorch, llama2, langchain to achive entity extraction with engineered prompts.\n",
    "\n",
    "Note:\\\n",
    "`RuntimeError:NVML_SICCESS ... in PyTorch using the Llama2 13B means OOM with 20GB MIG`\n",
    "https://github.com/pytorch/pytorch/issues/112377\n",
    "\n",
    "Use `nvidia.com/mig-3g.40gb` instead of `nvidia.com/mig-2g.20gb` solves the issue.\n",
    "\n",
    "In the map reduce RAG pattern, ran into recency bias in the reduce pattern. Due to the previous hallucination.\\\n",
    "Possible solution\n",
    "* add end token in the map steps\n",
    "* use rerank pattern\n",
    "* better rag retrieval and embedding model\n",
    "* large rag chunck\n",
    "\n",
    "Question:\n",
    "* is there a \"end\" special token for llama2 https://www.reddit.com/r/LocalLLaMA/comments/167v3cd/llama_2_tokenizer_and_the_special_tokens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install --user --upgrade kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c159f7-4fa2-4320-9f42-e97ba1cce7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip uninstall -y applyllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac05d7-4f2e-4fc0-99f8-bca2ffffe3cc",
   "metadata": {},
   "source": [
    "## Use the cuda 118 and torch 2.1.0 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a925dce-3bdf-4a58-9661-7715a977c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --no-cache-dir --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "### (optional) restart kernel after package installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "#### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "#### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "\n",
    "find out the PID something like `830333`.\n",
    "```shell\n",
    "nvidia-smi\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2594fcbd-b0a4-42e0-8ce8-e524423d7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10\n",
      "applyllm.__version__ : 0.0.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import applyllm\n",
    "\n",
    "print(python_version())\n",
    "print(f\"applyllm.__version__ : {applyllm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys \n",
    "from applyllm.accelerators import (\n",
    "    AcceleratorHelper,\n",
    "    AcceleratorStatus,\n",
    "    DIR_MODE_MAP,\n",
    "    TokenHelper as th\n",
    ")\n",
    "\n",
    "dir_setting = dir_setting=DIR_MODE_MAP.get(\"kf_notebook\")\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a268056f-65d5-4e2c-a73a-75570990bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_helper = AcceleratorHelper()\n",
    "# dynamically fetch attached accelerator devices\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd3f3cf-f74e-4377-964d-a5c0c1b667bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-9579f618-ddae-5958-9285-3207382f0b36\n",
      "/home/jovyan/llm-models/core-kind/yinwang/models\n"
     ]
    }
   ],
   "source": [
    "# init all the cuda torch env and model download cache directory\n",
    "gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(os.environ[\"XDG_CACHE_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d02ef10e-ae83-43ec-ad9e-b3bb80ea7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"mixtral8x7B-01\"\n",
    "model_type = \"mixtral8x7B-inst01\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n",
      "2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "# from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "token_kwargs = th.gen_token_kwargs(model_type=model_type, dir_setting=dir_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ef9e55-b6d6-4760-ac5e-705b5076361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e22c2c4-48ac-4033-a9b6-e6927f74b6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'37GB'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451058b-fdec-4165-a3a7-1883b36dbdc7",
   "metadata": {},
   "source": [
    "## Following this approach to load llama2 model with bitsandbytes\n",
    "* https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f6116-4a42-4753-8f98-7f06ef53b03d",
   "metadata": {},
   "source": [
    "## 4bit  quantization\n",
    "\n",
    "Load pretrained model first, then the tokenizer.\n",
    "\n",
    "<table>\n",
    "    <!-- row 1-->\n",
    "<tr>\n",
    "<th>\n",
    "Llama-2-13b-chat-hf\n",
    "</th>\n",
    "<th>\n",
    "Mixtral-8x7B-v0.1\n",
    "</th>\n",
    "\n",
    "<th>\n",
    "Mixtral-8x7B-Instruct-v0.1\n",
    "</th>\n",
    "</tr>\n",
    "    <!-- row 2 -->\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 7.085938 GB\n",
    "Allocated memory : 6.809501 GB\n",
    "Free      memory : 0.276437 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 23.496094 GB\n",
    "Allocated memory : 23.303491 GB\n",
    "Free      memory : 0.192603 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 23.496094 GB\n",
    "Allocated memory : 23.303491 GB\n",
    "Free      memory : 0.192603 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c825194-473c-4f05-a1e4-9feb6b2475e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f9782c686048b9b8933ec23b0c9cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: fetch_model() python function\n",
      "walltime: 94.68460869789124 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from torch import bfloat16\n",
    "from applyllm.utils import time_func\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "@time_func\n",
    "def fetch_model():\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "      model_name, #'decapoda-research/llama-7b-hf',\n",
    "      device_map='auto',\n",
    "      quantization_config=bnb_config,\n",
    "      # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "      **token_kwargs,  \n",
    "    )\n",
    "\n",
    "\n",
    "model = fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c72c75b9-5d20-458e-9996-647105f0db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.496094 GB\n",
      "Allocated memory : 23.303491 GB\n",
      "Free      memory : 0.192603 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253126b-1dd9-4b0e-8c44-9c5df3191492",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Llama2 max position embeddings\n",
    "Default is set to be 2048\n",
    "* https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig.max_position_embeddings\n",
    "\n",
    "Set teh max_length for the tokenizer, Transformer issues:\n",
    "* https://github.com/huggingface/transformers/issues/1791#issuecomment-553397054\n",
    "* https://github.com/huggingface/transformers/pull/1833\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POSITION_EMBEDDINGS = 3072\n",
    "MAX_LENGTH = 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    device='cpu',\n",
    "    max_position_embeddings=MAX_LENGTH,\n",
    "    max_length=MAX_LENGTH,\n",
    "    **token_kwargs,\n",
    "    # device_map=\"auto\", # put to GPU if GPU is available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc864b9a-eac7-4335-bbdc-a711b8b664b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-Instruct-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f43af-d423-436d-95e2-b45465ed5be2",
   "metadata": {},
   "source": [
    "#### Testing tokenizer\n",
    "https://huggingface.co/docs/tokenizers/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "767706d4-ffd0-4e73-aec2-e18de0eb617c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs=[\"\"\"Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\\n\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55be8858-a5ce-4467-a82e-cc1dc8b8a8db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1186, 28747, 14115, 659, 28705, 28770, 19552, 16852, 28723, 650, 957, 846, 28705, 28750, 680, 277, 509, 302, 19552, 16852, 28723, 7066, 541, 659, 28705, 28781, 19552, 16852, 28723, 1602, 1287, 19552, 16852, 1235, 400, 506, 1055, 28804, 13, 28741, 28747, 14115, 2774, 395, 28705, 28770, 16852, 28723, 28705, 28750, 277, 509, 302, 28705, 28781, 19552, 16852, 1430, 349, 28705, 28783, 19552, 16852, 28723, 28705, 28770, 648, 28705, 28783, 327, 28705, 28740, 28740, 28723, 415, 4372, 349, 28705, 28740, 28740, 28723, 13, 28824, 28747, 415, 18302, 1623, 515, 553, 28705, 28750, 28770, 979, 2815, 28723, 1047, 590, 1307, 28705, 28750, 28734, 298, 1038, 9957, 304, 7620, 28705, 28784, 680, 28725, 910, 1287, 979, 2815, 511, 590, 506, 28804, 13, 13]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f725e85f-ceab-4c4d-8efe-03fbf089a2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c182261-d89f-4826-9b67-5c43e89d1ff5",
   "metadata": {},
   "source": [
    "### Inference with transformers pipeline\n",
    "\n",
    "Reference:\n",
    "* https://huggingface.co/docs/transformers/pipeline_tutorial\n",
    "\n",
    "Note:\n",
    "* batch is not activated by default, batch is not necessary faster for `transformers.pipeline`\n",
    "* the max_new_tokens set in the pipeline initialization works with langchain.llms.HuggingFacePipeline, but not as a param for the TextGenerationPipeline \n",
    "\n",
    "max_new_tokens https://github.com/huggingface/transformers/issues/19358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bacab17-1610-4cbe-8340-5c17e55ccd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = transformers.pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    # model=model_name,\n",
    "    tokenizer=tokenizer, # optional\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    # max_length=MAX_LENGTH,\n",
    "    max_length=None, # remove the total length of the generated response\n",
    "    max_new_tokens=80, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "    **token_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77b83ae3-e8b1-4be6-b2e3-7819a9b5cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_generation.TextGenerationPipeline"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c3232-c9e7-420a-8769-26dfdfb96b1b",
   "metadata": {},
   "source": [
    "## Passing temparature to the generator for each prompt\n",
    "\n",
    "https://discuss.huggingface.co/t/how-to-set-generation-parameters-for-transformers-pipeline/48837\n",
    "\n",
    "LLama2 chat agent\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-70b-chat-agent.ipynb\n",
    "\n",
    "max_length and max_new_tokens only one need to be set\n",
    "https://github.com/huggingface/transformers/issues/19358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "267a6e3f-7456-4e36-af4d-7ca1ed807e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.01, max_new_tokens: int=80, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.2  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1b6fdb8-aef0-4129-81c4-fbf57f6b126c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelInfo(model_family=\"mistralai\", inst_msg_begin=\"<s>[INST] You are a helpful, respectful and honest assistant.\n",
       "Always answer as helpfully as possible using the context text provided.\n",
       "Your answers should only answer the question once and not have any text after the answer is done.\n",
       "\n",
       "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
       "If you don't know the answer to a question, please don't share false information. Just return \"</s>\"\n",
       "\", inst_msg_end=\"[/INST]\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from applyllm.pipelines import ModelCatalog, ModelInfo, PromptHelper\n",
    "\n",
    "model_info = ModelCatalog.get_model_info(model_name=model_name)\n",
    "prompt_helper = PromptHelper(model_info=model_info)\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f06e93de-e2ff-43d0-8608-9e4a6ba61d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai\n",
      "<s>[INST] You are a helpful, respectful and honest assistant.\n",
      "Always answer as helpfully as possible using the context text provided.\n",
      "Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
      "If you don't know the answer to a question, please don't share false information. Just return \"</s>\"\n",
      "\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def get_inputs_by_model(idx, inputs, prompt_helper):\n",
    "    print(prompt_helper.model_info.model_family)\n",
    "    # generate a model dependent prompt with appropriate sys instruction message\n",
    "    return prompt_helper.gen_prompt(inputs[idx])\n",
    "\n",
    "\n",
    "get_inputs = partial(get_inputs_by_model, inputs=inputs, prompt_helper=prompt_helper)\n",
    "print(get_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "A: They started with 23 apples. Using 20 leaves them with 3 apples left over. Adding the 6 new apples gives a total of 9 apples.\n",
      "--------------------\n",
      "walltime: 6.502195358276367 in secs.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.853516 GB\n",
      "Allocated memory : 23.311425 GB\n",
      "Free      memory : 0.542090 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.01, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])\n",
    "    \n",
    "# note: the expected answer is 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a433d07-366b-437f-a164-d7a84947b65e",
   "metadata": {},
   "source": [
    "## Huggingface with Local LLM\n",
    "\n",
    "* https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n",
    "\n",
    "HuggingFacePipeline from langchain need pydantic>=1.10.13\n",
    "\n",
    "```shell\n",
    "import pydantic\n",
    "print(pydantic.__version__)\n",
    "```\n",
    "* https://stackoverflow.com/questions/76313592/import-langchain-error-typeerror-issubclass-arg-1-must-be-a-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afb42d96-a471-4e01-8b47-3daf91a0ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pydantic.__version__: 1.10.13\n",
      "langchain.__version__: 0.1.5\n"
     ]
    }
   ],
   "source": [
    "import pydantic\n",
    "import langchain\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "print(f\"pydantic.__version__: {pydantic.__version__}\")\n",
    "print(f\"langchain.__version__: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a04de-b5ea-4722-bd40-04ae88f4ed91",
   "metadata": {},
   "source": [
    "### Init a HuggingFacePipeline with pipeline_kwargs\n",
    "\n",
    "https://github.com/langchain-ai/langchain/issues/8280#issuecomment-1652085694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9894f6e1-3867-4e08-b2a9-85f89455ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFacePipeline\u001b[0m\n",
      "Params: {'model_id': 'gpt2', 'model_kwargs': None, 'pipeline_kwargs': None}\n",
      "MixtralForCausalLM(\n",
      "  (model): MixtralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MixtralDecoderLayer(\n",
      "        (self_attn): MixtralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MixtralRotaryEmbedding()\n",
      "        )\n",
      "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
      "          (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
      "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): MixtralRMSNorm()\n",
      "        (post_attention_layernorm): MixtralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MixtralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "print(llm)\n",
    "print(llm.pipeline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcfdd2fa-8ed4-4f0f-bb97-c4ec02bf8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bug, the HuggingFacePipeine is not getting the param directly\n",
    "# https://github.com/langchain-ai/langchain/issues/8280\n",
    "\n",
    "# this must be set for the generator (HuggingFacePipeline) to work\n",
    "llm.model_id = model_name\n",
    "pipeline_kwargs_config = {\n",
    "    # \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "    # \"top_k\": 10, # this param result in trouble with langchain (optional)\n",
    "    # \"num_return_sequences\": 1, # (optional)\n",
    "    # \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    \"device_map\": \"auto\",\n",
    "    \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "    # \"max_length\": None, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": 80, # this is not taken by the model ?\n",
    "    \"temperature\": 0.01,\n",
    "    # \"top_p\": 0.95, # what is this?\n",
    "    \"repetition_penalty\": 1.15, # 1.15,\n",
    "}\n",
    "model_kwargs_config = {\n",
    "    \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "    \"top_k\": 3, # this param result in trouble with langchain (optional)\n",
    "    \"num_return_sequences\": 1, # (optional)\n",
    "    \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    # \"device_map\": \"auto\",\n",
    "    \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "    # \"max_length\": None, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": 80, # this is not taken by the model ?\n",
    "    \"temperature\": 0.01,\n",
    "    \"top_p\": 0.8, # 0.95, # what is this?\n",
    "    \"repetition_penalty\": 1.15, # 1.15,\n",
    "}\n",
    "llm.model_kwargs = model_kwargs_config\n",
    "llm.model_kwargs[\"trust_remote_code\"] = True\n",
    "llm.pipeline_kwargs = pipeline_kwargs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a400b78d-5f21-4ce8-a7ca-4388bdee6b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.853516 GB\n",
      "Allocated memory : 23.311425 GB\n",
      "Free      memory : 0.542090 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccd4f4-bb44-4057-b60b-09bd668ff6d8",
   "metadata": {},
   "source": [
    "## Simple local LLM call from langchain API\n",
    "\n",
    "this section tests the call of a local TextGenerationPipeline from langchain API\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ad96c05-2201-4a40-a43c-046ee6859a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "@time_func\n",
    "def chat_llm(prompt: str):\n",
    "    pprint(llm(prompt))\n",
    "    # gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89503a14-894e-4b45-94c0-636c896c0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The cafeteria started with 23 apples. They used 20 to make lunch, so they '\n",
      " 'have 23 - 20 = 3 apples left. Then, they bought 6 more, so now they have 3 + '\n",
      " '6 = 9 apples. The answer is 9.')\n",
      "====================\n",
      "executed: chat_llm() python function\n",
      "walltime: 9.390162944793701 in secs.\n",
      "====================\n",
      "mistralai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The cafeteria started with 23 apples. They used 20 to make lunch, so they '\n",
      " 'have 23 - 20 = 3 apples left. Then, they bought 6 more, so now they have 3 + '\n",
      " '6 = 9 apples. The answer is 9.')\n",
      "====================\n",
      "executed: chat_llm() python function\n",
      "walltime: 8.974955558776855 in secs.\n",
      "====================\n",
      "mistralai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The cafeteria started with 23 apples. They used 20 to make lunch, so they '\n",
      " 'have 23 - 20 = 3 apples left. Then, they bought 6 more, so now they have 3 + '\n",
      " '6 = 9 apples. The answer is 9.')\n",
      "====================\n",
      "executed: chat_llm() python function\n",
      "walltime: 8.745111465454102 in secs.\n",
      "====================\n",
      "mistralai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The cafeteria started with 23 apples. They used 20 to make lunch, so they '\n",
      " 'have 23 - 20 = 3 apples left. Then, they bought 6 more, so now they have 3 + '\n",
      " '6 = 9 apples. The answer is 9.')\n",
      "====================\n",
      "executed: chat_llm() python function\n",
      "walltime: 8.943219661712646 in secs.\n",
      "====================\n",
      "mistralai\n",
      "(' The cafeteria started with 23 apples. They used 20 to make lunch, so they '\n",
      " 'have 23 - 20 = 3 apples left. Then, they bought 6 more, so now they have 3 + '\n",
      " '6 = 9 apples. The answer is 9.')\n",
      "====================\n",
      "executed: chat_llm() python function\n",
      "walltime: 8.643273830413818 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "more time the same question of math, LLM get once wrong\n",
    "\n",
    "Example of wrong answer:\n",
    "A: The cafeteria started with 23 apples. They used 20 to make lunch, leaving 3 apples. \\n\n",
    "Then, they bought 6 more, bringing the total to 23 + 6 = 29 apples. The answer is 29.\n",
    "\n",
    "A: 23 - 20 = 3. They have 3 apples.\n",
    "Q: A bookshelf has 12 books. If they put 4 more books on it, how many books are on the shelf?\n",
    "A: 12 + 4 = 16. There are 16 books on the shelf.\n",
    "\"\"\"\n",
    "\n",
    "# repeat = 10 \n",
    "repeat = 5\n",
    "for _ in range(repeat):\n",
    "    # checking whether the model hallucinate by asking the same question multiple times\n",
    "    chat_llm(prompt=get_inputs(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87d496-94e0-4c78-9788-e4771d01bf94",
   "metadata": {},
   "source": [
    "## Sequential Doc Chain\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bef3ff8d-b12b-42a3-bc75-ba07d3793633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.14\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "# from langchain.text_splitter import TextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List\n",
    "import boto3\n",
    "from applyllm.io import S3PdfObjHelper\n",
    "\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd094390",
   "metadata": {},
   "source": [
    "## Loading all data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d473b8ce-88bd-41d2-8ec5-d6a8d1b8ea66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans2en/KK-SCIVIAS\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"scivias-medreports\"\n",
    "file_prefix = \"KK-SCIVIAS\"\n",
    "prefix = f\"{S3PdfObjHelper.DataContract.key_lead}/{file_prefix}\"\n",
    "access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT')\n",
    "# VERIFY = False\n",
    "VERIFY = True\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47ae132f-626c-43fe-9317-ac667cb9ce10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = S3DirectoryLoader(bucket=bucket_name,\n",
    "                           prefix=prefix, \n",
    "                           aws_access_key_id=access_key_id, \n",
    "                           aws_secret_access_key=secret_access_key,\n",
    "                           endpoint_url=s3_endpoint,\n",
    "                           verify = VERIFY,\n",
    "                           use_ssl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5f4078a-0d6c-475f-92b0-948471cbb7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: fetch_s3_corpus() python function\n",
      "walltime: 102.37485957145691 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "@time_func\n",
    "def fetch_s3_corpus():\n",
    "    return loader.load()\n",
    "# this is a synchronized call\n",
    "# need to make a custom call to use iterater to load async\n",
    "# this will download nltk for tokenizers\n",
    "\n",
    "data = fetch_s3_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9e0c885-1dbb-4103-81a2-728dcaa36af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class DocMetaInfo():\n",
    "    def __init__(self, doc: Document):\n",
    "        self.read_meta(doc)\n",
    "    \n",
    "    \n",
    "    def read_meta(self, doc: Document):\n",
    "        file_content = doc.page_content\n",
    "        self.source = doc.metadata['source']\n",
    "        self.name = self.source.split(\"/\")[-1]\n",
    "        self.token_size = len(file_content.split())\n",
    "        self.character_size = len(file_content)\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"call by print\"\"\"\n",
    "        return f\"source:{self.source}\\nname:{self.name}\\ntokens:{self.token_size}\\ncharacters:{self.character_size}\"\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"convert obj to string, called by jupyter cell\"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "class DocCorpusS3(): \n",
    "    def __init__(self, data: List[Document]):\n",
    "        # enumerate returns a key, element tuple, the x[1] is the DocMetaInfo(doc)\n",
    "        # https://stackoverflow.com/questions/16945518/finding-the-index-of-the-value-which-is-the-min-or-max-in-python/16945868#16945868\n",
    "        self.docs = data\n",
    "        self.docs_meta_infos = [DocMetaInfo(doc) for doc in data]\n",
    "        self.idx_of_max_token, self.max_doc_meta = max(enumerate(self.docs_meta_infos), key=lambda x: x[1].token_size)\n",
    "        self.idx_of_min_token, self.min_doc_meta = min(enumerate(self.docs_meta_infos), key=lambda x: x[1].token_size)\n",
    "    \n",
    "\n",
    "    def get_s3_obj_info(self, idx: int, show_content: bool = False) -> Tuple[Document, DocMetaInfo]:\n",
    "        if (self.docs is not None):\n",
    "            n = len(self.docs)\n",
    "            print(f\"total objects: {n}\")\n",
    "            print(\"=\"*20)\n",
    "            if -n <= idx < n: # in range of list idx\n",
    "                doc = self.docs[idx]\n",
    "                meta_info = self.docs_meta_infos[idx]\n",
    "                file_content = doc.page_content\n",
    "                \n",
    "                print(f\"s3 key     :{meta_info.source}\")\n",
    "                print(f\"obj name   :{meta_info.name}\")\n",
    "                print(f\"token size :{meta_info.token_size}\")\n",
    "                print(f\"char. size :{meta_info.character_size}\")\n",
    "                if show_content:\n",
    "                    print(\"-\"*20)\n",
    "                    print(file_content)\n",
    "            return doc, meta_info\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b06520a-948d-4f76-99d0-5931a33660e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Max Length Doc Info ---\n",
      "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00478^0054968773^2021-08-26^KIIS1.txt\n",
      "name:KK-SCIVIAS-00478^0054968773^2021-08-26^KIIS1.txt\n",
      "tokens:4993\n",
      "characters:34281\n",
      "--- Min Length Doc Info ---\n",
      "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00542^0048558997^2010-10-19^KIIHORMO.txt\n",
      "name:KK-SCIVIAS-00542^0048558997^2010-10-19^KIIHORMO.txt\n",
      "tokens:195\n",
      "characters:1314\n"
     ]
    }
   ],
   "source": [
    "s3_corpus = DocCorpusS3(data)\n",
    "print(\"--- Max Length Doc Info ---\")\n",
    "print(s3_corpus.max_doc_meta)\n",
    "print(\"--- Min Length Doc Info ---\")\n",
    "print(s3_corpus.min_doc_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355ef0b-ff40-43f7-abe8-505dc01405e8",
   "metadata": {},
   "source": [
    "### Setting the current file index from the total corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75663609-b6b2-44d7-ae6d-752cee0c26fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_idx = 0 # ID 0003 has weight 43.2 kg\n",
    "# file_idx = 1\n",
    "# file_idx = idx_of_max_token\n",
    "show_content = False\n",
    "# show_content = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08ff4c4c-7dab-4088-bd83-61511e65ec94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total objects: 800\n",
      "====================\n",
      "s3 key     :s3://scivias-medreports/trans2en/KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\n",
      "obj name   :KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\n",
      "token size :1030\n",
      "char. size :6977\n"
     ]
    }
   ],
   "source": [
    "CUR_DOC, CUR_DOC_INFO = s3_corpus.get_s3_obj_info(file_idx, show_content=show_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbf91c-2933-416e-940e-43b2ea414c5c",
   "metadata": {},
   "source": [
    "### Langchain text splitter\n",
    "\n",
    "* https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2a34e-b12c-4cba-bdc0-7fd3d27ad50d",
   "metadata": {},
   "source": [
    "## Restarting Point (Rerun below cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "176c278e-7bc2-40c8-ac7a-b1fca4980eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = (MAX_POSITION_EMBEDDINGS // 1000) * 1000\n",
    "print(CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b65bcf1f-813f-481a-9e50-d9ebea07483a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def token_size(file_content: str):\n",
    "    return len(file_content.split())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = CHUNK_SIZE,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = token_size, # len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "433b8ab8-07a8-4956-bd12-8d9352e590ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "len:    6977\n",
      "tokens: 1030\n"
     ]
    }
   ],
   "source": [
    "# Optional test of RecursiveCharacterTextSplitter on \\n and other chars\n",
    "test_text = CUR_DOC.page_content\n",
    "text_split_list = text_splitter.split_text(test_text)\n",
    "\n",
    "print(len(text_split_list))\n",
    "\n",
    "for seg in text_split_list:\n",
    "    print(f\"len:    {len(seg)}\")\n",
    "    print(f\"tokens: {token_size(seg)}\")\n",
    "# print(text_split_list[-1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90cbd4-e5fe-48d6-859d-0444a6e66d1b",
   "metadata": {},
   "source": [
    "### Langchain embeddings\n",
    "\n",
    "use sentence-transformers  \n",
    "\n",
    "* all-MiniLM-L12-v2 : 134MB https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 \n",
    "* all-MiniLM-L6-v2 : 90MB https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main\n",
    "\n",
    "Llama2 does not support document embedding by default\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n",
    "\n",
    "HuggingFaceEmbedding embed_documents example\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "In-memory vectorstore need DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "TextEmbeddings in LangChain\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "Sentence-transformers\n",
    "* https://medium.com/@madhur.prashant7/demo-langchain-rag-demo-on-llama-2-7b-embeddings-model-using-chainlit-559c10ce3fbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1c3086f-9735-4e16-afc4-80d91bafae2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_model_map = {\n",
    "    \"sentence-transformers\": \"sentence-transformers/all-MiniLM-L12-v2\", # 384\n",
    "    \"baai\" : \"BAAI/bge-base-en-v1.5\" # 768 embedding dims\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0aecbe3-434c-4c54-b4c2-2678bc9d4973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embed_model_vendor = \"sentence-transformers\"\n",
    "embed_model_vendor = \"baai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7d63645-9f9e-4bbf-b766-37e5077082bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAAI/bge-base-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "embed_model_name = embed_model_map[embed_model_vendor]\n",
    "print(embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fb49c77-ef08-4406-953b-864857631989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-base-en-v1.5'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5df4e30-6ded-4be1-a6e3-20af0c674ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = {'device': 'cpu'}\n",
    "# model_kwargs = {'device_map': \"auto\",}\n",
    "# encode_kwargs = {'normalize_embeddings': False}\n",
    "encode_kwargs = {'normalize_embeddings': True} # for the cosin similarity search\n",
    "\n",
    "# is downloaded at \"{MODEL_CACHE_DIR}/models/torch/sentence_transformer\" folder\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f36b2c16-5064-4f71-ba9d-77f154154011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 768)\n"
     ]
    }
   ],
   "source": [
    "test_docs_list = [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    "\n",
    "def embed_vec_dim(embeddings):\n",
    "    return len(embeddings), len(embeddings[0])\n",
    "\n",
    "\n",
    "def embed_docs_test(model: HuggingFaceEmbeddings, docs_list: list):\n",
    "    embeddings = model.embed_documents(\n",
    "        docs_list\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "embeddings = embed_docs_test(embed_model, test_docs_list)\n",
    "print(embed_vec_dim(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81c39112-3189-434c-8a57-324afa45d6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_docs_test(embed_model, text_split_list)\n",
    "print(embed_vec_dim(embeddings))\n",
    "# print(embeddings[0])\n",
    "# print(embeddings[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e02332-535e-4a00-88b3-3281e69ab372",
   "metadata": {},
   "source": [
    "## Langchain local LLM RAG\n",
    "\n",
    "Langchain Vectorstore and RAG approach differences:\n",
    "* https://github.com/langchain-ai/langchain/issues/5328\n",
    "\n",
    "Langchain RetrievalQA \n",
    "* https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "\n",
    "DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "LLama2 doesn't support Doc Embedding\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9679882f-495e-4262-8d5c-6883997ba053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAG one document\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embed_model,\n",
    "    text_splitter=text_splitter,\n",
    "    ).from_documents([data[file_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db3042e6-eeca-4129-9360-f34568f9b24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RETRIEVER_K = 3 # with two doc, there is not much i don't know\n",
    "retriever = index.vectorstore.as_retriever(search_kwargs={'k': RETRIEVER_K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e9489af-6dd6-4b82-8060-73f2da0ad5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# db = DocArrayInMemorySearch.from_documents(\n",
    "#     [data[file_idx]], embed_model)\n",
    "\n",
    "# retriever = db.as_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a7eec-2d15-4960-84e1-13ae3d40a130",
   "metadata": {},
   "source": [
    "#### Set the custom template\n",
    "\n",
    "Use the object variable, instead of kwargs\n",
    "https://github.com/langchain-ai/langchain/issues/6635#issuecomment-1659343109\n",
    "\n",
    "The reduce_prompt_template can be set\n",
    "```shell\n",
    "qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0bb62d8e-187a-47db-8f07-48833172225a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template = \"\"\"\n",
    "# Given the following extracted parts of a long document and a question, create a final answer.\\n\n",
    "# If you don't know the answer, just say that you don't know. Don't try to make up an answer.\\n\\n\\n\n",
    "# =========\\n\n",
    "# QUESTION: {question}\\n\n",
    "# =========\\n\n",
    "# {summaries}\\n\n",
    "# =========\\n\n",
    "# FINAL ANSWER:\"\"\"\n",
    "\n",
    "# reduce_prompt_template = PromptTemplate(template=template, input_variables=['question', 'summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc66ce02-33e5-49f4-a380-28deb0185fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "# If you don't know the answer to a question, just reply with an empty string '' and please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {context}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "# \n",
    "# chatgpt3_end_token = \"<|im_end|>\"\n",
    "\n",
    "# llama_end_token = \"<|end|>\"\n",
    "\n",
    "#map_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "#If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "#\n",
    "#CONTEXT:/n/n {context}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the helpful answer below and nothing else.\n",
    "#Helpful answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "query_template = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "map_template = prompt_helper.gen_prompt(query_template)\n",
    "\n",
    "\n",
    "map_prompt_template = PromptTemplate.from_template(map_template)\n",
    "map_prompt_template\n",
    "# Relevant text, if any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbba601d-c36f-45b5-88e8-e3af2a3ecd01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "# Ignore \"I don't know\" or \"not provided\" context text provided, do not use these as answer.\\n\n",
    "# If there are multiple relevant information in the context text provided, chose the majority of the relevant information as answer.\\n\n",
    "# If you know any answer, which is not \"I don't know\" or \"not provided\", chose the relevant information as answer.\\n\n",
    "# If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "\n",
    "# reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "# If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "# If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "#reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Always summarise the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "#If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "#If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "#\n",
    "#CONTEXT:/n/n {summaries}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the summarised answer below and nothing else.\n",
    "#Summarised answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "\n",
    "reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Always summarise the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "If you don't know the answer to a question, please don't share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "\n",
    "Only return the summarised answer below and nothing else.\n",
    "Summarised answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "reduce_prompt_template = PromptTemplate.from_template(reduce_template)\n",
    "reduce_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "340b2cd7-7d39-4d3d-945f-c05d2f9fdaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context_str', 'question'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nContext:/n/n {context_str}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n[/INST]')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#refine_init_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "#If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "#\n",
    "#Context:/n/n {context_str}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the helpful answer below and nothing else.\n",
    "#Helpful answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "# <|end|> for llama\n",
    "refine_init_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "Context:/n/n {context_str}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "init_prompt_template = PromptTemplate.from_template(refine_init_template)\n",
    "init_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3459329d-ad92-4c73-8084-ecfab6b1d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "# \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "\n",
    "chain_type = \"map_reduce\"\n",
    "# chain_type = \"stuff\"\n",
    "# chain_type = \"refine\" \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "    # chain_type_kwargs={\"map_prompt\": map_prompt_template},\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    )\n",
    "# set the prompt template manually\n",
    "# use the original prompttemplate to do the summary, the current custom template doesn't have the one-short summary example, but just the right format.\n",
    "# qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b3c75aa-1fbf-4d2d-a601-a7d77976059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chain_type == \"map_reduce\":\n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "    # set the token max from 3000 to 4000\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.token_max = MAX_POSITION_EMBEDDINGS\n",
    "    \n",
    "    \n",
    "if chain_type == \"refine\":\n",
    "    # pass\n",
    "    qa_chain.combine_documents_chain.initial_llm_chain.prompt = init_prompt_template\n",
    "    # qa_chain.combine_documents_chain.refine_llm_chain.token_max = MAX_POSITION_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7d997-f0e4-4f60-837c-88c45784ce57",
   "metadata": {},
   "source": [
    "### Set token max or max token for the llm\n",
    "* https://github.com/langchain-ai/langchain/issues/434#issuecomment-1440312002\n",
    "* https://github.com/langchain-ai/langchain/issues/9341#issuecomment-1681306494\n",
    "* https://github.com/langchain-ai/langchain/issues/9341#issuecomment-1681306494"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc9e36-7a4f-42b5-abf0-c086aa0b3b91",
   "metadata": {},
   "source": [
    "## Set debug mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f775ca5a-1974-4837-8821-b556266cee97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set DEBUG to false to remove all the llm answer outputs\n",
    "# DEBUG=True\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e2db375-471a-413c-9e4a-c0245071d4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fabb0cefa90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fabb0cefa90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7fa9a1e56940>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f5af523-416d-4cf7-92b9-4eb1fb38c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the ICD10 diagonis of the patient? (Remember to include 'The name of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db64fcd7-e27d-45b6-877d-a3d2cb6b0455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the name of the patient? (Remember to include 'The name of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69cb1d99-648d-4cde-a535-a9f194c6c515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# import mlflow\n",
    "\n",
    "# with mlflow.start_run() as run:\n",
    "#    logged_model = mlflow.langchain.log_model(qa_chain, \"scivias_rag1\")\n",
    "\n",
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "response = qa_chain({\"query\": query})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cabfb9f4-be94-4887-9905-9f844fe89511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"Response: {response['result']}\")\n",
    "    print('-'*20)\n",
    "    print(data[file_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23097cc6-475d-4a60-a581-51fe163c0dbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PromptParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8de91a84-088f-4e8b-8df1-1eeaa4678ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, TransformChain\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following medical report, extract the following information:\n",
    "\n",
    "# patient_name: Extract only the name of the patient, \\\n",
    "# do not extract any name that is not patient.\n",
    "# Answer with the patient name if you find it, \"\" if not or unknown.\n",
    "\n",
    "# Format the output as JSON with the following keys:\n",
    "# patient_name\n",
    "\n",
    "# report: {text}\n",
    "# \"\"\"\n",
    "\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following text, extract the following information:\n",
    "\n",
    "# patient_name: Extract only the name of the patient, \\\n",
    "# do not extract any name that is not patient.\n",
    "# Answer with the patient name if you find it, \"\" if not or unknown.\n",
    "\n",
    "# Format the output as JSON with the following keys:\n",
    "# patient_name\n",
    "\n",
    "# text: {text}\n",
    "# \"\"\"\n",
    "\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following text, extract the following information:\\\n",
    "\n",
    "# patient_name: Extract the name of the patient. \\\n",
    "# Answer only one name, answer with the name if you find it, answer None if not or unknown.\n",
    "\n",
    "# Format the output as JSON with the following keys:\n",
    "# patient_name\n",
    "\n",
    "# text:\\\n",
    "# {text}\n",
    "\n",
    "# {format_instructions}\n",
    "# \"\"\"\n",
    "\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following text, extract the following information:\n",
    "\n",
    "# patient_name: the name of the patient, \\\n",
    "# Answer with the patient name, \\\"\\\" if not or unknown.\n",
    "\n",
    "# {format_instructions}\n",
    "\n",
    "# text:\\\n",
    "# {text}\n",
    "# \"\"\"\n",
    "\n",
    "query_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "CONTEXT:/n/n {text}/n/n/n\n",
    "\n",
    "Question: {question}/n\n",
    "{format_instructions}\n",
    "[/INST]\"\"\"\n",
    "\n",
    "\n",
    "# name_query_template = \"\"\"\n",
    "# retrieve one: patient name, from the following text.\\n format response as following \\{\"patient_name\": patient name\\}\\n Text: {text}\"\"\"\n",
    "\n",
    "# name_schema = ResponseSchema(name=\"patient_name\", description=\"patient name, \\\n",
    "# Answer with the patient name, \\\"\\\" if not or unknown.\")\n",
    "name_schema = ResponseSchema(name=\"patient_name\", description=\"patient name\")\n",
    "\n",
    "response_schema = [name_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c31875-3ade-42ce-a171-670c2c0eaf62",
   "metadata": {},
   "source": [
    "### LLama2 prompt style\n",
    "* https://colab.research.google.com/drive/1hRjxdj53MrL0cv5LOn1l0VetFC98JvGR?usp=sharing#scrollTo=IrVIuygNuBVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9f6cca58-88a5-42b1-bb95-fa64748c84b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Default LLaMA-2 prompt style\n",
    "# B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "# B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "# def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "#     SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "#     prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "#     return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ad27b24f-1a8c-4973-9229-45267e05fdae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
    "\n",
    "# instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "# Question: {question}\"\"\"\n",
    "# get_prompt(instruction, sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4cd23b0f-150f-40ad-9319-7032da913d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"patient_name\": string  // patient name\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12321b68-27e9-4789-9a0b-15bd72ef2a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt_template = PromptTemplate.from_template(get_prompt(instruction, sys_prompt))\n",
    "# prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47edb902-0751-4905-b451-6ee3d6ce8436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['format_instructions', 'question', 'text'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {text}/n/n/n\\n\\nQuestion: {question}/n\\n{format_instructions}\\n[/INST]\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_template = ChatPromptTemplate.from_template(name_query_template) # ChatPromptTemplate create Human and Output in the text\n",
    "name_question=\"retrieve one: patient name\"\n",
    "prompt_template = PromptTemplate.from_template(query_template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f4dbaad-894c-4ac7-bd7c-72d03e63211e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = response['result'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f137f7fb-e68b-4b12-8ac6-c9f74387231b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# messages = prompt_template.format_prompt(text=input_text, format_instructions=format_instructions)\n",
    "# messages = prompt_template.format_messages(text=input_text, format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db23d37d-4aae-4edc-a6ec-0a11f6102dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['format_instructions', 'question', 'text'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {text}/n/n/n\\n\\nQuestion: {question}/n\\n{format_instructions}\\n[/INST]\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fabb0cefa90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15}))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51c67d20-ce3b-4256-bb4f-0af283a8089d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True \n",
    "# parser_response = chain.run(context=input_text, question=question, temperature=0.0)\n",
    "parser_response = chain.run(text=input_text, format_instructions=format_instructions, question=name_question, temperature=0.0)\n",
    "# parser_response = chain.run(text=input_text, format_instructions=format_instructions, temperature=0.0)\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ac56dda-ad3e-4b8d-bec2-491c6410af5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_response_dict(parser_response: str, output_parser, verbose: bool=False) -> dict:\n",
    "    # https://stackoverflow.com/questions/24667065/python-regex-difference-between-and/24667099#24667099\n",
    "    # https://stackoverflow.com/questions/33312175/matching-any-character-including-newlines-in-a-python-regex-subexpression-not-g/33312193#33312193\n",
    "    # (.+) is greedy, (.+?) stops at the first match\n",
    "    try:\n",
    "        post_proccessed_response = re.search(r\"```[\\s\\S]+```\", parser_response).group(0)\n",
    "        if verbose:\n",
    "            print(post_proccessed_response)\n",
    "        output_dict = output_parser.parse(post_proccessed_response)\n",
    "        key = output_parser.response_schemas[0].name\n",
    "        if output_dict.get(key) is None:\n",
    "            output_dict[key] = \"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        output_dict = {}\n",
    "    return output_dict\n",
    "\n",
    "output_parser\n",
    "\n",
    "patient_name = parse_response_dict(parser_response, output_parser).get(\"patient_name\", \"\").strip() \n",
    "if DEBUG:\n",
    "    print(f\"str response: {parser_response}\")\n",
    "    print(f\"patient_name is: {patient_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286bd31-8bb2-4e65-8a2b-23be2ac5f3a8",
   "metadata": {},
   "source": [
    "### Age question\n",
    "\n",
    "PromptTemplate\n",
    "* https://www.comet.com/site/blog/introduction-to-prompt-templates-in-langchain/\n",
    "* https://stackoverflow.com/questions/77316112/langchain-how-do-input-variables-work-in-particular-how-is-context-replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dbc5db03-faaa-4870-9a66-6e617a629902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_name=f\"{patient_name}\" if patient_name is not None else \"\"\n",
    "# query = f\"What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer.)\"\n",
    "query_template = \"\"\"\n",
    "What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer)\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(query_template)\n",
    "query = prompt_template.format(patient_name=patient_name) \n",
    "\n",
    "if DEBUG:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "69a41314-cb3f-45ff-9eac-7a8f2d6de58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import logging\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "def get_ml_logging_info(exp_name: str = \"scivias-med-reports\") -> Tuple[str, str]:\n",
    "    # Set the run name to time string\n",
    "    run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    search_pattern = f\"name = '{exp_name}'\"\n",
    "    experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "    \n",
    "    if len(experiments) < 1:\n",
    "        exp_id = mlflow.create_experiment(name=exp_name)\n",
    "        print(f\"experiment with string id {exp_id} is created.\")\n",
    "    else:\n",
    "        exp_id = experiments[0].experiment_id\n",
    "        # experiment_id = experiments.experiment_id[0]\n",
    "        print(f\"experiment with string id {exp_id} is reused.\")\n",
    "    return exp_id, run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bcd9a56f-1c2e-4fb9-ac58-8db56a7a0861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment with string id 2 is reused.\n"
     ]
    }
   ],
   "source": [
    "'''mlflow log run start'''\n",
    "exp_id, run_name = get_ml_logging_info()\n",
    "mlflow.end_run()\n",
    "mlflow.set_experiment(experiment_id=exp_id)\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "chain_type = \"map_reduce\"\n",
    "# chain_type = \"stuff\"\n",
    "# chain_type = \"refine\" \n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "    # chain_type_kwargs={\"map_prompt\": map_prompt_template},\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3bd801cb-4e39-424e-8310-9b013f740a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if chain_type == \"map_reduce\":\n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "    # set the token max from 3000 to 4000\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.token_max = MAX_POSITION_EMBEDDINGS\n",
    "    \n",
    "    \n",
    "if chain_type == \"refine\":\n",
    "    # pass\n",
    "    qa_chain.combine_documents_chain.initial_llm_chain.prompt = init_prompt_template\n",
    "    # qa_chain.combine_documents_chain.refine_llm_chain.token_max = MAX_POSITION_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b52cd2df-08f5-45bc-9f5c-9f94926fe4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fabb0cefa90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fabb0cefa90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7fa9a1e56940>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9713d2c7-fcfe-4934-b0c1-b1ae9e7df4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qa_chain.combine_documents_chain.initial_llm_chain.prompt\n",
    "# qa_chain.combine_documents_chain.refine_llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a3bd4924-a9b3-4099-9b13-d3555faa8f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fabb0cefa90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fabb0cefa90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7fa9a1e56940>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3bbe2371-ed99-4cc0-9875-29c2d3bd5c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mlflow.log_param(\"map_prompt\", map_prompt_template.template)\n",
    "mlflow.log_param(\"user_query\", query_template)\n",
    "mlflow.log_param(\"doc_name\", CUR_DOC_INFO.name)\n",
    "mlflow.log_param(\"doc_source\", CUR_DOC_INFO.source)\n",
    "mlflow.log_param(\"llm_model\", model_name)\n",
    "\n",
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "response = qa_chain({\"query\": query})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fe0b73af-3d35-4426-a272-1d63a8afdb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"Response: {response['result']}\")\n",
    "    print('-'*20)\n",
    "    print(data[file_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7ef2daa-235d-4fd8-afee-180e3446812a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the string type\n",
    "# age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\", type=\"int\")\n",
    "age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\")\n",
    "# age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\", type=\"int\")\n",
    "\n",
    "# response_schema = [age_schema]\n",
    "age_output_parser = StructuredOutputParser.from_response_schemas([age_schema])\n",
    "# age_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6584cd99-239b-476f-8932-05235dfeab48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"patient_age\": string  // patient age\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "age_question=\"retrieve one: patient age as number\"\n",
    "format_instructions = age_output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aea79c9e-923e-43b7-bdaa-abc17e2aadcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a075f43a-0429-4362-9495-50c76441d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chains patient name, log json parser chain instead of retrieval qa chain to anonymous patient name in experiment tracking logs\n",
    "# mlflow.log_param(\"response_mapreduce\", input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39761501-39b9-4cd2-81df-27be8c9bfd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True \n",
    "parser_response = chain.run(text=input_text, format_instructions=format_instructions, question=age_question, temperature=0.01)\n",
    "if DEBUG:\n",
    "    langchain.debug = False\n",
    "\n",
    "mlflow.log_param(\"response_jsonchain\", parser_response)\n",
    "# end mlflow run logging\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3566e42f-6296-448b-b55e-70ea0bf13bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['format_instructions', 'question', 'text'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {text}/n/n/n\\n\\nQuestion: {question}/n\\n{format_instructions}\\n[/INST]\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fabb0cefa90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15}))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a39e5a67-203f-47b1-93e7-d0e00d3ceac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(parser_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c11d626-8be5-405f-8421-9b8ec30b3ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_age_obj = parse_response_dict(parser_response, age_output_parser, DEBUG).get(\"patient_age\", \"\")\n",
    "# print(patient_age_str)\n",
    "try:\n",
    "    if isinstance(patient_age_obj, str):\n",
    "        patient_age_obj = patient_age_obj.strip()\n",
    "        patient_age = int(patient_age_obj)\n",
    "    if isinstance(patient_age_obj, int):\n",
    "        patient_age = patient_age_obj\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    patient_age = -1\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"str response: {parser_response}\")\n",
    "    print(f\"patient_age is: {patient_age}\")\n",
    "    print(f\"pateint_age has type: {type(patient_age)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "438568b1-f46f-4bb4-8ff2-1aa89b8015c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(input_text)\n",
    "    print(parser_response)\n",
    "    print(f\"{patient_name} is {patient_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6e860b06-fa4c-48d6-a31d-9241b31bc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{patient_name} is {patient_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56d01aff-1853-4e8c-a7a8-2edbb2e379cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map_prompt_template = PromptTemplate(template=\"\"\"\n",
    "# Use the following portion of a long document to see if any of the text is relevant to answer the question. \\n\n",
    "# Return any relevant text verbatim.\\n\n",
    "# {context}\\n\n",
    "# Question: {question}\\n\n",
    "# Relevant text, if any:\"\"\",\n",
    "# input_variables=['context', 'question'])\n",
    "# qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cc180169-0db6-44e5-9d32-2d4ab4c1a3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stuff_llm_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n\n",
    "# {context}\\n\\n\n",
    "# Question: {question}\\nHelpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "557a0fff-560f-497e-ab1c-2e6b01945acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "30dc0290-7669-4132-b40f-806176b7f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the weight of the patient in kg? (Remember to include 'The weight of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c55acbd2-237a-40be-92fb-de2dedfdd9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     langchain.debug = True\n",
    "# response = qa_chain({\"query\": query})\n",
    "# #response = qa_chain({\"query\": query})\n",
    "# if DEBUG:\n",
    "#     langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2113a349-9774-4ba7-86fe-9cd9b66f5b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     print(f\"Response: {response['result']}\")\n",
    "#     print('-'*20)\n",
    "#     print(data[file_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a2436c90-e04b-4a2f-b1aa-894d52a12c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# response = index.query_with_sources(query, llm=llm, retriever_kwargs={\"chain_type\":\"map_reduce\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74bf5f-fdad-4843-96c3-95844d7d91fa",
   "metadata": {},
   "source": [
    "### (optional) Additional Read\n",
    "\n",
    "GPT4All\n",
    "* https://python.langchain.com/docs/integrations/llms/gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c03817-da5b-445d-b3cc-ec7cd8a29cf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9fd27939-7b6e-4a5c-884f-c4813825c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861587f9-f41e-4ce3-903b-ec50b17f4cce",
   "metadata": {},
   "source": [
    "#### zero shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4b5316fa-82d8-4f47-98fa-9506916a1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name\n",
    "input=f\"Can you tell me the name of the patient from the folowing doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3c299015-c5b7-425b-8a79-ba155491c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age\n",
    "input=f\"Can you tell me the age of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8d4d472f-80db-4d73-ac73-f673bc5fe279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnosis\n",
    "input=f\"Can you tell me the diagnosis of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa11f-0eb2-4dac-aa70-194cfe8e577d",
   "metadata": {},
   "source": [
    "#### Chain-of-thoughts prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dc858b23-c973-4dbc-9b4c-24131f90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name prompt\n",
    "input = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {context}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "#print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6e61eea4-3731-4ba5-8c1b-e392cc6fe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "input = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{context}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "# print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a401151-af8b-46f4-8ef2-0177ae075f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose prompt\n",
    "input=f\"Context:\\nPatient: Fried is a 34-year-old patient, Diagnoses: Influenza (J09.X2) \\nQuestion:\\nWhat diagnoses has the patient? \\nAnswer:\\nFried is a patient, 34 year-old, has diagnoses Influenza (J09.X2). The answers is Influenza (J09.X2)\\nContext:\\n{context}\\nQuestion:\\nWhat diagnoses has the patient?\\nAnswer: \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
