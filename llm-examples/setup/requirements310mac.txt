huggingface_hub==0.23.1

transformers==4.41.0 #support gemma model


#xformers==0.0.20 # do not install xformers, which makes accelerate not loading the llm model into gpus
# huggingface_hub use_auth_token need this.

# urllib3==2.0.4
urllib3==1.26.16
jsonschema==4.19.0
# for showing download widget in jupyter notebook
ipywidgets==8.1.0
# for python script input arg generation
click==8.1.7
# argparse==1.4.0
#
# monitor nvidia gpu usage
# have no permission to access
# gpustat==1.1.1
# nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv
# from a kubeflow notebook
#
# kfp==1.8.22
# 
# python method overload
multipledispatch==1.0.0
# For T5 huggingface need sentencepiece
sentencepiece==0.1.99
#
# Moses Tokenizer for German custom translator
sacremoses==0.0.53
# pdf reader
# pypdf==3.15.5
pypdf==4.2.0

# Testing MPS memory features
torch==2.3.0
torchaudio==2.3.0
torchvision==0.18.0
# vision library
# fastai==2.7.14 # fastai depends on torch version

## langchain LLM
langchain==0.2.0

# need Pydantic 1.10.12 to better evaluate typehints
# https://github.com/langchain-ai/langchain/issues/8577#issuecomment-1663249273
pydantic==1.10.13
# pydantic==2.7.1 
# docarray doesn't work with pydantic 2.xx, pydantic_core is installed with 2.xx, must be removed. 
unstructured~=0.13.7 # for langchain S3DirectoryLoader to load txt file
sentence-transformers~=2.7.0 # for langchain vectorestore embedding model
docarray==0.40.0 # langchain DocArrayInMemorySearch nned docarray

# s3 client
boto3==1.34.107

## GPU/MPS training speed up for tranformers
# accelerate==0.27.2
accelerate==0.30.1
peft==0.11.1

# For tensorflow and macosx m1 gput
# tensorflow==2.14.1
# backend of pretrained google model from tensorflow hub are on Kaggle
tensorflow==2.16.1 
tensorflow-metal==1.1.0
tensorflow-hub==0.16.1

## install huggingface datasets for fine-tuning on MPS backend device using torch
datasets==2.19.1
# huggingface evaluate for evaluate the fine-tuned model
evaluate==0.4.2

# transformer model quantization
bitsandbytes==0.42.0
#bitsandbytes==0.43.1

# Nvidia GPU only (speed up transformers architecture), macosx arm chip has integrated gpu
# xformers==0.0.23.post1

# ARM64 runtimes
# https://github.com/huggingface/optimum
# https://discuss.huggingface.co/t/optimum-arm64-quantized-models-on-apple-silicon-m1/31867
# optimum[onnxruntime]==1.16.2

# mlflow LLM evaluation
# mlflow==2.11.1
mlflow==2.12.1
typing-extensions>=4.2.0

aim==3.19.3
aim-mlflow==0.2.1
textstat==0.7.3
tiktoken==0.7.0
# tiktoken==0.5.2

# build python packages
# build==1.0.3
build==1.2.1
# twine==5.0.0 # upload .whl package distribution file
twine==5.0.0 # upload .whl package distribution file

# install your own gitlab package 
# --index-url https://gitlab.lrz.de/api/v4/projects/150553/packages/pypi/simple
# --trusted-host https://gitlab.lrz.de
applyllm==0.0.6

# install kfp for local pipeline development
kfp==1.8.22
# kfp==2.7.0

# For Stable DiffusionPipeline
diffusers==0.27.2

# Cuda dependeny for phi-3
# flash-attention==1.0.0
# flash-attn==2.5.8