# huggingface_hub==0.20.2
huggingface_hub==0.20.3

transformers==4.37.2
# transformers==4.36.2
# transformers==4.35.2

#xformers==0.0.20 # do not install xformers, which makes accelerate not loading the llm model into gpus
# huggingface_hub use_auth_token need this.

# urllib3==2.0.4
urllib3==1.26.16
jsonschema==4.19.0
fastai==2.7.13 # 2.7.12
# for showing download widget in jupyter notebook
ipywidgets==8.1.0
# for python script input arg generation
click==8.1.7
# argparse==1.4.0
#
# monitor nvidia gpu usage
# have no permission to access
# gpustat==1.1.1
# nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv
# from a kubeflow notebook
#
# kfp==1.8.22
# 
# python method overload
multipledispatch==1.0.0
# For T5 huggingface need sentencepiece
sentencepiece==0.1.99
#
# Moses Tokenizer for German custom translator
sacremoses==0.0.53
# pdf reader
pypdf==3.15.5

#torch==2.0.1
#torchaudio==2.0.2
#torchvision==0.15.2

#torch-tensorrt==1.4.0

torch==2.1.2
torchaudio==2.1.2
torchvision==0.16.2

#torchdata==0.6.1
#torchtext==0.15.2


## langchain LLM
langchain==0.1.4 # 0.1.0 
# langchain==0.0.354
# need Pydantic 1.10.12 to better evaluate typehints
# https://github.com/langchain-ai/langchain/issues/8577#issuecomment-1663249273
# pydantic==2.5.2
pydantic==1.10.13 
# docarray doesn't work with pydantic 2.xx, pydantic_core is installed with 2.xx, must be removed. 
# pydantic>=1.10.13 # typehints
unstructured==0.11.0 # for langchain S3DirectoryLoader to load txt file
sentence-transformers==2.2.2 # for langchain vectorestore embedding model
docarray==0.39.1 # langchain DocArrayInMemorySearch nned docarray

# llama-index==0.9.8.post1
# langchain==0.0.312 # HuggingFacePipeline broken after 0.0.313 to 0.0.340

# s3 client
boto3==1.34.14 # previously 1.29.0

## GPU/MPS training speed up for tranformers
# accelerate==0.26.0
accelerate==0.26.1
peft==0.7.1

# For tensorflow and macosx m1 gput
tensorflow==2.14.1
tensorflow-metal==1.1.0

## install huggingface datasets for fine-tuning on MPS backend device using torch
datasets==2.16.1
# huggingface evaluate for evaluate the fine-tuned model
evaluate==0.4.1

# transformer model quantization
bitsandbytes==0.42.0

# Nvidia GPU only (speed up transformers architecture), macosx arm chip has integrated gpu
# xformers==0.0.23.post1

# ARM64 runtimes
# https://github.com/huggingface/optimum
# https://discuss.huggingface.co/t/optimum-arm64-quantized-models-on-apple-silicon-m1/31867
# optimum[onnxruntime]==1.16.2

# mlflow LLM evaluation
mlflow==2.10.0
typing-extensions>=4.2.0
aim==3.17.5
aim-mlflow==0.2.1
textstat==0.7.3
tiktoken==0.5.2

# build python packages
build==1.0.3
twine==4.0.2 # upload .whl package distribution file

# install your own gitlab package 
--index-url https://gitlab.lrz.de/api/v4/projects/150553/packages/pypi/simple
--trusted-host https://gitlab.lrz.de
applyllm==0.0.1