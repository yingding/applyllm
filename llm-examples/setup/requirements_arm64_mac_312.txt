# huggingface_hub==0.24.5
huggingface-hub==1.2.3
# huggingface-hub==0.36.0

# transformers==4.44.0 #support gemma model
# transformers==4.57.3 #support gemma model
transformers~=5.0.0rc1
# langchain-huggingface==1.2.0 # need huggingface-hub==0.36.0

#xformers==0.0.20 # do not install xformers, which makes accelerate not loading the llm model into gpus
# huggingface_hub use_auth_token need this.

# urllib3==1.26.16
urllib3==2.6.2
# jsonschema==4.19.0
jsonschema==4.25.1
# for showing download widget in jupyter notebook
ipywidgets==8.1.8
# for python script input arg generation
# click==8.1.7
click==8.3.1
# argparse==1.4.0
#
# monitor nvidia gpu usage
# have no permission to access
# gpustat==1.1.1
# nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv
# from a kubeflow notebook
#
# kfp==1.8.22
# 
# python method overload
multipledispatch==1.0.0
# For T5 huggingface need sentencepiece
# sentencepiece==0.1.99
sentencepiece==0.2.1
#
# Moses Tokenizer for German custom translator
# sacremoses==0.0.53
sacremoses==0.1.1
# pdf reader
# pypdf==4.2.0
pypdf==6.4.2

# Testing MPS memory features
# torch==2.4.0
# torchaudio==2.4.0
# torchvision==0.19.0
torch==2.9.1
torchaudio==2.9.1
torchvision==0.24.1
# vision library
# fastai==2.7.14 # fastai depends on torch version

## langchain LLM
# langchain==0.2.14
# langchain-community==0.2.12
langchain==1.2.0
langchain-community==0.4.1
# langchain-openai==1.1.3

# need Pydantic 1.10.12 to better evaluate typehints
# https://github.com/langchain-ai/langchain/issues/8577#issuecomment-1663249273
# pydantic==1.10.13
pydantic==2.12.5 

# docarray doesn't work with pydantic 2.xx, pydantic_core is installed with 2.xx, must be removed. 
# unstructured~=0.13.7 # for langchain S3DirectoryLoader to load txt file
# sentence-transformers~=2.7.0 # for langchain vectorestore embedding model
# docarray==0.40.0 # langchain DocArrayInMemorySearch nned docarray
unstructured~=0.18.21
sentence-transformers~=5.2.0
docarray==0.41.0

# s3 client
# boto3==1.34.107
boto3~=1.42.9

## GPU/MPS training speed up for tranformers
# accelerate==0.30.1
# peft==0.11.1
accelerate==1.12.0
peft==0.18.0

# For tensorflow and macosx m1 gput
# backend of pretrained google model from tensorflow hub are on Kaggle
# tensorflow==2.16.1 
# tensorflow-metal==1.1.0
# tensorflow-hub==0.16.1
# tensorflow==2.20.0 
# tensorflow-metal==1.2.0
# tensorflow-hub==0.16.1

## install huggingface datasets for fine-tuning on MPS backend device using torch
# datasets==2.19.1
# evaluate==0.4.2
datasets==4.4.1
# huggingface evaluate for evaluate the fine-tuned model
evaluate==0.4.6

# transformer model quantization
# bitsandbytes==0.42.0
bitsandbytes==0.49.0

# Nvidia GPU only (speed up transformers architecture), macosx arm chip has integrated gpu
# xformers==0.0.23.post1

# ARM64 runtimes
# https://github.com/huggingface/optimum
# https://discuss.huggingface.co/t/optimum-arm64-quantized-models-on-apple-silicon-m1/31867
# optimum[onnxruntime]==1.16.2

# mlflow LLM evaluation
# mlflow==2.12.1
# typing-extensions>=4.2.0
mlflow~=3.7.0
# typing-extensions>=4.15.0

aim==3.29.1
aim-mlflow==0.2.1
textstat==0.7.12
tiktoken==0.12.0

# build python packages
# build==1.2.1
build==1.3.0
# twine==5.0.0 # upload .whl package distribution file
twine==6.2.0 # upload .whl package distribution file

# install your own gitlab package 
# --index-url https://gitlab.lrz.de/api/v4/projects/150553/packages/pypi/simple
# --trusted-host https://gitlab.lrz.de
applyllm==0.0.9rc3

# install kfp for local pipeline development
# kfp==1.8.22
kfp==2.15.2

# For Stable DiffusionPipeline
# diffusers==0.27.2
diffusers==0.36.0

onnx==1.20.0

# Cuda dependeny for phi-3
# flash-attn==2.8.3