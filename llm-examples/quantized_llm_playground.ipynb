{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d49fc33-61c9-41aa-a446-c9cf79cafe2c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "@Author: Yingding Wang\\\n",
    "@Created: 10.Jan 2024\\\n",
    "@Updated: 22.Jan 2024\\\n",
    "@Version: 1\n",
    "\n",
    "this notebook book demonstrate Llama2 quantization with bitsandbytes(bnb), the CUDA toolkit is used by bnb. Thus a custom pytorch cu118 image is built and used for this example. This custom pytorch image extends the kubeflow pytorch cuda base image. \n",
    "\n",
    "Reference:\n",
    "* https://www.youtube.com/watch?v=ypzmPwLH_Q4\n",
    "* https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23095b04-43fb-43d5-9331-202bdd4fafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow build local packages\n",
    "# !{sys.executable} -m pip install --user --upgrade build==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0787cfd1-8c34-4255-9025-87bf64d59345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp                       1.8.22\n",
      "kfp-pipeline-spec         0.1.16\n",
      "kfp-server-api            1.8.5\n"
     ]
    }
   ],
   "source": [
    "# examing the kfp python sdk version inside a KubeFlow v1.5.1\n",
    "!{sys.executable} -m pip list | grep kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3b4487-c06c-47b2-9867-29990621e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bfc1f14-17f8-4307-aa90-aa5ed336c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes quantization does not work with macosx MPS GPU backend\n",
    "quantization_enabled = True\n",
    "# quantization_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfeb62e-237f-428d-a50d-543cf1f1b043",
   "metadata": {},
   "source": [
    "### LD_LIBRARY_PATH\n",
    "\n",
    "bitsandbytes shall find the `LD_LIBRARY_PATH` automatically\n",
    "```console\n",
    "/usr/local/cuda-11.8/lib64:/usr/local/cuda-11/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f4f9da-ab2e-4974-9e25-9904d81b992b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda-11.8/lib64:/usr/local/cuda-11/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "applyllm.__version__: 0.0.2\n"
     ]
    }
   ],
   "source": [
    "import os, applyllm\n",
    "\n",
    "lb_library_path = os.environ['LD_LIBRARY_PATH']\n",
    "\n",
    "print(lb_library_path)\n",
    "print(f\"applyllm.__version__: {applyllm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c70a197-b4a8-41af-9030-706699ef557e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --trusted-host https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f5748b3-454f-4a70-8f1f-bef58eb85506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/llm-models/core-kind/yinwang/models'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from applyllm.accelerators import (\n",
    "    AcceleratorHelper,\n",
    "    AcceleratorStatus,\n",
    "    DIR_MODE_MAP, \n",
    "    DirectorySetting,\n",
    "    TokenHelper as th\n",
    ")\n",
    "import os\n",
    "\n",
    "dir_setting = dir_setting=DIR_MODE_MAP.get(\"kf_notebook\")\n",
    "gpu_helper = AcceleratorHelper()\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)\n",
    "gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "\n",
    "os.environ['XDG_CACHE_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-9579f618-ddae-5958-9285-3207382f0b36\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from platform import python_version\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\" # for debugging\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "The following python command shall show the available MIG memory\n",
    "```shell\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)\n",
    "```\n",
    "The first tuple shows the availabe MIG cuda memory, if it goes to zero, and no process is attached,\n",
    "this means a cuda process is hang.\n",
    "```console\n",
    "(20748107776, 20937965568)\n",
    "19.32318115234375\n",
    "19.5\n",
    "```\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "```shell\n",
    "nvidia-smi # find out the PID something like 830333\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e320b2a-cac5-421a-abd5-036c6212b612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f4ad7f1-d083-48a3-9415-1cb6535af553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n",
      "2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76144154-fb66-4792-9da5-edd5a1993c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"mixtral8x7B-01\"\n",
    "# model_type = \"mixtral8x7B-inst01\"\n",
    "model_type = \"llama13B-chat\"\n",
    "\n",
    "# model_type = \"llama70B-chat\" # 37GB GPU MEM is too small for 4bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "def gen_token_kwargs():\n",
    "    # call method from token helper class\n",
    "    if th.need_token(model_type):\n",
    "        # kwargs = {\"use_auth_token\": get_token(dir_setting)}\n",
    "        token_kwargs = {\n",
    "            \"token\": th.get_token(dir_setting),\n",
    "            # \"truncation_side\": \"left\",\n",
    "            # \"return_tensors\": \"pt\",            \n",
    "                        }\n",
    "        print(\"huggingface token loaded\")\n",
    "    else:\n",
    "        token_kwargs = {}\n",
    "        print(\"huggingface token is NOT needed\")\n",
    "    return token_kwargs\n",
    "\n",
    "token_kwargs = gen_token_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-13b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77ceca5c-2926-45cc-8949-8d3515bae37a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9f3db6a-64d4-4122-8418-44ee95681ada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'37GB'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49cbb3-10e7-4c90-9e94-28e0ac3925f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Following this approach to load llama2 model with bitsandbytes\n",
    "* https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85613cf3-925a-47f6-9fe7-002a2732f28b",
   "metadata": {},
   "source": [
    "## 4bit quantization\n",
    "\n",
    "<table>\n",
    "    <!-- row 1-->\n",
    "<tr>\n",
    "<th>\n",
    "Llama-2-13b-chat-hf\n",
    "</th>\n",
    "<th>\n",
    "Mixtral-8x7B-v0.1\n",
    "</th>\n",
    "</tr>\n",
    "    <!-- row 2 -->\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 7.085938 GB\n",
    "Allocated memory : 6.809501 GB\n",
    "Free      memory : 0.276437 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 23.496094 GB\n",
    "Allocated memory : 23.303491 GB\n",
    "Free      memory : 0.192603 GB\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "855f6076-3849-465a-9c28-4ecd96e8bc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29406956-7d27-497f-b23b-00ee5c5f1e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fb022a3a5048ceae95b0f5255eb204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import bfloat16\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name, #'decapoda-research/llama-7b-hf',\n",
    "  device_map='auto',\n",
    "  quantization_config=bnb_config,\n",
    "  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "  **token_kwargs,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e6909-3578-4700-b39e-a650e17a43ce",
   "metadata": {},
   "source": [
    "## 8bit quantization\n",
    "\n",
    "<table>\n",
    "    <!-- row 1-->\n",
    "<tr>\n",
    "<th>\n",
    "Llama-2-13b-chat-hf\n",
    "</th>\n",
    "<th>\n",
    "Mixtral-8x7B-v0.1\n",
    "</th>\n",
    "</tr>\n",
    "    <!-- row 2 -->\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 13.185547 GB\n",
    "Allocated memory : 12.572203 GB\n",
    "Free      memory : 0.613344 GB\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "\n",
    "n.a\n",
    "\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0f61c44-f2b1-4dfc-bf5c-43d2834b4797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#  model_name, #'decapoda-research/llama-7b-hf',\n",
    "#  device_map='auto',\n",
    "#  load_in_8bit=True,\n",
    "#  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "#  **token_kwargs,  \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31ff607c-54a5-43da-ad33-d13bc458f5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#  model_name, #'decapoda-research/llama-7b-hf',\n",
    "#  device_map='auto',\n",
    "#  load_in_8bit=True,\n",
    "#  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "#  **token_kwargs,  \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a56ce2c-92a4-4199-8da9-a0c4fcc911bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 7.085938 GB\n",
      "Allocated memory : 6.809501 GB\n",
      "Free      memory : 0.276437 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "050e9165-fb53-4595-80ca-a6255d098a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a63aa23c-4383-4bd4-92ad-fdf33cfce29d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      ")\n",
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.eval())\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8ea5b-54b5-421e-95d0-58d88ec77a0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Issue with \n",
    "probability tensor contains either inf, nan or element < 0\n",
    "* https://github.com/facebookresearch/llama/issues/380#issuecomment-1716832417\n",
    "* https://github.com/facebookresearch/llama/issues/380\n",
    "\n",
    "The bitsandbytes quantized model might be corrupt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4d44570-fde0-4ca2-91a8-597206c7a0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    device='cpu',\n",
    "    # device=device,\n",
    "    **token_kwargs,\n",
    ")\n",
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1ef10d2-a1c8-4513-9010-292504a159f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=80,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68b515e6-3377-4bca-bc36-b087db5aa85a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain to me the difference between nuclear fission and fusion.\n",
      "\n",
      "Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing a large amount of energy in the process. This process typically occurs when an atom is bombarded with a high-energy particle, such as a neutron. When the nucleus splits, it releases a large amount of energy in the form of kin\n"
     ]
    }
   ],
   "source": [
    "res = generator(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5da7da7-5c3e-4d52-8713-353416007e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 7.509766 GB\n",
      "Allocated memory : 6.817435 GB\n",
      "Free      memory : 0.692330 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90052e36-c96f-4e2f-b03c-61f98f6a8e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_cuda_memory(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        # tokenizer is load in cpu\n",
    "        # tokenizer.model.cpu()\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n",
    "    \n",
    "# clear_cuda_memory(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac18541-1aef-4885-84b1-216c15cdbe1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HuggingFace Pipeline doesn't seem to work with Bits and Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6315ce5c-b9d8-4661-8a87-6c15df9913cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a18b436d-e07e-4c3b-a1f7-e73c602b7638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-13b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04e5a080-7da4-40a3-bd63-ca14f39df606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quantization_enabled = True\n",
    "# bitsandbytes quantization does not work with MPS \n",
    "# quantization_enabled = False\n",
    "\n",
    "#if quantization_enabled:\n",
    "#    compression_kwargs = {\n",
    "#        \"load_in_8bit\": True,\n",
    "#        # \"load_in_4bit\": True,\n",
    "#    }\n",
    "#else:\n",
    "#    compression_kwargs = {\n",
    "#        \"torch_dtype\": torch.float16\n",
    "#    }\n",
    "\n",
    "#generator = transformers.pipeline(\n",
    "#    \"text-generation\",\n",
    "#    model=model_name,\n",
    "#    tokenizer=tokenizer, # optional\n",
    "#    # torch_dtype=torch.float16, #bfloat16 is not supported on MPS backend\n",
    "#    # torch_dtype=torch.float32,\n",
    "#    device_map=\"auto\",\n",
    "#    # max_length=MAX_LENGTH,\n",
    "#    max_length=None, # remove the total length of the generated response\n",
    "#    max_new_tokens=100, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "#    use_fast = True,\n",
    "#    **token_kwargs,\n",
    "#    **compression_kwargs,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc0308f6-5431-4ed9-af26-b9d0be6c3d03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9fbc6de-0c5e-4db7-a74a-6ccf0f8646de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.1, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.1  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09491b84-0de7-45f7-b903-144c473e59d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a helpful, respectful and honest assistant.\n",
      "Always answer as helpfully as possible using the context text provided.\n",
      "Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
      "If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message=\"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "# testing prompt\n",
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']\n",
    "\n",
    "def get_inputs(idx):   \n",
    "    return f\"{system_message}{inputs[idx]}\"\n",
    "\n",
    "print(get_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d860ac56-4ce7-4763-ad72-bfc4e86c5aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: They started with 23 apples. Used 20 for lunch, so they have 23 - 20 = 3 apples left. Then they bought 6 more, so they have 3 + 6 = 9 apples.\n",
      "--------------------\n",
      "walltime: 4.117828130722046 in secs.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 7.509766 GB\n",
      "Allocated memory : 6.817435 GB\n",
      "Free      memory : 0.692330 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.001, max_new_tokens = 80, verbose=verbose)\n",
    "# batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "897b8269-5855-4d49-8b2a-f028dcffb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def print_answer(answer: list)-> None:\n",
    "#    if DEBUG:\n",
    "#        print(\"-\"*10)\n",
    "#        print(answer[0])\n",
    "#        print(\"-\"*10)\n",
    "#        print(answer[0].split(\"\\n\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d49de0f-2ff0-49ee-a4f0-456ae41f035d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 5.746094 GB\n",
      "Allocated memory : 0.212502 GB\n",
      "Free      memory : 5.533591 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "clear_cuda_memory(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef52954-7b28-45f6-9789-ca39b80ce7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
