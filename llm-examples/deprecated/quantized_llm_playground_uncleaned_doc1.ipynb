{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d49fc33-61c9-41aa-a446-c9cf79cafe2c",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "* https://www.youtube.com/watch?v=ypzmPwLH_Q4\n",
    "* https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7ed45-fe39-42b0-ae08-80b6e7be97d8",
   "metadata": {},
   "source": [
    "# Install cuda toolkit\n",
    "* https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#conda-installation\n",
    "\n",
    "1. Run the following cmd in terminal\n",
    "```shell\n",
    "conda install -y cuda -c nvidia/label/cuda-11.8.0\n",
    "# conda install -y cuda-toolkit-11.8.0 -c nvidia/label/cuda-11.8.0\n",
    "conda install -y anaconda::make\n",
    "# g++\n",
    "conda install -y gxx -c conda-forge\n",
    "```\n",
    "\n",
    "2. compile you bitsandbytes:\n",
    "```shell\n",
    "git clone https://github.com/timdettmers/bitsandbytes.git\n",
    "cd bitsandbytes\n",
    "CUDA_VERSION=118 make cuda11x\n",
    "# CUDA_VERSION=118 make cuda118\n",
    "python setup.py install\n",
    "# python setup.py install --user # doesn't work with --user flag\n",
    "```\n",
    "\n",
    "Debug test bitsandbytes:\n",
    "```shell\n",
    "python -m bitsandbytes\n",
    "```\n",
    "\n",
    "Reference:\n",
    "* https://stackoverflow.com/questions/4495120/combine-user-with-prefix-error-with-setup-py-install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e47ee59-c1c0-4119-bbdc-4c7fb6a5bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y cuda -c nvidia/label/cuda-11.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a185bc88-6df2-489d-8cf0-207563d140d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0787cfd1-8c34-4255-9025-87bf64d59345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp                       1.8.22\n",
      "kfp-pipeline-spec         0.1.16\n",
      "kfp-server-api            1.8.5\n"
     ]
    }
   ],
   "source": [
    "# examing the kfp python sdk version inside a KubeFlow v1.5.1\n",
    "!{sys.executable} -m pip list | grep kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3b4487-c06c-47b2-9867-29990621e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46791c4f-e00e-4793-bfab-abe7b9344576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bfc1f14-17f8-4307-aa90-aa5ed336c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_enabled = True\n",
    "# bitsandbytes quantization does not work with MPS \n",
    "# quantization_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "314f8910-4cb1-4e17-97eb-c34e731b2039",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# find / -name libcuda.so 2>/dev/null\n",
    "# os.environ['LD_LIBRARY_PATH']='/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib/x86_64-linux-gnu'\n",
    "# os.environ['LD_LIBRARY_PATH']='/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib/:/opt/conda/pkgs/cuda-cudart-dev-11.8.89-0/lib/'\n",
    "\n",
    "# use `find / -name libcudart.so 2>/dev/null`\n",
    "# /usr/local/cuda-11.8/lib64:/usr/local/cuda-11/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "lb_library_path = os.environ['LD_LIBRARY_PATH']\n",
    "# os.environ['LD_LIBRARY_PATH'] = \"/usr/local/nvidia/lib:/usr/local/nvidia/lib64\"\n",
    "# lb_library_path = \"/usr/local/cuda-11/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\"\n",
    "# lb_library_path = f\"/usr/local/cuda-11.8/targets/x86_64-linux/lib/:{lb_library_path}\"\n",
    "print(lb_library_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536793f8-5bb4-4dac-8ca6-ec8e0872f24a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bitsandbytes.cextension import CUDASetup\n",
    "# import torch\n",
    "# lib = CUDASetup.get_instance().lib\n",
    "# lib.cadam32bit_g32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c70a197-b4a8-41af-9030-706699ef557e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608244d5-b5e5-4740-b44e-81ea0d67ca08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --trusted-host https://download.pytorch.org/whl/cu118 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48662e9c-04da-455f-a80d-bd8136ee5f2e",
   "metadata": {},
   "source": [
    "#### Useful installation for KF notebook 1.7.0 cu111 drivers\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade transformers==4.31.0\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==1.10.2+cu111 fastai==2.7.12 fastcore==1.5.29 fastdownload==0.0.7 torchvision==0.11.3+cu111 --extra-index-url https://download.pytorch.org/whl/cu111\n",
    "#!{sys.executable} -m pip install --user --upgrade accelerate==0.20.3\n",
    "```\n",
    "\n",
    "We shall use the cuda 11.8 version (Cuda118)\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==2.0.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "`xformers==0.0.21` need `torch==2.0.1``\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade xformers==0.0.21 torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "```\n",
    "\n",
    "show js loading with ipywidgets\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0 comm==0.1.4 jupyterlab-widgets==3.0.8 widgetsnbextension==4.0.8\n",
    "```\n",
    "\n",
    "uninstall\n",
    "```shell\n",
    "#!{sys.executable} -m pip uninstall accelerator transformers xformers torch -y \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f5748b3-454f-4a70-8f1f-bef58eb85506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/llm-models/core-kind/yinwang/models'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.accelerator_utils import (\n",
    "    AcceleratorHelper, \n",
    "    DIR_MODE_MAP, \n",
    "    DirectorySetting,\n",
    "    TokenHelper as th\n",
    ")\n",
    "import os\n",
    "\n",
    "dir_setting = dir_setting=DIR_MODE_MAP.get(\"kf_notebook\")\n",
    "gpu_helper = AcceleratorHelper()\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)\n",
    "gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "\n",
    "os.environ['XDG_CACHE_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-d071ca60-69fc-5176-b46f-7acb36ea3e31\n",
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "from platform import python_version\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\" # for debugging\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "The following python command shall show the available MIG memory\n",
    "```shell\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)\n",
    "```\n",
    "The first tuple shows the availabe MIG cuda memory, if it goes to zero, and no process is attached,\n",
    "this means a cuda process is hang.\n",
    "```console\n",
    "(20748107776, 20937965568)\n",
    "19.32318115234375\n",
    "19.5\n",
    "```\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "```shell\n",
    "nvidia-smi # find out the PID something like 830333\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e320b2a-cac5-421a-abd5-036c6212b612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f4ad7f1-d083-48a3-9415-1cb6535af553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.2\n",
      "2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76144154-fb66-4792-9da5-edd5a1993c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "model_type = \"mixtral8x7B-01\"\n",
    "# model_type = \"mixtral8x7B-inst01\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "# model_type = \"llama70B-chat\" # 37GB GPU MEM is too small for 4bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "def gen_token_kwargs():\n",
    "    # call method from token helper class\n",
    "    if th.need_token(model_type):\n",
    "        # kwargs = {\"use_auth_token\": get_token(dir_setting)}\n",
    "        token_kwargs = {\n",
    "            \"token\": th.get_token(dir_setting),\n",
    "            # \"truncation_side\": \"left\",\n",
    "            # \"return_tensors\": \"pt\",            \n",
    "                        }\n",
    "        print(\"huggingface token loaded\")\n",
    "    else:\n",
    "        token_kwargs = {}\n",
    "        print(\"huggingface token is NOT needed\")\n",
    "    return token_kwargs\n",
    "\n",
    "token_kwargs = gen_token_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-v0.1\n"
     ]
    }
   ],
   "source": [
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77ceca5c-2926-45cc-8949-8d3515bae37a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 1g.10gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 14\n",
      "Physical  memory : 9.500000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from util.accelerator_utils import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9f3db6a-64d4-4122-8418-44ee95681ada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7GB'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49cbb3-10e7-4c90-9e94-28e0ac3925f6",
   "metadata": {},
   "source": [
    "## Following this approach to load llama2 model with bitsandbytes\n",
    "* https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85613cf3-925a-47f6-9fe7-002a2732f28b",
   "metadata": {},
   "source": [
    "## 4bit quantization\n",
    "\n",
    "<table>\n",
    "    <!-- row 1-->\n",
    "<tr>\n",
    "<th>\n",
    "Llama-2-13b-chat-hf\n",
    "</th>\n",
    "<th>\n",
    "Mixtral-8x7B-v0.1\n",
    "</th>\n",
    "</tr>\n",
    "    <!-- row 2 -->\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 7.085938 GB\n",
    "Allocated memory : 6.809501 GB\n",
    "Free      memory : 0.276437 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 23.496094 GB\n",
    "Allocated memory : 23.303491 GB\n",
    "Free      memory : 0.192603 GB\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "855f6076-3849-465a-9c28-4ecd96e8bc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29406956-7d27-497f-b23b-00ee5c5f1e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n\u001b[1;32m      2\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mBitsAndBytesConfig(\n\u001b[1;32m      3\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mbfloat16\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#'decapoda-research/llama-7b-hf',\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtoken_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:3646\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3642\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3643\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[1;32m   3644\u001b[0m         }\n\u001b[1;32m   3645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 3646\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3647\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3648\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[1;32m   3649\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[1;32m   3650\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m             )\n\u001b[1;32m   3656\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[1;32m   3658\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "from torch import bfloat16\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name, #'decapoda-research/llama-7b-hf',\n",
    "  device_map='auto',\n",
    "  quantization_config=bnb_config,\n",
    "  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "  **token_kwargs,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e6909-3578-4700-b39e-a650e17a43ce",
   "metadata": {},
   "source": [
    "## 8bit quantization\n",
    "\n",
    "<table>\n",
    "    <!-- row 1-->\n",
    "<tr>\n",
    "<th>\n",
    "Llama-2-13b-chat-hf\n",
    "</th>\n",
    "<th>\n",
    "Mixtral-8x7B-v0.1\n",
    "</th>\n",
    "</tr>\n",
    "    <!-- row 2 -->\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 13.185547 GB\n",
    "Allocated memory : 12.572203 GB\n",
    "Free      memory : 0.613344 GB\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "\n",
    "n.a\n",
    "\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f61c44-f2b1-4dfc-bf5c-43d2834b4797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#  model_name, #'decapoda-research/llama-7b-hf',\n",
    "#  device_map='auto',\n",
    "#  load_in_8bit=True,\n",
    "#  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "#  **token_kwargs,  \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff607c-54a5-43da-ad33-d13bc458f5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#  model_name, #'decapoda-research/llama-7b-hf',\n",
    "#  device_map='auto',\n",
    "#  load_in_8bit=True,\n",
    "#  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "#  **token_kwargs,  \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56ce2c-92a4-4199-8da9-a0c4fcc911bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e9165-fb53-4595-80ca-a6255d098a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63aa23c-4383-4bd4-92ad-fdf33cfce29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.eval())\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8ea5b-54b5-421e-95d0-58d88ec77a0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Issue with \n",
    "probability tensor contains either inf, nan or element < 0\n",
    "* https://github.com/facebookresearch/llama/issues/380#issuecomment-1716832417\n",
    "* https://github.com/facebookresearch/llama/issues/380\n",
    "\n",
    "The bitsandbytes quantized model might be corrupt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d44570-fde0-4ca2-91a8-597206c7a0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    **token_kwargs,\n",
    ")\n",
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef10d2-a1c8-4513-9010-292504a159f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=80,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b515e6-3377-4bca-bc36-b087db5aa85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = generator(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da7da7-5c3e-4d52-8713-353416007e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90052e36-c96f-4e2f-b03c-61f98f6a8e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_cuda_memory(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        # tokenizer is load in cpu\n",
    "        # tokenizer.model.cpu()\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n",
    "    \n",
    "# clear_cuda_memory(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac18541-1aef-4885-84b1-216c15cdbe1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HuggingFace Pipeline doesn't seem to work with Bits and Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315ce5c-b9d8-4661-8a87-6c15df9913cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f4d72-9725-4daa-86f7-602b534d9b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "#bnb_config = transformers.BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_quant_type='nf4',\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_compute_dtype=bfloat16\n",
    "#)\n",
    "\n",
    "#bnb_config = transformers.BitsAndBytesConfig(\n",
    "#    load_in_8bit=True,\n",
    "#    bnb_4bit_quant_type='nf4',\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_compute_dtype=bfloat16\n",
    "#)\n",
    "\n",
    "#model_config = transformers.AutoConfig.from_pretrained(\n",
    "#    model_name,\n",
    "#    **token_kwargs,\n",
    "#)\n",
    "\n",
    "#print(bnb_config)\n",
    "#print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd2513-5d73-446a-9ccb-471963c668c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#    model_name,\n",
    "#    trust_remote_code=True,\n",
    "#    config=model_config,\n",
    "#    quantization_config=bnb_config,\n",
    "#    device_map='auto',\n",
    "#    **token_kwargs,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4aac8-fa61-4026-a821-b546f45656e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bitsandbytes.cextension import CUDASetup\n",
    "# import torch\n",
    "# lib = CUDASetup.get_instance().lib\n",
    "# lib.cadam32bit_g32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\n",
    "#    model_name, \n",
    "#    device=\"cpu\",\n",
    "#    trust_remote_code=True,\n",
    "#    **token_kwargs,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b436d-e07e-4c3b-a1f7-e73c602b7638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5a080-7da4-40a3-bd63-ca14f39df606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# quantization_enabled = True\n",
    "# bitsandbytes quantization does not work with MPS \n",
    "# quantization_enabled = False\n",
    "\n",
    "#if quantization_enabled:\n",
    "#    compression_kwargs = {\n",
    "#        \"load_in_8bit\": True,\n",
    "#        # \"load_in_4bit\": True,\n",
    "#    }\n",
    "#else:\n",
    "#    compression_kwargs = {\n",
    "#        \"torch_dtype\": torch.float16\n",
    "#    }\n",
    "\n",
    "#generator = transformers.pipeline(\n",
    "#    \"text-generation\",\n",
    "#    model=model_name,\n",
    "#    tokenizer=tokenizer, # optional\n",
    "#    # torch_dtype=torch.float16, #bfloat16 is not supported on MPS backend\n",
    "#    # torch_dtype=torch.float32,\n",
    "#    device_map=\"auto\",\n",
    "#    # max_length=MAX_LENGTH,\n",
    "#    max_length=None, # remove the total length of the generated response\n",
    "#    max_new_tokens=100, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "#    use_fast = True,\n",
    "#    **token_kwargs,\n",
    "#    **compression_kwargs,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac8ee4-f7ac-440e-a081-136fc122fa88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from util.accelerator_utils import AcceleratorStatus\n",
    "\n",
    "#gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "#gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0308f6-5431-4ed9-af26-b9d0be6c3d03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fbc6de-0c5e-4db7-a74a-6ccf0f8646de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.1, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.1  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09491b84-0de7-45f7-b903-144c473e59d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message=\"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "# testing prompt\n",
    "inputs=['Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\\nA: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\n']\n",
    "\n",
    "def get_inputs(idx):   \n",
    "    return f\"{system_message}{inputs[idx]}\"\n",
    "\n",
    "print(get_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860ac56-4ce7-4763-ad72-bfc4e86c5aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.001, max_new_tokens = 80, verbose=verbose)\n",
    "# batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b8269-5855-4d49-8b2a-f028dcffb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_answer(answer: list)-> None:\n",
    "    if DEBUG:\n",
    "        print(\"-\"*10)\n",
    "        print(answer[0])\n",
    "        print(\"-\"*10)\n",
    "        print(answer[0].split(\"\\n\")[-1])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49de0f-2ff0-49ee-a4f0-456ae41f035d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_cuda_memory(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        # tokenizer is load in cpu\n",
    "        # tokenizer.model.cpu()\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n",
    "    \n",
    "clear_cuda_memory(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64be04-c528-426b-91f1-47b71996d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gc\n",
    "#def free_memory_gen(\n",
    "#    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "#    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):\n",
    "#    \"\"\"\n",
    "#    \"\"\"\n",
    "#    def local():\n",
    "#        l_generator = generator\n",
    "#        l_tokenizer = tokenizer\n",
    "#        #l_generator.cpu()\n",
    "#        #l_tokenizer.cpu()\n",
    "#        # model.cpu()\n",
    "#        \n",
    "#        del l_tokenizer, l_generator\n",
    "#        gc.collect()\n",
    "#        torch.cuda.empty_cache()\n",
    "#        #for device_idx in range(torch.cuda.device_count()):\n",
    "#        #    print(device_idx)\n",
    "#        #    device = torch.device(f\"cuda:{device_idx}\")\n",
    "#        #    device.reset()\n",
    "#    return local\n",
    "#\n",
    "# free_memory = free_memory_gen(generator, tokenizer)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
