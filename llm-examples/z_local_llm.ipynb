{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "this notebook demos example of using llm in a MPS backend (apple silicon GPU) using torch 2.x\n",
    "\n",
    "Referece:\n",
    "* torch 2.x MPS Backend: https://pytorch.org/docs/stable/notes/mps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import applyllm as apl\n",
    "\n",
    "print(apl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# check that MPS is availabe (Metal Performance Shaders)\n",
    "if not torch.backends.mps.is_available():\n",
    "    print(\"MPS is not available\")\n",
    "else:\n",
    "    print(\"MPS is available\")\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(mps_device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yingding/MODELS\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import (\n",
    "    DirectorySetting,\n",
    "    TokenHelper as th,\n",
    ")\n",
    "    \n",
    "dir_mode_map = {\n",
    "    \"kf_notebook\": DirectorySetting(),\n",
    "    \"mac_local\": DirectorySetting(home_dir=\"/Users/yingding\", transformers_cache_home=\"MODELS\", huggingface_token_file=\"MODELS/.huggingface_token\"),\n",
    "}\n",
    "\n",
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"gemma7b-it\": \"google/gemma-7b-it\",\n",
    "    \"gemma7b\" : \"google/gemma-7b\",\n",
    "    \"gemma2b-it\": \"google/gemma-2b-it\",\n",
    "    \"gemma2b\" : \"google/gemma-2b\",\n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\"\n",
    "\n",
    "dir_setting = dir_mode_map[default_dir_mode]\n",
    "\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ['XDG_CACHE_HOME'] = dir_setting.get_cache_home()\n",
    "\n",
    "print(os.environ['XDG_CACHE_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.38.0\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemma-7b-it\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "model_type = \"gemma7b-it\"\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"llama7B-chat\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, default_model_type)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast tokenizer\n",
    "\n",
    "* https://github.com/huggingface/transformers/issues/23889#issuecomment-1584090357"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Model and then Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n",
      "token_kwargs: {}\n",
      "model_kwargs: {'torch_dtype': torch.float16, 'max_length': None}\n",
      "pipeline_kwargs: {'torch_dtype': torch.float16, 'max_length': None, 'task': 'text-generation', 'max_new_tokens': 200, 'do_sample': True, 'temperature': 0.01, 'device_map': 'auto', 'framework': 'pt', 'add_special_tokens': True}\n"
     ]
    }
   ],
   "source": [
    "from torch import bfloat16\n",
    "from applyllm.pipelines import (\n",
    "    ModelCatalog,\n",
    "    KwargsBuilder\n",
    ")\n",
    "token_kwargs = th.gen_token_kwargs(model_type=model_type, dir_setting=dir_setting)\n",
    "print(f\"token_kwargs: {token_kwargs}\")\n",
    "\n",
    "# mixtral model has no max_new_tokens limit, so it is not set here.\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.float16, #bfloat16 is not supported on MPS backend, float16 only on GPU accelerator\n",
    "    # torch_dtype=torch.float32,\n",
    "    # max_length=MAX_LENGTH,\n",
    "    \"max_length\" : None, # remove the total length of the generated response\n",
    "}\n",
    "print(f\"model_kwargs: {model_kwargs}\")\n",
    "\n",
    "# set the transformers.pipeline kwargs\n",
    "# the torch_dtype shall be set both for the model and the pipeline, due to a transformer issue.\n",
    "# otherwise it will cause unnecessary more memory usage in the pipeline of transformers\n",
    "# https://github.com/huggingface/transformers/issues/28817\n",
    "# https://github.com/mlflow/mlflow/pull/10979\n",
    "pipeline_kwargs = {\n",
    "    \"task\": \"text-generation\",\n",
    "    \"max_new_tokens\" : 200,\n",
    "    \"do_sample\" : True, # do_sample True is required for temperature\n",
    "    \"temperature\" : 0.01, # \n",
    "    \"device_map\" : \"auto\", # use the MPS device if available\n",
    "    \"framework\": \"pt\", # use pytorch as framework \n",
    "}\n",
    "\n",
    "gemma_pipeline_kwargs = {\n",
    "    \"add_special_tokens\": True,\n",
    "    # \"torch_dtype\": torch.bfloat16,\n",
    "    \"torch_dtype\": torch.float16,\n",
    "}\n",
    "\n",
    "# pipeline_kwargs override the model_kwargs during the merge\n",
    "pipeline_kwargs = KwargsBuilder([model_kwargs]).override(pipeline_kwargs).build()\n",
    "\n",
    "if model_name.startswith(ModelCatalog.GOOGLE_FAMILY):\n",
    "    pipeline_kwargs = KwargsBuilder([pipeline_kwargs]).override(gemma_pipeline_kwargs).build()\n",
    "\n",
    "print(f\"pipeline_kwargs: {pipeline_kwargs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mps.driver_allocated_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0GB\n",
      "0GB\n"
     ]
    }
   ],
   "source": [
    "driver_allocated_mem = f\"{int(torch.mps.driver_allocated_memory()/1024**3)}GB\"\n",
    "lm_allocated_mem = f\"{int(torch.mps.current_allocated_memory()/1024**3)}GB\"\n",
    "print(driver_allocated_mem)\n",
    "print(lm_allocated_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max memory to offload parts of LLM model to the CPU memory\n",
    "* https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#designing-a-device-map\n",
    "\n",
    "Note:\n",
    "* Max Memory offload to CPU is CUDA implementation only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d07ca050a94430ab3b4175d106419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from applyllm.utils import time_func\n",
    "from applyllm.pipelines import ModelConfig, LocalCausalLMConfig\n",
    "\n",
    "# For a M1 max with 64GB memory, set the limit to 48GB\n",
    "# cuda_max_memory = {\n",
    "#   0: \"48GB\", # GPU device 0\n",
    "#   \"cpu\": \"1GB\", # CPU device with no memory, since M1 max has unified memory\n",
    "# } \n",
    "\n",
    "base_lm_config = ModelConfig(\n",
    "  model_config = {\n",
    "    \"pretrained_model_name_or_path\": model_name,\n",
    "    \"device_map\": \"auto\",\n",
    "    # \"max_memory\": cuda_max_memory,\n",
    "  }\n",
    ")\n",
    "\n",
    "# No bitsandbytes qunatization support for MPS backend yet, set quantized to False\n",
    "kwargs = {\n",
    "  \"quantized\": False,\n",
    "  \"model_config\": base_lm_config.get_config(),\n",
    "  \"quantization_config\": {\n",
    "    \"quantization_config\": transformers.BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type='nf4',\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=bfloat16\n",
    "      )\n",
    "    }\n",
    "}\n",
    "\n",
    "lm_config = LocalCausalLMConfig(**kwargs)\n",
    "\n",
    "@time_func\n",
    "def load_model():\n",
    "  return AutoModelForCausalLM.from_pretrained(    \n",
    "    **lm_config.get_config(),\n",
    "    **token_kwargs,  \n",
    "  )\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"model_config\": {\n",
    "        \"pretrained_model_name_or_path\": model_name,\n",
    "        \"device\": \"cpu\",\n",
    "        # \"device_map\": \"auto\", # put to GPU if GPU is available\n",
    "        # \"max_position_embeddings\": MAX_LENGTH,\n",
    "        # \"max_length\": MAX_LENGTH,\n",
    "    },\n",
    "}\n",
    "tokenizer_config = ModelConfig(**tokenizer_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    **tokenizer_config.get_config(),\n",
    "    **token_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing token\n",
    "* https://huggingface.co/docs/tokenizers/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from applyllm.pipelines import (\n",
    "    ModelCatalog\n",
    ")\n",
    "\n",
    "if model_name.startswith(ModelCatalog.GOOGLE_FAMILY):\n",
    "    inputs=[\"\"\"           \n",
    "BEGIN EXAMPLE\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "END EXAMPLE\n",
    "\n",
    "Your turn:            \n",
    "Q1: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\n",
    "Q2: What color is the sound of music?\n",
    "     \n",
    "\"\"\"]\n",
    "\n",
    "else: \n",
    "    inputs=[\"\"\"\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(f\"{len(input_test_encoded)}\")\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes quantization does not work with MPS backend\n",
    "print(pipeline_kwargs)\n",
    "\n",
    "# transformer pipeline kwargs\n",
    "tp_kwargs = {\n",
    "    \"model\": model,\n",
    "    \"tokenizer\": tokenizer,\n",
    "}\n",
    "\n",
    "tp_config = ModelConfig(model_config = tp_kwargs)\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    **tp_config.get_config(),\n",
    "    **pipeline_kwargs,\n",
    "    **token_kwargs,\n",
    "    # **compression_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install autopep8 or black extension in VSCode\n",
    "`shift + opt + F` to auto format python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from applyllm.accelerators import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic, time\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.001, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=3,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            # eos_token_id=tokenizer.eos_token_id, # for llama\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.15,  # without this output begins repeating\n",
    "            add_special_tokens=True,  # for gemma \n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    # pprint(result)\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from applyllm.pipelines import ModelCatalog, ModelInfo, PromptHelper\n",
    "# print(model_name)\n",
    "# print(ModelCatalog.MISTRAL_FAMILY)\n",
    "\n",
    "model_info = ModelCatalog.get_model_info(model_name=model_name)\n",
    "prompt_helper = PromptHelper(model_info=model_info)\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_inputs_by_model(idx, inputs, prompt_helper):\n",
    "    print(prompt_helper.model_info.model_family)\n",
    "    # generate a model dependent prompt with appropriate sys instruction message\n",
    "    return prompt_helper.gen_prompt(inputs[idx])\n",
    "\n",
    "get_inputs = partial(get_inputs_by_model, inputs=inputs, prompt_helper=prompt_helper)\n",
    "print(get_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.1, max_new_tokens = 200, verbose=verbose)\n",
    "\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlflow autologging langchain\n",
    "* https://mlflow.org/docs/latest/llms/langchain/guide/index.html+\n",
    "* https://github.com/mlflow/mlflow/issues/9237#issuecomment-1667549626\n",
    "\n",
    "#### Issue\n",
    "* HuggingFacePipeline is not callable from mlflow run: https://github.com/langchain-ai/langchain/issues/8858\n",
    "\n",
    "#### LangChain Callback Handler\n",
    "* https://python.langchain.com/docs/integrations/providers/aim_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_ai_gateway\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow\n",
    "* https://api.python.langchain.com/en/latest/_modules/langchain_community/callbacks/mlflow_callback.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"./mlruns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "# from langchain.callbacks import MlflowCallbackHandler\n",
    "\n",
    "# Set the run name to time string\n",
    "run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_name = \"langchain\"\n",
    "search_pattern = f\"name = '{experiment_name}'\"\n",
    "experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "if len(experiments) < 1:\n",
    "    experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "    print(f\"experiment with string id {experiment_id} is created.\")\n",
    "else:\n",
    "    experiment_id = experiments[0].experiment_id\n",
    "    # experiment_id = experiments.experiment_id[0]\n",
    "    print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "# mlflow_callback = MlflowCallbackHandler(experiment=experiment_name, name=run_name)\n",
    "\n",
    "mlflow.end_run()\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "template = prompt_helper.gen_prompt(\"{input}\")\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "\n",
    "mlflow.log_param(\"system_prompt\", template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[mlflow_callback])\n",
    "\n",
    "# mlflow_callback.flush_tracker(llm_chain)\n",
    "\n",
    "# print(llm_chain.invoke({\"input\": inputs[0]}))\n",
    "# format the output of print with multiple lines of 60 max line length\n",
    "# response = llm_chain.run(inputs[0])\n",
    "dict_response = llm_chain.invoke(input={\"input\":inputs[0]})\n",
    "response = dict_response.get(\"text\", \"\")\n",
    "\n",
    "mlflow.log_param(\"response\", response)\n",
    "\n",
    "# Evaluate the model on some example questions\n",
    "import pandas as pd\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"input\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \" +\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \" +\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \" +\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \" +\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \" +\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \" +\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \" +\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \" +\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \" +\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(eval_data)\n",
    "\n",
    "class LocalHfpModel():\n",
    "    \"\"\"local huggingface pipeline model\"\"\"\n",
    "    def __init__(self, llm_chain):\n",
    "        self.llm_chain = llm_chain\n",
    "    \n",
    "\n",
    "    def __call__(self, data):\n",
    "        # single call returns string\n",
    "        # response = self.llm_chain.run(data[\"input\"].tolist())\n",
    "        # self.results.append(response)\n",
    "        # GPU batch\n",
    "        response = self.llm_chain.batch(data[\"input\"].tolist())\n",
    "        # print(type(response))\n",
    "        # print(response)\n",
    "        return [ _dict[\"text\"] for _dict in response]\n",
    "\n",
    "# load the LocalHfpModel() to mlflow.evaluate\n",
    "results = mlflow.evaluate(\n",
    "    model=LocalHfpModel(llm_chain),\n",
    "    model_type=\"question-answering\",\n",
    "    targets=\"ground_truth\",\n",
    "    data=eval_data,\n",
    ")\n",
    "print(f\"See aggregated evaluation results below: \\n{results.metrics}\")\n",
    "\n",
    "# Evaluation result for each data record is available in `results.tables`.\n",
    "eval_table = results.tables[\"eval_results_table\"]\n",
    "print(f\"See evaluation table below: \\n{eval_table}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "pprint(response, indent=0, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the run name to time string\n",
    "# run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# experiment_name = \"local_llm_test\"\n",
    "# search_pattern = f\"name = '{experiment_name}'\"\n",
    "# experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "# if len(experiments) < 1:\n",
    "#     experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "#     print(f\"experiment with string id {experiment_id} is created.\")\n",
    "# else:\n",
    "#     experiment_id = experiments[0].experiment_id\n",
    "#     # experiment_id = experiments.experiment_id[0]\n",
    "#     print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "    \n",
    "# try:\n",
    "#     with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:\n",
    "#         logged_model = mlflow.langchain.log_model(\n",
    "#             lc_model=llm_chain,\n",
    "#             artifact_path=\"models\")\n",
    "        \n",
    "#     # Load the logged model using MLflow's Python function flavor\n",
    "#     loaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\n",
    "\n",
    "#     # Predict using the loaded model, with defined input schema from prompt template\n",
    "#     print(loaded_model.predict([{\"input\": inputs[0]}]))\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We automatically log the model and trace related artifacts\n",
    "# A model with name `lc_model` is registered, we can load it back as a PyFunc model\n",
    "# model_name = \"lc_model\"\n",
    "# model_version = 1\n",
    "# loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "# print(loaded_model.predict(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_mps_memory(tokenizer, generator):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_MEMORY = False\n",
    "# CLEAR_MEMORY = True\n",
    "\n",
    "if CLEAR_MEMORY:\n",
    "    clear_mps_memory(tokenizer=tokenizer, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3.10",
   "language": "python",
   "name": "llm3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
