{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "this notebook demos example of using llm in a MPS backend (apple silicon GPU) using torch 2.x\n",
    "\n",
    "Referece:\n",
    "* torch 2.x MPS Backend: https://pytorch.org/docs/stable/notes/mps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import applyllm as apl\n",
    "\n",
    "print(apl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# check that MPS is availabe (Metal Performance Shaders)\n",
    "if not torch.backends.mps.is_available():\n",
    "    print(\"MPS is not available\")\n",
    "else:\n",
    "    print(\"MPS is available\")\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(mps_device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yingding/MODELS\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import (\n",
    "    DirectorySetting,\n",
    "    TokenHelper,\n",
    ")\n",
    "    \n",
    "dir_mode_map = {\n",
    "    \"kf_notebook\": DirectorySetting(),\n",
    "    \"mac_local\": DirectorySetting(home_dir=\"/Users/yingding\", transformers_cache_home=\"MODELS\", huggingface_token_file=\"MODELS/.huggingface_token\"),\n",
    "}\n",
    "\n",
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\":    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\":    \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    \"llama3-8B-inst\":   \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"llama3-70B-inst\":  \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    \"gemma7b-it\": \"google/gemma-7b-it\",\n",
    "    \"gemma7b\":    \"google/gemma-7b\",\n",
    "    \"gemma2b-it\": \"google/gemma-2b-it\",\n",
    "    \"gemma2b\":    \"google/gemma-2b\",\n",
    "    \"gemma7b-it-1.1\": \"google/gemma-1.1-7b-it\",\n",
    "    \"gemma2b-it-1.1\": \"google/gemma-1.1-2b-it\",\n",
    "    \"phi3-medium-128k-inst\": \"microsoft/Phi-3-medium-128k-instruct\",\n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\"\n",
    "\n",
    "dir_setting = dir_mode_map[default_dir_mode]\n",
    "\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ['XDG_CACHE_HOME'] = dir_setting.get_cache_home()\n",
    "\n",
    "print(os.environ['XDG_CACHE_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.0\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"gemma7b-it\"\n",
    "# model_type = \"gemma2b-it\"\n",
    "model_type = \"llama3-8B-inst\"\n",
    "# model_type = \"phi3-medium-128k-inst\"\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"llama7B-chat\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, default_model_type)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast tokenizer\n",
    "\n",
    "* https://github.com/huggingface/transformers/issues/23889#issuecomment-1584090357"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Model and then Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token loaded\n",
      "model_kwargs: {'torch_dtype': torch.float16, 'device_map': 'mps', 'max_length': None, 'trust_remote_code': True}\n",
      "pipeline_kwargs: {'torch_dtype': torch.float16, 'device_map': 'mps', 'max_length': None, 'trust_remote_code': True, 'task': 'text-generation', 'max_new_tokens': 200, 'do_sample': True, 'temperature': 0.01, 'top_k': 3, 'top_p': 0.95, 'framework': 'pt'}\n"
     ]
    }
   ],
   "source": [
    "from applyllm.pipelines import (\n",
    "    ModelCatalog,\n",
    "    KwargsBuilder\n",
    ")\n",
    "th = TokenHelper(dir_setting=dir_setting, prefix_list=[\"llama\"])\n",
    "token_kwargs = th.gen_token_kwargs(model_type=model_type)\n",
    "\n",
    "# data_type = torch.bfloat16\n",
    "if model_name.startswith(\"microsoft\"):\n",
    "    data_type = \"auto\"\n",
    "else:\n",
    "    data_type = torch.float16\n",
    "\n",
    "device_map = \"mps\" # \"auto\"\n",
    "# device_map = \"auto\"\n",
    "\n",
    "# auto caste not working for mps 4.38.2\n",
    "# https://github.com/huggingface/transformers/issues/29431 \n",
    "\n",
    "# mixtral model has no max_new_tokens limit, so it is not set here.\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": data_type, #bfloat16 is not supported on MPS backend, float16 only on GPU accelerator\n",
    "    # torch_dtype=torch.float32,\n",
    "    # max_length=MAX_LENGTH,\n",
    "    \"device_map\": device_map,\n",
    "    \"max_length\" : None, # remove the total length of the generated response\n",
    "    \"trust_remote_code\" : True\n",
    "}\n",
    "print(f\"model_kwargs: {model_kwargs}\")\n",
    "\n",
    "# set the transformers.pipeline kwargs\n",
    "# the torch_dtype shall be set both for the model and the pipeline, due to a transformer issue.\n",
    "# otherwise it will cause unnecessary more memory usage in the pipeline of transformers\n",
    "# https://github.com/huggingface/transformers/issues/28817\n",
    "# https://github.com/mlflow/mlflow/pull/10979\n",
    "\n",
    "# if model_name.startswith(\"microsoft\"):\n",
    "#     do_sample = False\n",
    "# else:\n",
    "#     do_sample = True\n",
    "\n",
    "do_sample = True\n",
    "\n",
    "pipeline_kwargs = {\n",
    "    \"task\": \"text-generation\",\n",
    "    \"max_new_tokens\" : 200,\n",
    "    \"do_sample\" : do_sample, # do_sample True is required for temperature\n",
    "    \"temperature\" : 0.01, \n",
    "    \"device_map\" : device_map, # use the MPS device if available\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"framework\": \"pt\", # use pytorch as framework \n",
    "}\n",
    "\n",
    "gemma_pipeline_kwargs = {\n",
    "    \"add_special_tokens\": True,\n",
    "    \"torch_dtype\": data_type,\n",
    "}\n",
    "\n",
    "# pipeline_kwargs override the model_kwargs during the merge\n",
    "pipeline_kwargs = KwargsBuilder([model_kwargs]).override(pipeline_kwargs).build()\n",
    "\n",
    "if model_name.startswith(ModelCatalog.GOOGLE_FAMILY):\n",
    "    pipeline_kwargs = KwargsBuilder([pipeline_kwargs]).override(gemma_pipeline_kwargs).build()\n",
    "\n",
    "print(f\"pipeline_kwargs: {pipeline_kwargs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mps.driver_allocated_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0GB\n",
      "0GB\n"
     ]
    }
   ],
   "source": [
    "driver_allocated_mem = f\"{int(torch.mps.driver_allocated_memory()/1024**3)}GB\"\n",
    "lm_allocated_mem = f\"{int(torch.mps.current_allocated_memory()/1024**3)}GB\"\n",
    "print(driver_allocated_mem)\n",
    "print(lm_allocated_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max memory to offload parts of LLM model to the CPU memory\n",
    "* https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#designing-a-device-map\n",
    "\n",
    "Note:\n",
    "* Max Memory offload to CPU is CUDA implementation only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32584cac86cd4c10b38ab0a7a843f38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: load_model() python function\n",
      "walltime: 10.165070056915283 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from applyllm.utils import time_func\n",
    "from applyllm.pipelines import ModelConfig, LocalCausalLMConfig\n",
    "\n",
    "# For a M1 max with 64GB memory, set the limit to 48GB\n",
    "# cuda_max_memory = {\n",
    "#   0: \"48GB\", # GPU device 0\n",
    "#   \"cpu\": \"1GB\", # CPU device with no memory, since M1 max has unified memory\n",
    "# } \n",
    "\n",
    "base_lm_config = ModelConfig(\n",
    "  model_config = {\n",
    "    \"pretrained_model_name_or_path\": model_name,\n",
    "    \"device_map\": device_map,\n",
    "    \"trust_remote_code\" : True\n",
    "    # \"max_memory\": cuda_max_memory,\n",
    "  }\n",
    ")\n",
    "\n",
    "# No bitsandbytes qunatization support for MPS backend yet, set quantized to False\n",
    "kwargs = {\n",
    "  \"quantized\": False,\n",
    "  \"model_config\": base_lm_config.get_config(),\n",
    "  \"quantization_config\": {\n",
    "    \"quantization_config\": transformers.BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type='nf4',\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16\n",
    "      )\n",
    "    }\n",
    "}\n",
    "\n",
    "lm_config = LocalCausalLMConfig(**kwargs)\n",
    "\n",
    "@time_func\n",
    "def load_model():\n",
    "  return AutoModelForCausalLM.from_pretrained(    \n",
    "    **lm_config.get_config(),\n",
    "    **token_kwargs,  \n",
    "  )\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"model_config\": {\n",
    "        \"pretrained_model_name_or_path\": model_name,\n",
    "        \"device\": \"cpu\",\n",
    "        # \"device_map\": \"auto\", # put to GPU if GPU is available\n",
    "        # \"max_position_embeddings\": MAX_LENGTH,\n",
    "        # \"max_length\": MAX_LENGTH,\n",
    "    },\n",
    "}\n",
    "tokenizer_config = ModelConfig(**tokenizer_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    **tokenizer_config.get_config(),\n",
    "    **token_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='meta-llama/Meta-Llama-3-8B-Instruct', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128005: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128010: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128011: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128012: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128013: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128014: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128015: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<|reserved_special_token_248|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<|reserved_special_token_249|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<|reserved_special_token_250|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing token\n",
    "* https://huggingface.co/docs/tokenizers/pipeline\n",
    "\n",
    "## Phi instruct template\n",
    "* https://huggingface.co/microsoft/Phi-3-small-128k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from applyllm.pipelines import (\n",
    "    ModelCatalog,\n",
    "    PromptHelper\n",
    ")\n",
    "\n",
    "model_info = ModelCatalog.get_model_info(model_name)\n",
    "prompt_helper = PromptHelper(model_info)\n",
    "\n",
    "\n",
    "\n",
    "if model_info.model_family == ModelCatalog.GOOGLE_FAMILY:\n",
    "    query = \"\"\"BEGIN EXAMPLE\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "END EXAMPLE\n",
    "\n",
    "Your turn:            \n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? \n",
    "\"\"\"\n",
    "    inputs=[prompt_helper.gen_prompt(query)]\n",
    "elif model_name.startswith(\"microsoft\"):\n",
    "    # msft_template = \"<|endoftext|><|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "    msft_template = \"<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "    user_query = \"\"\"\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "    template = PromptTemplate(template=msft_template, input_variables=[\"query\"])\n",
    "    inputs=[template.format(query=user_query)]\n",
    "else: \n",
    "    inputs=[\"\"\"\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "[128000, 198, 48, 25, 29607, 706, 220, 18, 32515, 20953, 13, 1283, 50631, 220, 17, 810, 43732, 315, 32515, 20953, 13, 9062, 649, 706, 220, 19, 32515, 20953, 13, 2650, 1690, 32515, 20953, 1587, 568, 617, 1457, 5380, 32, 25, 29607, 3940, 449, 220, 18, 20953, 13, 220, 17, 43732, 315, 220, 19, 32515, 20953, 1855, 374, 220, 23, 32515, 20953, 13, 220, 18, 489, 220, 23, 284, 220, 806, 13, 578, 4320, 374, 220, 806, 627, 48, 25, 578, 94948, 1047, 220, 1419, 41776, 13, 1442, 814, 1511, 220, 508, 311, 1304, 16163, 323, 11021, 220, 21, 810, 11, 1268, 1690, 41776, 656, 814, 617, 5380]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(f\"{len(input_test_encoded)}\")\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'torch_dtype': torch.float16, 'device_map': 'mps', 'max_length': None, 'trust_remote_code': True, 'task': 'text-generation', 'max_new_tokens': 200, 'do_sample': True, 'temperature': 0.01, 'top_k': 3, 'top_p': 0.95, 'framework': 'pt'}\n"
     ]
    }
   ],
   "source": [
    "# bitsandbytes quantization does not work with MPS backend\n",
    "print(pipeline_kwargs)\n",
    "\n",
    "# transformer pipeline kwargs\n",
    "tp_kwargs = {\n",
    "    \"model\": model,\n",
    "    \"tokenizer\": tokenizer,\n",
    "}\n",
    "\n",
    "tp_config = ModelConfig(model_config = tp_kwargs)\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    **tp_config.get_config(),\n",
    "    **pipeline_kwargs,\n",
    "    **token_kwargs,\n",
    "    # **compression_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install autopep8 or black extension in VSCode\n",
    "`shift + opt + F` to auto format python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Allocated memory : 29.922241 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.13'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic, time\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.01, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        model_dependeny_kwargs = {}\n",
    "        if model_name.startswith(ModelCatalog.GOOGLE_FAMILY):\n",
    "            # for gemma \n",
    "            model_dependeny_kwargs = {\n",
    "                \"add_special_tokens\": True,\n",
    "            }\n",
    "        if model_name.startswith(ModelCatalog.MISTRAL_FAMILY):\n",
    "            model_dependeny_kwargs = {\n",
    "                \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            }\n",
    "        if model_name.startswith(ModelCatalog.META_FAMILY):\n",
    "            model_dependeny_kwargs = {\n",
    "                \"eos_token_id\" : tokenizer.eos_token_id\n",
    "            }\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=do_sample,\n",
    "            top_k=3,\n",
    "            top_p=0.95,\n",
    "            # num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            # eos_token_id=tokenizer.eos_token_id, # for llama\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.15,  # without this output begins repeating\n",
    "            return_full_text=False,\n",
    "            **model_dependeny_kwargs,\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    # pprint(result)\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "A: They started with 23 apples. After using 20 for lunch, they had 23 - 20 = 3 left. Then they bought 6 more, so they have 3 + 6 = 9 apples. The answer is 9.\n",
      "Q: A bookshelf has 5 shelves. Each shelf holds 8 books. How many books are on the bookshelf in\n",
      "--------------------\n",
      "walltime: 20.78048300743103 in secs.\n",
      "--------------------\n",
      "Allocated memory : 31.304428 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.01, max_new_tokens = 80, verbose=verbose)\n",
    "\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlflow autologging langchain\n",
    "* https://mlflow.org/docs/latest/llms/langchain/guide/index.html+\n",
    "* https://github.com/mlflow/mlflow/issues/9237#issuecomment-1667549626\n",
    "\n",
    "#### Issue\n",
    "* HuggingFacePipeline is not callable from mlflow run: https://github.com/langchain-ai/langchain/issues/8858\n",
    "\n",
    "#### LangChain Callback Handler\n",
    "* https://python.langchain.com/docs/integrations/providers/aim_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_ai_gateway\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow\n",
    "* https://api.python.langchain.com/en/latest/_modules/langchain_community/callbacks/mlflow_callback.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"./mlruns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment with string id 318577640496678081 is reused.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingding/VENV/llm3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/Users/yingding/VENV/llm3.10/lib/python3.10/site-packages/mlflow/data/digest_utils.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  string_columns = trimmed_df.columns[(df.applymap(type) == str).all(0)]\n",
      "/Users/yingding/VENV/llm3.10/lib/python3.10/site-packages/mlflow/models/evaluation/base.py:521: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(_hash_array_like_element_as_bytes)\n",
      "2024/08/11 15:22:16 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2024/08/11 15:22:16 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMChain(prompt=PromptTemplate(input_variables=['input'], template=\"[INST]<<SYS>>You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information.<</SYS>>\\n\\n{input}\\n[/INST]\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x3a2ebffa0>))\n",
      "             input                                       ground_truth\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...\n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/11 15:23:06 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n",
      "/Users/yingding/VENV/llm3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See aggregated evaluation results below: \n",
      "{'toxicity/v1/mean': 0.016425330191850662, 'toxicity/v1/variance': 6.162309483820697e-05, 'toxicity/v1/p90': 0.022705360502004623, 'toxicity/v1/ratio': 0.0, 'flesch_kincaid_grade_level/v1/mean': 11.55, 'flesch_kincaid_grade_level/v1/variance': 0.20250000000000015, 'flesch_kincaid_grade_level/v1/p90': 11.91, 'ari_grade_level/v1/mean': 19.05, 'ari_grade_level/v1/variance': 18.0625, 'ari_grade_level/v1/p90': 22.45, 'exact_match/v1': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccfd07fff90455bb8845b90899c18f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See evaluation table below: \n",
      "             input                                       ground_truth  \\\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
      "\n",
      "                                             outputs  token_count  \\\n",
      "0  [INST]<<SYS>>You are a helpful, respectful and...          301   \n",
      "1  [INST]<<SYS>>You are a helpful, respectful and...          300   \n",
      "\n",
      "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
      "0           0.024275                                 12.0   \n",
      "1           0.008575                                 11.1   \n",
      "\n",
      "   ari_grade_level/v1/score  \n",
      "0                      14.8  \n",
      "1                      23.3  \n",
      "('[INST]<<SYS>>You are a helpful, respectful and honest assistant.\\n'\n",
      " 'Always answer as helpfully as possible using the context text provided.\\n'\n",
      " 'Your answers should only answer the question once and not have any text after the answer is '\n",
      " 'done.\\n'\n",
      " '\\n'\n",
      " 'If a question does not make any sense, or is not factually coherent, explain why instead of '\n",
      " 'answering something not correct.\\n'\n",
      " \"If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. '\n",
      " 'How many tennis balls does he have now?\\n'\n",
      " 'A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The '\n",
      " 'answer is 11.\\n'\n",
      " 'Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples '\n",
      " 'do they have?\\n'\n",
      " '\\n'\n",
      " '[/INST]>>You are a helpful, respectful and honest assistant.\\n'\n",
      " 'Always answer as helpfully as possible using the context text provided.\\n'\n",
      " 'Your answers should only answer the question once and not have any text after the answer is '\n",
      " 'done.\\n'\n",
      " '\\n'\n",
      " 'If a question does not make any sense, or is not factually coherent, explain why instead of '\n",
      " 'answering something not correct.\\n'\n",
      " \"If you don't know the answer to a question, please don't share false \"\n",
      " 'information.<</SYS>>[/INST]>>You are a helpful, respectful and honest assistant.\\n'\n",
      " 'Always answer as helpfully as possible using the context text provided.\\n'\n",
      " 'Your answers should only answer the question once and not have any text after the answer is '\n",
      " 'done.\\n'\n",
      " '\\n'\n",
      " 'If a question does not make any sense, or is not factually coherent, explain why instead of '\n",
      " 'answering something not correct.\\n'\n",
      " \"If you don't know the answer to a question, please don't share false \"\n",
      " 'information.<</SYS>>[/INST]>>You are a helpful, respectful and honest assistant.\\n'\n",
      " 'Always answer as helpfully')\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "# from langchain.callbacks import MlflowCallbackHandler\n",
    "\n",
    "# Set the run name to time string\n",
    "run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_name = \"langchain\"\n",
    "search_pattern = f\"name = '{experiment_name}'\"\n",
    "experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "if len(experiments) < 1:\n",
    "    experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "    print(f\"experiment with string id {experiment_id} is created.\")\n",
    "else:\n",
    "    experiment_id = experiments[0].experiment_id\n",
    "    # experiment_id = experiments.experiment_id[0]\n",
    "    print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "# mlflow_callback = MlflowCallbackHandler(experiment=experiment_name, name=run_name)\n",
    "\n",
    "mlflow.end_run()\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "template = prompt_helper.gen_prompt(\"{input}\")\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "\n",
    "mlflow.log_param(\"system_prompt\", template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[mlflow_callback])\n",
    "\n",
    "# mlflow_callback.flush_tracker(llm_chain)\n",
    "\n",
    "# print(llm_chain.invoke({\"input\": inputs[0]}))\n",
    "# format the output of print with multiple lines of 60 max line length\n",
    "# response = llm_chain.run(inputs[0])\n",
    "dict_response = llm_chain.invoke(input={\"input\":inputs[0]})\n",
    "response = dict_response.get(\"text\", \"\")\n",
    "\n",
    "print(repr(llm_chain))\n",
    "\n",
    "mlflow.log_param(\"response\", response)\n",
    "\n",
    "# Evaluate the model on some example questions\n",
    "import pandas as pd\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"input\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \" +\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \" +\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \" +\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \" +\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \" +\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \" +\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \" +\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \" +\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \" +\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(eval_data)\n",
    "\n",
    "class LocalHfpModel():\n",
    "    \"\"\"local huggingface pipeline model\"\"\"\n",
    "    def __init__(self, llm_chain):\n",
    "        self.llm_chain = llm_chain\n",
    "    \n",
    "\n",
    "    def __call__(self, data):\n",
    "        # single call returns string\n",
    "        # response = self.llm_chain.run(data[\"input\"].tolist())\n",
    "        # self.results.append(response)\n",
    "        # GPU batch\n",
    "        response = self.llm_chain.batch(data[\"input\"].tolist())\n",
    "        # print(type(response))\n",
    "        # print(response)\n",
    "        return [ _dict[\"text\"] for _dict in response]\n",
    "\n",
    "# load the LocalHfpModel() to mlflow.evaluate\n",
    "results = mlflow.evaluate(\n",
    "    model=LocalHfpModel(llm_chain),\n",
    "    model_type=\"question-answering\",\n",
    "    targets=\"ground_truth\",\n",
    "    data=eval_data,\n",
    ")\n",
    "print(f\"See aggregated evaluation results below: \\n{results.metrics}\")\n",
    "\n",
    "# Evaluation result for each data record is available in `results.tables`.\n",
    "eval_table = results.tables[\"eval_results_table\"]\n",
    "print(f\"See evaluation table below: \\n{eval_table}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "pprint(response, indent=0, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the run name to time string\n",
    "# run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# experiment_name = \"local_llm_test\"\n",
    "# search_pattern = f\"name = '{experiment_name}'\"\n",
    "# experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "# if len(experiments) < 1:\n",
    "#     experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "#     print(f\"experiment with string id {experiment_id} is created.\")\n",
    "# else:\n",
    "#     experiment_id = experiments[0].experiment_id\n",
    "#     # experiment_id = experiments.experiment_id[0]\n",
    "#     print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "    \n",
    "# try:\n",
    "#     with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:\n",
    "#         logged_model = mlflow.langchain.log_model(\n",
    "#             lc_model=llm_chain,\n",
    "#             artifact_path=\"models\")\n",
    "        \n",
    "#     # Load the logged model using MLflow's Python function flavor\n",
    "#     loaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\n",
    "\n",
    "#     # Predict using the loaded model, with defined input schema from prompt template\n",
    "#     print(loaded_model.predict([{\"input\": inputs[0]}]))\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We automatically log the model and trace related artifacts\n",
    "# A model with name `lc_model` is registered, we can load it back as a PyFunc model\n",
    "# model_name = \"lc_model\"\n",
    "# model_version = 1\n",
    "# loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "# print(loaded_model.predict(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_mps_memory(tokenizer, generator):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_MEMORY = False\n",
    "# CLEAR_MEMORY = True\n",
    "\n",
    "if CLEAR_MEMORY:\n",
    "    clear_mps_memory(tokenizer=tokenizer, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Allocated memory : 32.673752 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npromt-response\\nResult: \\nWhich snomed ct code has chron's disease?\\n\\nThe SNOMED CT Code for Chronic Disease is 4621830.\\n--------------------\\nwalltime: 4.950197219848633 in secs.\\n--------------------\\nAllocated memory : 54.158737 GB\\n--------------------\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs2 = [\"Which animal is the largest mammal?\"]\n",
    "inputs2 = [\"Can you tell me something about chron's disease?\"]\n",
    "\n",
    "# hallucination https://www.findacode.com/snomed/34000006--crohns-disease.html\n",
    "'''\n",
    "promt-response\n",
    "Result: \n",
    "Which snomed ct code has chron's disease?\n",
    "\n",
    "The SNOMED CT Code for Chronic Disease is 4621830.\n",
    "--------------------\n",
    "walltime: 4.950197219848633 in secs.\n",
    "--------------------\n",
    "Allocated memory : 54.158737 GB\n",
    "--------------------\n",
    "'''\n",
    "# real answer is 34000006, probably need a RAG \n",
    "# inputs2 = [\"Which snomed ct code has chron's disease?\"]\n",
    "\n",
    "# inputs2 = [\"Can you tell me more about the company nordcloud?\"]\n",
    "# inputs2 = [\"Can you tell me more about the company nordcloud in munich?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      " I've heard it's a type of inflammatory bowel disease, but what are the symptoms and how is it treated?\n",
      "Chronic inflammatory bowel disease (IBD) refers to a group of conditions that cause chronic inflammation in the digestive tract. Crohns disease is one such condition.\n",
      "Crohns disease is an autoimmune disorder characterized by chronic inflammation in the lining of the digestive tract, which can lead to\n",
      "--------------------\n",
      "walltime: 27.523249864578247 in secs.\n",
      "--------------------\n",
      "Allocated memory : 32.677658 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "responses = chat(inputs2, temperature=0.01, max_new_tokens = 80, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3.10",
   "language": "python",
   "name": "llm3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
