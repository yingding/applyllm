{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "this notebook demos example of using llm in a MPS backend (apple silicon GPU) using torch 2.x\n",
    "\n",
    "Referece:\n",
    "* torch 2.x MPS Backend: https://pytorch.org/docs/stable/notes/mps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import applyllm as apl\n",
    "\n",
    "print(apl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# check that MPS is availabe (Metal Performance Shaders)\n",
    "if not torch.backends.mps.is_available():\n",
    "    print(\"MPS is not available\")\n",
    "else:\n",
    "    print(\"MPS is available\")\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(mps_device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yingding/MODELS\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import DirectorySetting\n",
    "    \n",
    "dir_mode_map = {\n",
    "    \"kf_notebook\": DirectorySetting(),\n",
    "    \"mac_local\": DirectorySetting(home_dir=\"/Users/yingding\", transformers_cache_home=\"MODELS\", huggingface_token_file=\"MODELS/.huggingface_token\"),\n",
    "}\n",
    "\n",
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\"\n",
    "\n",
    "dir_setting = dir_mode_map[default_dir_mode]\n",
    "\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ['XDG_CACHE_HOME'] = dir_setting.get_cache_home()\n",
    "\n",
    "print(os.environ['XDG_CACHE_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.2\n",
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"llama7B-chat\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, default_model_type)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast tokenizer\n",
    "\n",
    "* https://github.com/huggingface/transformers/issues/23889#issuecomment-1584090357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n"
     ]
    }
   ],
   "source": [
    "# MAX_POSITION_EMBEDDINGS = 3072\n",
    "# MAX_LENGTH = 4096\n",
    "\n",
    "def need_token(model_type: str, model_name_prefix: str=\"llama\"):\n",
    "    \"\"\"check if the model needs token\"\"\"\n",
    "    return model_type.startswith(model_name_prefix)\n",
    "\n",
    "def get_token(dir_setting: DirectorySetting):\n",
    "    \"\"\"get the token from the token file\"\"\"\n",
    "    token_file_path = dir_setting.get_token_file()\n",
    "    with open(token_file_path, \"r\") as file:\n",
    "        # file read add a new line to the token, remove it.\n",
    "        token = file.read().replace('\\n', '')\n",
    "    return token\n",
    "\n",
    "if need_token(model_type):\n",
    "    # kwargs = {\"use_auth_token\": get_token(dir_setting)}\n",
    "    token_kwargs = {\n",
    "        \"token\": get_token(dir_setting),\n",
    "        # \"truncation_side\": \"left\",\n",
    "        # \"return_tensors\": \"pt\",            \n",
    "                    }\n",
    "    print(\"huggingface token loaded\")\n",
    "else:\n",
    "    token_kwargs = {}\n",
    "    print(\"huggingface token is NOT needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Model and then Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "from applyllm.pipelines import KwargsBuilder\n",
    "\n",
    "# set the transformers.pipeline kwargs\n",
    "pipeline_kwargs = {\n",
    "    \"max_new_tokens\" : 80,\n",
    "    \"temperature\" : 0.001,\n",
    "    \"device_map\" : \"auto\", # use the MPS device if available\n",
    "}\n",
    "\n",
    "# mixtral model has no max_new_tokens limit, so it is not set here.\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.float16, #bfloat16 is not supported on MPS backend, float16 only on GPU accelerator\n",
    "    # torch_dtype=torch.float32,\n",
    "    # max_length=MAX_LENGTH,\n",
    "    \"max_length\" : None, # remove the total length of the generated response\n",
    "}\n",
    "\n",
    "pipeline_kwargs = KwargsBuilder([model_kwargs]).append(pipeline_kwargs).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1ae4d49fd74561b1444a84dd165d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: load_model() python function\n",
      "walltime: 5.309041976928711 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from applyllm.utils import time_func  \n",
    "# from optimum.onnxruntime import ORTModelForCausalLM\n",
    "\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )\n",
    "\n",
    "# pretrained_model_name_or_path\n",
    "# TODO: use model config to set the max_length\n",
    "# model = ORTModelForCausalLM.from_pretrained(\n",
    "\n",
    "@time_func\n",
    "def load_model():\n",
    "  return AutoModelForCausalLM.from_pretrained(    \n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    # max_length= None, # remove the total length of the generated response\n",
    "    # max_new_tokens=80,\n",
    "    # quantization_config=bnb_config,\n",
    "    # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "    # torch_dtype=torch.float16\n",
    "    **model_kwargs,\n",
    "    **token_kwargs,  \n",
    "  )\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"device\": \"cpu\",\n",
    "    # \"device_map\" :\"auto\", # put to GPU\n",
    "    # \"device\": \"mps\",\n",
    "    # \"max_position_embeddings\" : MAX_LENGTH,\n",
    "    # \"max_length\" : MAX_LENGTH,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    **tokenizer_kwargs,\n",
    "    **token_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing token\n",
    "* https://huggingface.co/docs/tokenizers/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\"\"\"\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "[1, 28705, 13, 28824, 28747, 14115, 659, 28705, 28770, 19552, 16852, 28723, 650, 957, 846, 28705, 28750, 680, 277, 509, 302, 19552, 16852, 28723, 7066, 541, 659, 28705, 28781, 19552, 16852, 28723, 1602, 1287, 19552, 16852, 1235, 400, 506, 1055, 28804, 13, 28741, 28747, 14115, 2774, 395, 28705, 28770, 16852, 28723, 28705, 28750, 277, 509, 302, 28705, 28781, 19552, 16852, 1430, 349, 28705, 28783, 19552, 16852, 28723, 28705, 28770, 648, 28705, 28783, 327, 28705, 28740, 28740, 28723, 415, 4372, 349, 28705, 28740, 28740, 28723, 13, 28824, 28747, 415, 18302, 1623, 515, 553, 28705, 28750, 28770, 979, 2815, 28723, 1047, 590, 1307, 28705, 28750, 28734, 298, 1038, 9957, 304, 7620, 28705, 28784, 680, 28725, 910, 1287, 979, 2815, 511, 590, 506, 28804, 13]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(f\"{len(input_test_encoded)}\")\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'torch_dtype': torch.float16, 'max_length': None, 'max_new_tokens': 80, 'temperature': 0.001, 'device_map': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "# bitsandbytes quantization does not work with MPS backend\n",
    "print(pipeline_kwargs)\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, # optional\n",
    "    **pipeline_kwargs,\n",
    "    **token_kwargs,\n",
    "    # **compression_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install autopep8 or black extension in VSCode\n",
    "`shift + opt + F` to auto format python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Allocated memory : 28.008636 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.13'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic, time\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.1, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.1  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    pprint(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptHelper():\n",
    "    \"\"\"\n",
    "    mistral instruction example:\n",
    "    https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "    llama2 instruction examples:\n",
    "    https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "    \"\"\"\n",
    "    meta = \"meta-llama\"\n",
    "    mistral = \"mistralai\"\n",
    "    INST_MSG_MAP = {\n",
    "        mistral: \"\"\"<s>[INST] You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information. Just return \\\"</s>\\\"\n",
    "\"\"\",\n",
    "        meta: \"\"\"[INST]<<SYS>>You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.<</SYS>>\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    \n",
    "    def gen_prompt(self, query: str) -> str:      \n",
    "        if model_name.startswith(self.meta):\n",
    "            inst_msg = self.INST_MSG_MAP[self.meta]\n",
    "        elif model_name.startswith(self.mistral):\n",
    "            inst_msg = self.INST_MSG_MAP[self.mistral]\n",
    "        else:\n",
    "            inst_msg = \"\"\n",
    "\n",
    "        prompt = f\"\"\"{inst_msg}\\n{query}\\n[/INST]\"\"\" if query is not None or len(query) > 0 else f\"\"\"{inst_msg}\\n[/INST]\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def get_inst_msg(self) -> str:\n",
    "        return self.gen_prompt(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "<s>[INST] You are a helpful, respectful and honest assistant.\n",
      "Always answer as helpfully as possible using the context text provided.\n",
      "Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
      "If you don't know the answer to a question, please don't share false information. Just return \"</s>\"\n",
      "\n",
      "\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "prompt_helper = PromptHelper(model_name)\n",
    "\n",
    "def get_inputs_by_model(idx, inputs, prompt_helper):\n",
    "    print(prompt_helper.model_name)\n",
    "    # generate a model dependent prompt with appropriate sys instruction message\n",
    "    return prompt_helper.gen_prompt(inputs[idx])\n",
    "\n",
    "get_inputs = partial(get_inputs_by_model, inputs=inputs, prompt_helper=prompt_helper)\n",
    "print(get_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "('Result: \\n'\n",
      " '\\n'\n",
      " 'Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can '\n",
      " 'has 4 tennis balls. How many tennis balls does he have now?\\n'\n",
      " 'A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis '\n",
      " 'balls. 3 + 8 = 11. The answer is 11.\\n'\n",
      " 'Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 '\n",
      " 'more, how many apples do they have?\\n'\n",
      " 'A: The cafeteria originally had 23 apples. They used 20 for lunch, so that '\n",
      " 'leaves 3 apples remaining. Then they bought 6 more apples, making the total '\n",
      " 'number of apples 3 + 6 = 9. However, we made an error in our calculation. We '\n",
      " 'should have subtracted the 20 apples used from')\n",
      "--------------------\n",
      "walltime: 7.301661252975464 in secs.\n",
      "--------------------\n",
      "Allocated memory : 28.479446 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.001, max_new_tokens = 80, verbose=verbose)\n",
    "\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlflow autologging langchain\n",
    "* https://mlflow.org/docs/latest/llms/langchain/guide/index.html+\n",
    "* https://github.com/mlflow/mlflow/issues/9237#issuecomment-1667549626\n",
    "\n",
    "#### Issue\n",
    "* HuggingFacePipeline is not callable from mlflow run: https://github.com/langchain-ai/langchain/issues/8858\n",
    "\n",
    "#### LangChain Callback Handler\n",
    "* https://python.langchain.com/docs/integrations/providers/aim_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_ai_gateway\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow\n",
    "* https://api.python.langchain.com/en/latest/_modules/langchain_community/callbacks/mlflow_callback.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"./mlruns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment with string id 318577640496678081 is reused.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingding/VENV/llm3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/Users/yingding/VENV/llm3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/Users/yingding/VENV/llm3.10/lib/python3.10/site-packages/mlflow/data/digest_utils.py:29: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  string_columns = trimmed_df.columns[(df.applymap(type) == str).all(0)]\n",
      "/Users/yingding/VENV/llm3.10/lib/python3.10/site-packages/mlflow/models/evaluation/base.py:414: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(_hash_array_like_element_as_bytes)\n",
      "2024/02/05 21:22:56 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2024/02/05 21:22:56 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "/Users/yingding/VENV/llm3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             input                                       ground_truth\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...\n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/05 21:23:06 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n",
      "2024/02/05 21:23:09 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2024/02/05 21:23:09 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2024/02/05 21:23:10 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2024/02/05 21:23:10 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2024/02/05 21:23:10 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See aggregated evaluation results below: \n",
      "{'toxicity/v1/mean': 0.00014677690342068672, 'toxicity/v1/variance': 2.3893283430098113e-11, 'toxicity/v1/p90': 0.00015068736393004657, 'toxicity/v1/ratio': 0.0, 'flesch_kincaid_grade_level/v1/mean': 13.399999999999999, 'flesch_kincaid_grade_level/v1/variance': 7.839999999999999, 'flesch_kincaid_grade_level/v1/p90': 15.639999999999999, 'ari_grade_level/v1/mean': 17.2, 'ari_grade_level/v1/variance': 12.25, 'ari_grade_level/v1/p90': 20.0, 'exact_match/v1': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e0f6b5081f4b889d3fad602a28df75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See evaluation table below: \n",
      "             input                                       ground_truth  \\\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
      "\n",
      "                                             outputs  token_count  \\\n",
      "0   MLflow is an open-source platform for the com...           27   \n",
      "1   Apologies for any potential confusion, but yo...           73   \n",
      "\n",
      "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
      "0           0.000142                                 16.2   \n",
      "1           0.000152                                 10.6   \n",
      "\n",
      "   ari_grade_level/v1/score  \n",
      "0                      20.7  \n",
      "1                      13.7  \n",
      "(' A: The cafeteria started with 23 apples. They used 20, so they had 3 apples left. Then they '\n",
      " 'bought 6 more, so they have 9 apples in total. The answer is 9.')\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "# from langchain.callbacks import MlflowCallbackHandler\n",
    "\n",
    "# Set the run name to time string\n",
    "run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_name = \"langchain\"\n",
    "search_pattern = f\"name = '{experiment_name}'\"\n",
    "experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "if len(experiments) < 1:\n",
    "    experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "    print(f\"experiment with string id {experiment_id} is created.\")\n",
    "else:\n",
    "    experiment_id = experiments[0].experiment_id\n",
    "    # experiment_id = experiments.experiment_id[0]\n",
    "    print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "# mlflow_callback = MlflowCallbackHandler(experiment=experiment_name, name=run_name)\n",
    "\n",
    "mlflow.end_run()\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "template = prompt_helper.gen_prompt(\"{input}\")\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "\n",
    "mlflow.log_param(\"system_prompt\", template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[mlflow_callback])\n",
    "\n",
    "# mlflow_callback.flush_tracker(llm_chain)\n",
    "\n",
    "# print(llm_chain.invoke({\"input\": inputs[0]}))\n",
    "# format the output of print with multiple lines of 60 max line length\n",
    "response = llm_chain.run(inputs[0])\n",
    "mlflow.log_param(\"response\", response)\n",
    "\n",
    "# Evaluate the model on some example questions\n",
    "import pandas as pd\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"input\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \" +\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \" +\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \" +\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \" +\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \" +\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \" +\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \" +\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \" +\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \" +\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(eval_data)\n",
    "\n",
    "class LocalHfpModel():\n",
    "    \"\"\"local huggingface pipeline model\"\"\"\n",
    "    def __init__(self, llm_chain):\n",
    "        self.llm_chain = llm_chain\n",
    "    \n",
    "\n",
    "    def __call__(self, data):\n",
    "        # single call returns string\n",
    "        # response = self.llm_chain.run(data[\"input\"].tolist())\n",
    "        # self.results.append(response)\n",
    "        # GPU batch\n",
    "        response = self.llm_chain.batch(data[\"input\"].tolist())\n",
    "        # print(type(response))\n",
    "        # print(response)\n",
    "        return [ _dict[\"text\"] for _dict in response]\n",
    "\n",
    "# load the LocalHfpModel() to mlflow.evaluate\n",
    "results = mlflow.evaluate(\n",
    "    model=LocalHfpModel(llm_chain),\n",
    "    model_type=\"question-answering\",\n",
    "    targets=\"ground_truth\",\n",
    "    data=eval_data,\n",
    ")\n",
    "print(f\"See aggregated evaluation results below: \\n{results.metrics}\")\n",
    "\n",
    "# Evaluation result for each data record is available in `results.tables`.\n",
    "eval_table = results.tables[\"eval_results_table\"]\n",
    "print(f\"See evaluation table below: \\n{eval_table}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "pprint(response, indent=0, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the run name to time string\n",
    "# run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# experiment_name = \"local_llm_test\"\n",
    "# search_pattern = f\"name = '{experiment_name}'\"\n",
    "# experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "# if len(experiments) < 1:\n",
    "#     experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "#     print(f\"experiment with string id {experiment_id} is created.\")\n",
    "# else:\n",
    "#     experiment_id = experiments[0].experiment_id\n",
    "#     # experiment_id = experiments.experiment_id[0]\n",
    "#     print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "    \n",
    "# try:\n",
    "#     with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:\n",
    "#         logged_model = mlflow.langchain.log_model(\n",
    "#             lc_model=llm_chain,\n",
    "#             artifact_path=\"models\")\n",
    "        \n",
    "#     # Load the logged model using MLflow's Python function flavor\n",
    "#     loaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\n",
    "\n",
    "#     # Predict using the loaded model, with defined input schema from prompt template\n",
    "#     print(loaded_model.predict([{\"input\": inputs[0]}]))\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We automatically log the model and trace related artifacts\n",
    "# A model with name `lc_model` is registered, we can load it back as a PyFunc model\n",
    "# model_name = \"lc_model\"\n",
    "# model_version = 1\n",
    "# loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "# print(loaded_model.predict(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_mps_memory(tokenizer, generator):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_MEMORY = False\n",
    "# CLEAR_MEMORY = True\n",
    "\n",
    "if CLEAR_MEMORY:\n",
    "    clear_mps_memory(tokenizer=tokenizer, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3.10",
   "language": "python",
   "name": "llm3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
