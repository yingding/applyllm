{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "this notebook demos example of using llm in a MPS backend (apple silicon GPU) using torch 2.x\n",
    "\n",
    "Referece:\n",
    "* torch 2.x MPS Backend: https://pytorch.org/docs/stable/notes/mps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import applyllm as apl\n",
    "\n",
    "print(apl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# check that MPS is availabe (Metal Performance Shaders)\n",
    "if not torch.backends.mps.is_available():\n",
    "    print(\"MPS is not available\")\n",
    "else:\n",
    "    print(\"MPS is available\")\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(mps_device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yingding/MODELS\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import (\n",
    "    DirectorySetting,\n",
    "    TokenHelper as th,\n",
    ")\n",
    "    \n",
    "dir_mode_map = {\n",
    "    \"kf_notebook\": DirectorySetting(),\n",
    "    \"mac_local\": DirectorySetting(home_dir=\"/Users/yingding\", transformers_cache_home=\"MODELS\", huggingface_token_file=\"MODELS/.huggingface_token\"),\n",
    "}\n",
    "\n",
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\":    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\":    \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    \"llama3-8B-inst\":   \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"llama3-70B-inst\":  \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    \"gemma7b-it\": \"google/gemma-7b-it\",\n",
    "    \"gemma7b\":    \"google/gemma-7b\",\n",
    "    \"gemma2b-it\": \"google/gemma-2b-it\",\n",
    "    \"gemma2b\":    \"google/gemma-2b\",\n",
    "    \"gemma7b-it-1.1\": \"google/gemma-1.1-7b-it\",\n",
    "    \"gemma2b-it-1.1\": \"google/gemma-1.1-2b-it\",\n",
    "    \"phi3-medium-128k-inst\": \"microsoft/Phi-3-medium-128k-instruct\",\n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\"\n",
    "\n",
    "dir_setting = dir_mode_map[default_dir_mode]\n",
    "\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ['XDG_CACHE_HOME'] = dir_setting.get_cache_home()\n",
    "\n",
    "print(os.environ['XDG_CACHE_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.0\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/Phi-3-medium-128k-instruct\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"gemma7b-it\"\n",
    "# model_type = \"gemma2b-it\"\n",
    "# model_type = \"llama3-8B-inst\"\n",
    "model_type = \"phi3-medium-128k-inst\"\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"llama7B-chat\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, default_model_type)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast tokenizer\n",
    "\n",
    "* https://github.com/huggingface/transformers/issues/23889#issuecomment-1584090357"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Model and then Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n",
      "model_kwargs: {'torch_dtype': 'auto', 'device_map': 'auto', 'max_length': None, 'trust_remote_code': True}\n",
      "pipeline_kwargs: {'torch_dtype': 'auto', 'device_map': 'auto', 'max_length': None, 'trust_remote_code': True, 'task': 'text-generation', 'max_new_tokens': 200, 'do_sample': True, 'temperature': 0.01, 'top_k': 3, 'top_p': 0.95, 'framework': 'pt'}\n"
     ]
    }
   ],
   "source": [
    "from applyllm.pipelines import (\n",
    "    ModelCatalog,\n",
    "    KwargsBuilder\n",
    ")\n",
    "token_kwargs = th.gen_token_kwargs(model_type=model_type, dir_setting=dir_setting)\n",
    "\n",
    "# data_type = torch.bfloat16\n",
    "if model_name.startswith(\"microsoft\"):\n",
    "    data_type = \"auto\"\n",
    "else:\n",
    "    data_type = torch.float16\n",
    "# device_map = \"mps\" # \"auto\"\n",
    "\n",
    "device_map = \"auto\"  \n",
    "# auto caste not working for mps 4.38.2\n",
    "# https://github.com/huggingface/transformers/issues/29431 \n",
    "\n",
    "# mixtral model has no max_new_tokens limit, so it is not set here.\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": data_type, #bfloat16 is not supported on MPS backend, float16 only on GPU accelerator\n",
    "    # torch_dtype=torch.float32,\n",
    "    # max_length=MAX_LENGTH,\n",
    "    \"device_map\": device_map,\n",
    "    \"max_length\" : None, # remove the total length of the generated response\n",
    "    \"trust_remote_code\" : True\n",
    "}\n",
    "print(f\"model_kwargs: {model_kwargs}\")\n",
    "\n",
    "# set the transformers.pipeline kwargs\n",
    "# the torch_dtype shall be set both for the model and the pipeline, due to a transformer issue.\n",
    "# otherwise it will cause unnecessary more memory usage in the pipeline of transformers\n",
    "# https://github.com/huggingface/transformers/issues/28817\n",
    "# https://github.com/mlflow/mlflow/pull/10979\n",
    "\n",
    "# if model_name.startswith(\"microsoft\"):\n",
    "#     do_sample = False\n",
    "# else:\n",
    "#     do_sample = True\n",
    "\n",
    "do_sample = True\n",
    "\n",
    "pipeline_kwargs = {\n",
    "    \"task\": \"text-generation\",\n",
    "    \"max_new_tokens\" : 200,\n",
    "    \"do_sample\" : do_sample, # do_sample True is required for temperature\n",
    "    \"temperature\" : 0.01, \n",
    "    \"device_map\" : device_map, # use the MPS device if available\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"framework\": \"pt\", # use pytorch as framework \n",
    "}\n",
    "\n",
    "gemma_pipeline_kwargs = {\n",
    "    \"add_special_tokens\": True,\n",
    "    \"torch_dtype\": data_type,\n",
    "}\n",
    "\n",
    "# pipeline_kwargs override the model_kwargs during the merge\n",
    "pipeline_kwargs = KwargsBuilder([model_kwargs]).override(pipeline_kwargs).build()\n",
    "\n",
    "if model_name.startswith(ModelCatalog.GOOGLE_FAMILY):\n",
    "    pipeline_kwargs = KwargsBuilder([pipeline_kwargs]).override(gemma_pipeline_kwargs).build()\n",
    "\n",
    "print(f\"pipeline_kwargs: {pipeline_kwargs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mps.driver_allocated_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0GB\n",
      "0GB\n"
     ]
    }
   ],
   "source": [
    "driver_allocated_mem = f\"{int(torch.mps.driver_allocated_memory()/1024**3)}GB\"\n",
    "lm_allocated_mem = f\"{int(torch.mps.current_allocated_memory()/1024**3)}GB\"\n",
    "print(driver_allocated_mem)\n",
    "print(lm_allocated_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max memory to offload parts of LLM model to the CPU memory\n",
    "* https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#designing-a-device-map\n",
    "\n",
    "Note:\n",
    "* Max Memory offload to CPU is CUDA implementation only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8616313bdf45599f69f13e655cc727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: load_model() python function\n",
      "walltime: 29.37050700187683 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from applyllm.utils import time_func\n",
    "from applyllm.pipelines import ModelConfig, LocalCausalLMConfig\n",
    "\n",
    "# For a M1 max with 64GB memory, set the limit to 48GB\n",
    "# cuda_max_memory = {\n",
    "#   0: \"48GB\", # GPU device 0\n",
    "#   \"cpu\": \"1GB\", # CPU device with no memory, since M1 max has unified memory\n",
    "# } \n",
    "\n",
    "base_lm_config = ModelConfig(\n",
    "  model_config = {\n",
    "    \"pretrained_model_name_or_path\": model_name,\n",
    "    \"device_map\": device_map,\n",
    "    \"trust_remote_code\" : True\n",
    "    # \"max_memory\": cuda_max_memory,\n",
    "  }\n",
    ")\n",
    "\n",
    "# No bitsandbytes qunatization support for MPS backend yet, set quantized to False\n",
    "kwargs = {\n",
    "  \"quantized\": False,\n",
    "  \"model_config\": base_lm_config.get_config(),\n",
    "  \"quantization_config\": {\n",
    "    \"quantization_config\": transformers.BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type='nf4',\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16\n",
    "      )\n",
    "    }\n",
    "}\n",
    "\n",
    "lm_config = LocalCausalLMConfig(**kwargs)\n",
    "\n",
    "@time_func\n",
    "def load_model():\n",
    "  return AutoModelForCausalLM.from_pretrained(    \n",
    "    **lm_config.get_config(),\n",
    "    **token_kwargs,  \n",
    "  )\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"model_config\": {\n",
    "        \"pretrained_model_name_or_path\": model_name,\n",
    "        \"device\": \"cpu\",\n",
    "        # \"device_map\": \"auto\", # put to GPU if GPU is available\n",
    "        # \"max_position_embeddings\": MAX_LENGTH,\n",
    "        # \"max_length\": MAX_LENGTH,\n",
    "    },\n",
    "}\n",
    "tokenizer_config = ModelConfig(**tokenizer_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    **tokenizer_config.get_config(),\n",
    "    **token_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='microsoft/Phi-3-medium-128k-instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing token\n",
    "* https://huggingface.co/docs/tokenizers/pipeline\n",
    "\n",
    "## Phi instruct template\n",
    "* https://huggingface.co/microsoft/Phi-3-small-128k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/Phi-3-medium-128k-instruct\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from applyllm.pipelines import (\n",
    "    ModelCatalog,\n",
    "    PromptHelper\n",
    ")\n",
    "\n",
    "model_info = ModelCatalog.get_model_info(model_name)\n",
    "prompt_helper = PromptHelper(model_info)\n",
    "\n",
    "\n",
    "\n",
    "if model_info.model_family == ModelCatalog.GOOGLE_FAMILY:\n",
    "    query = \"\"\"BEGIN EXAMPLE\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "END EXAMPLE\n",
    "\n",
    "Your turn:            \n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? \n",
    "\"\"\"\n",
    "    inputs=[prompt_helper.gen_prompt(query)]\n",
    "elif model_name.startswith(\"microsoft\"):\n",
    "    # msft_template = \"<|endoftext|><|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "    msft_template = \"<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "    user_query = \"\"\"\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "    template = PromptTemplate(template=msft_template, input_variables=[\"query\"])\n",
    "    inputs=[template.format(query=user_query)]\n",
    "else: \n",
    "    inputs=[\"\"\"\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "[32010, 660, 29901, 14159, 756, 29871, 29941, 22556, 26563, 29889, 940, 1321, 952, 29871, 29906, 901, 508, 29879, 310, 22556, 26563, 29889, 7806, 508, 756, 29871, 29946, 22556, 26563, 29889, 1128, 1784, 22556, 26563, 947, 540, 505, 1286, 29973, 13, 29909, 29901, 14159, 4687, 411, 29871, 29941, 26563, 29889, 29871, 29906, 508, 29879, 310, 29871, 29946, 22556, 26563, 1269, 338, 29871, 29947, 22556, 26563, 29889, 29871, 29941, 718, 29871, 29947, 353, 29871, 29896, 29896, 29889, 450, 1234, 338, 29871, 29896, 29896, 29889, 13, 29984, 29901, 450, 274, 2142, 1308, 423, 750, 29871, 29906, 29941, 623, 793, 29889, 960, 896, 1304, 29871, 29906, 29900, 304, 1207, 301, 3322, 322, 18093, 29871, 29953, 901, 29892, 920, 1784, 623, 793, 437, 896, 505, 29973, 13, 32007, 32001]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(f\"{len(input_test_encoded)}\")\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "<|end|><|assistant|>\n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'torch_dtype': 'auto', 'device_map': 'auto', 'max_length': None, 'trust_remote_code': True, 'task': 'text-generation', 'max_new_tokens': 200, 'do_sample': True, 'temperature': 0.01, 'top_k': 3, 'top_p': 0.95, 'framework': 'pt'}\n"
     ]
    }
   ],
   "source": [
    "# bitsandbytes quantization does not work with MPS backend\n",
    "print(pipeline_kwargs)\n",
    "\n",
    "# transformer pipeline kwargs\n",
    "tp_kwargs = {\n",
    "    \"model\": model,\n",
    "    \"tokenizer\": tokenizer,\n",
    "}\n",
    "\n",
    "tp_config = ModelConfig(model_config = tp_kwargs)\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    **tp_config.get_config(),\n",
    "    **pipeline_kwargs,\n",
    "    **token_kwargs,\n",
    "    # **compression_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install autopep8 or black extension in VSCode\n",
    "`shift + opt + F` to auto format python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Allocated memory : 53.965210 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.13'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic, time\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.01, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        model_dependeny_kwargs = {}\n",
    "        if model_name.startswith(ModelCatalog.GOOGLE_FAMILY):\n",
    "            # for gemma \n",
    "            model_dependeny_kwargs = {\n",
    "                \"add_special_tokens\": True,\n",
    "            }\n",
    "        if model_name.startswith(ModelCatalog.MISTRAL_FAMILY):\n",
    "            model_dependeny_kwargs = {\n",
    "                \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            }\n",
    "        if model_name.startswith(ModelCatalog.META_FAMILY):\n",
    "            model_dependeny_kwargs = {\n",
    "                \"eos_token_id\" : tokenizer.eos_token_id\n",
    "            }\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=do_sample,\n",
    "            top_k=3,\n",
    "            top_p=0.95,\n",
    "            # num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            # eos_token_id=tokenizer.eos_token_id, # for llama\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.15,  # without this output begins repeating\n",
    "            **model_dependeny_kwargs,\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    # pprint(result)\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.01, max_new_tokens = 80, verbose=verbose)\n",
    "\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlflow autologging langchain\n",
    "* https://mlflow.org/docs/latest/llms/langchain/guide/index.html+\n",
    "* https://github.com/mlflow/mlflow/issues/9237#issuecomment-1667549626\n",
    "\n",
    "#### Issue\n",
    "* HuggingFacePipeline is not callable from mlflow run: https://github.com/langchain-ai/langchain/issues/8858\n",
    "\n",
    "#### LangChain Callback Handler\n",
    "* https://python.langchain.com/docs/integrations/providers/aim_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_ai_gateway\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow\n",
    "* https://api.python.langchain.com/en/latest/_modules/langchain_community/callbacks/mlflow_callback.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"./mlruns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "# from langchain.callbacks import MlflowCallbackHandler\n",
    "\n",
    "# Set the run name to time string\n",
    "run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_name = \"langchain\"\n",
    "search_pattern = f\"name = '{experiment_name}'\"\n",
    "experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "if len(experiments) < 1:\n",
    "    experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "    print(f\"experiment with string id {experiment_id} is created.\")\n",
    "else:\n",
    "    experiment_id = experiments[0].experiment_id\n",
    "    # experiment_id = experiments.experiment_id[0]\n",
    "    print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "# mlflow_callback = MlflowCallbackHandler(experiment=experiment_name, name=run_name)\n",
    "\n",
    "mlflow.end_run()\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "template = prompt_helper.gen_prompt(\"{input}\")\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "\n",
    "mlflow.log_param(\"system_prompt\", template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[mlflow_callback])\n",
    "\n",
    "# mlflow_callback.flush_tracker(llm_chain)\n",
    "\n",
    "# print(llm_chain.invoke({\"input\": inputs[0]}))\n",
    "# format the output of print with multiple lines of 60 max line length\n",
    "# response = llm_chain.run(inputs[0])\n",
    "dict_response = llm_chain.invoke(input={\"input\":inputs[0]})\n",
    "response = dict_response.get(\"text\", \"\")\n",
    "\n",
    "print(repr(llm_chain))\n",
    "\n",
    "mlflow.log_param(\"response\", response)\n",
    "\n",
    "# Evaluate the model on some example questions\n",
    "import pandas as pd\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"input\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \" +\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \" +\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \" +\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \" +\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \" +\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \" +\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \" +\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \" +\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \" +\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(eval_data)\n",
    "\n",
    "class LocalHfpModel():\n",
    "    \"\"\"local huggingface pipeline model\"\"\"\n",
    "    def __init__(self, llm_chain):\n",
    "        self.llm_chain = llm_chain\n",
    "    \n",
    "\n",
    "    def __call__(self, data):\n",
    "        # single call returns string\n",
    "        # response = self.llm_chain.run(data[\"input\"].tolist())\n",
    "        # self.results.append(response)\n",
    "        # GPU batch\n",
    "        response = self.llm_chain.batch(data[\"input\"].tolist())\n",
    "        # print(type(response))\n",
    "        # print(response)\n",
    "        return [ _dict[\"text\"] for _dict in response]\n",
    "\n",
    "# load the LocalHfpModel() to mlflow.evaluate\n",
    "results = mlflow.evaluate(\n",
    "    model=LocalHfpModel(llm_chain),\n",
    "    model_type=\"question-answering\",\n",
    "    targets=\"ground_truth\",\n",
    "    data=eval_data,\n",
    ")\n",
    "print(f\"See aggregated evaluation results below: \\n{results.metrics}\")\n",
    "\n",
    "# Evaluation result for each data record is available in `results.tables`.\n",
    "eval_table = results.tables[\"eval_results_table\"]\n",
    "print(f\"See evaluation table below: \\n{eval_table}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "pprint(response, indent=0, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the run name to time string\n",
    "# run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# experiment_name = \"local_llm_test\"\n",
    "# search_pattern = f\"name = '{experiment_name}'\"\n",
    "# experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "# if len(experiments) < 1:\n",
    "#     experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "#     print(f\"experiment with string id {experiment_id} is created.\")\n",
    "# else:\n",
    "#     experiment_id = experiments[0].experiment_id\n",
    "#     # experiment_id = experiments.experiment_id[0]\n",
    "#     print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "    \n",
    "# try:\n",
    "#     with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:\n",
    "#         logged_model = mlflow.langchain.log_model(\n",
    "#             lc_model=llm_chain,\n",
    "#             artifact_path=\"models\")\n",
    "        \n",
    "#     # Load the logged model using MLflow's Python function flavor\n",
    "#     loaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\n",
    "\n",
    "#     # Predict using the loaded model, with defined input schema from prompt template\n",
    "#     print(loaded_model.predict([{\"input\": inputs[0]}]))\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We automatically log the model and trace related artifacts\n",
    "# A model with name `lc_model` is registered, we can load it back as a PyFunc model\n",
    "# model_name = \"lc_model\"\n",
    "# model_version = 1\n",
    "# loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "# print(loaded_model.predict(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_mps_memory(tokenizer, generator):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_MEMORY = False\n",
    "# CLEAR_MEMORY = True\n",
    "\n",
    "if CLEAR_MEMORY:\n",
    "    clear_mps_memory(tokenizer=tokenizer, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs2 = [\"Which animal is the largest mammal?\"]\n",
    "inputs2 = [\"Can you tell me something about chron's disease?\"]\n",
    "\n",
    "# hallucination https://www.findacode.com/snomed/34000006--crohns-disease.html\n",
    "'''\n",
    "promt-response\n",
    "Result: \n",
    "Which snomed ct code has chron's disease?\n",
    "\n",
    "The SNOMED CT Code for Chronic Disease is 4621830.\n",
    "--------------------\n",
    "walltime: 4.950197219848633 in secs.\n",
    "--------------------\n",
    "Allocated memory : 54.158737 GB\n",
    "--------------------\n",
    "'''\n",
    "# real answer is 34000006, probably need a RAG \n",
    "# inputs2 = [\"Which snomed ct code has chron's disease?\"]\n",
    "\n",
    "# inputs2 = [\"Can you tell me more about the company nordcloud?\"]\n",
    "# inputs2 = [\"Can you tell me more about the company nordcloud in munich?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = chat(inputs2, temperature=0.01, max_new_tokens = 80, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
