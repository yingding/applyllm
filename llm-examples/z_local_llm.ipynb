{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "this notebook demos example of using llm in a MPS backend (apple silicon GPU) using torch 2.x\n",
    "\n",
    "Referece:\n",
    "* torch 2.x MPS Backend: https://pytorch.org/docs/stable/notes/mps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.9rc0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import applyllm as apl\n",
    "\n",
    "print(apl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yingding/Code/MODELS\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import (\n",
    "    DirectorySetting,\n",
    "    TokenHelper,\n",
    "    AcceleratorHelper,\n",
    ")\n",
    "  \n",
    "dir_mode_map = {\n",
    "    \"kf_notebook\": DirectorySetting(),\n",
    "    \"mac_local\": DirectorySetting(\n",
    "        home_dir=\"/Users/yingding/Code\", # \"/Users/yingding\"\n",
    "        transformers_cache_home=\"MODELS\", \n",
    "        huggingface_token_file=\"MODELS/.huggingface_token\"),\n",
    "}\n",
    "dir_setting = dir_mode_map[\"mac_local\"]\n",
    "\n",
    "# setup accelerator environment\n",
    "AcceleratorHelper.init_torch_env(accelerator=\"mps\", dir_setting=dir_setting)\n",
    "\n",
    "# global model maps setup\n",
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\":    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\":    \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    \"llama3-8B-inst\":   \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"llama3.2-3B-inst\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"llama3-70B-inst\":  \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    \"gemma7b-it\": \"google/gemma-7b-it\",\n",
    "    \"gemma7b\":    \"google/gemma-7b\",\n",
    "    \"gemma2b-it\": \"google/gemma-2b-it\",\n",
    "    \"gemma2b\":    \"google/gemma-2b\",\n",
    "    \"gemma7b-it-1.1\": \"google/gemma-1.1-7b-it\",\n",
    "    \"gemma2b-it-1.1\": \"google/gemma-1.1-2b-it\",\n",
    "    \"phi3-medium-128k-inst\": \"microsoft/Phi-3-medium-128k-instruct\",\n",
    "}\n",
    "\n",
    "# default_model_type = \"mistral7B-01\"\n",
    "default_model_type = \"llama3.2-3B-inst\"\n",
    "\n",
    "# default_dir_mode = \"mac_local\"\n",
    "# dir_setting = dir_mode_map[default_dir_mode]\n",
    "\n",
    "# os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "# os.environ['XDG_CACHE_HOME'] = dir_setting.get_cache_home()\n",
    "\n",
    "print(os.environ['XDG_CACHE_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0.0rc1\n",
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# check that MPS is availabe (Metal Performance Shaders)\n",
    "if not torch.backends.mps.is_available():\n",
    "    print(\"MPS is not available\")\n",
    "else:\n",
    "    print(\"MPS is available\")\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(mps_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"gemma7b-it\"\n",
    "# model_type = \"gemma2b-it\"\n",
    "# model_type = \"llama3-8B-inst\"\n",
    "model_type = \"llama3.2-3B-inst\"\n",
    "# model_type = \"phi3-medium-128k-inst\"\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"llama7B-chat\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, default_model_type)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast tokenizer\n",
    "\n",
    "* https://github.com/huggingface/transformers/issues/23889#issuecomment-1584090357"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Model and then Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token loaded\n",
      "model_kwargs: {'dtype': torch.float16, 'device_map': 'mps', 'max_length': None, 'trust_remote_code': True}\n",
      "pipeline_kwargs: {'dtype': torch.float16, 'device_map': 'mps', 'max_length': None, 'trust_remote_code': True, 'task': 'text-generation', 'max_new_tokens': 200, 'do_sample': True, 'temperature': 0.01, 'top_k': 3, 'top_p': 0.95}\n"
     ]
    }
   ],
   "source": [
    "from applyllm.pipelines import (\n",
    "    ModelCatalog,\n",
    "    KwargsBuilder\n",
    ")\n",
    "th = TokenHelper(dir_setting=dir_setting, prefix_list=[\"llama\"])\n",
    "token_kwargs = th.gen_token_kwargs(model_type=model_type)\n",
    "\n",
    "# data_type = torch.bfloat16\n",
    "if model_name.startswith(\"microsoft\"):\n",
    "    data_type = \"auto\"\n",
    "else:\n",
    "    data_type = torch.float16\n",
    "\n",
    "device_map = \"mps\" # \"auto\"\n",
    "# device_map = \"auto\"\n",
    "\n",
    "# auto caste not working for mps 4.38.2\n",
    "# https://github.com/huggingface/transformers/issues/29431 \n",
    "\n",
    "# mixtral model has no max_new_tokens limit, so it is not set here.\n",
    "model_kwargs = {\n",
    "    # \"torch_dtype\": data_type, # deprecated, replace with dtype\n",
    "    \"dtype\": data_type, #bfloat16 is not supported on MPS backend, float16 only on GPU accelerator\n",
    "    # torch_dtype=torch.float32,\n",
    "    # max_length=MAX_LENGTH,\n",
    "    \"device_map\": device_map,\n",
    "    \"max_length\" : None, # remove the total length of the generated response\n",
    "    \"trust_remote_code\" : True\n",
    "}\n",
    "print(f\"model_kwargs: {model_kwargs}\")\n",
    "\n",
    "# set the transformers.pipeline kwargs\n",
    "# the torch_dtype shall be set both for the model and the pipeline, due to a transformer issue.\n",
    "# otherwise it will cause unnecessary more memory usage in the pipeline of transformers\n",
    "# https://github.com/huggingface/transformers/issues/28817\n",
    "# https://github.com/mlflow/mlflow/pull/10979\n",
    "\n",
    "# if model_name.startswith(\"microsoft\"):\n",
    "#     do_sample = False\n",
    "# else:\n",
    "#     do_sample = True\n",
    "\n",
    "do_sample = True\n",
    "\n",
    "pipeline_kwargs = {\n",
    "    \"task\": \"text-generation\",\n",
    "    \"max_new_tokens\" : 200,\n",
    "    \"do_sample\" : do_sample, # do_sample True is required for temperature\n",
    "    \"temperature\" : 0.01, \n",
    "    \"device_map\" : device_map, # use the MPS device if available\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    # \"framework\": \"pt\", # use pytorch as framework, deprecated since transformers v5 only support torch\n",
    "}\n",
    "\n",
    "gemma_pipeline_kwargs = {\n",
    "    \"add_special_tokens\": True,\n",
    "    \"torch_dtype\": data_type,\n",
    "}\n",
    "\n",
    "# pipeline_kwargs override the model_kwargs during the merge\n",
    "pipeline_kwargs = KwargsBuilder([model_kwargs]).override(pipeline_kwargs).build()\n",
    "\n",
    "if model_name.startswith(ModelCatalog.GOOGLE_FAMILY):\n",
    "    pipeline_kwargs = KwargsBuilder([pipeline_kwargs]).override(gemma_pipeline_kwargs).build()\n",
    "\n",
    "print(f\"pipeline_kwargs: {pipeline_kwargs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475136"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mps.driver_allocated_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Recom.Max memory : 96.000000 GB\n",
      "Allocated memory : 0.000443 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import AcceleratorStatus\n",
    "acc_status = AcceleratorStatus.create_accelerator_status()\n",
    "\n",
    "# acc_status.accelerator_mem_info()\n",
    "acc_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver_allocated_mem = f\"{int(torch.mps.driver_allocated_memory()/1024**3)}GB\"\n",
    "# lm_allocated_mem = f\"{int(torch.mps.current_allocated_memory()/1024**3)}GB\"\n",
    "# print(driver_allocated_mem)\n",
    "# print(lm_allocated_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max memory to offload parts of LLM model to the CPU memory\n",
    "* https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#designing-a-device-map\n",
    "\n",
    "Note:\n",
    "* Max Memory offload to CPU is CUDA implementation only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accf5c355abd45a0bb82e7c5866375d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: load_model() python function\n",
      "walltime: 4.2993857860565186 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from applyllm.utils import time_func\n",
    "from applyllm.pipelines import ModelConfig, LocalCausalLMConfig\n",
    "\n",
    "# For a M1 max with 64GB memory, set the limit to 48GB\n",
    "# cuda_max_memory = {\n",
    "#   0: \"48GB\", # GPU device 0\n",
    "#   \"cpu\": \"1GB\", # CPU device with no memory, since M1 max has unified memory\n",
    "# } \n",
    "\n",
    "base_lm_config = ModelConfig(\n",
    "  model_config = {\n",
    "    \"pretrained_model_name_or_path\": model_name,\n",
    "    \"device_map\": device_map,\n",
    "    \"trust_remote_code\" : True,\n",
    "    # \"max_memory\": cuda_max_memory,\n",
    "  }\n",
    ")\n",
    "\n",
    "# No bitsandbytes qunatization support for MPS backend yet, set quantized to False\n",
    "kwargs = {\n",
    "  \"quantized\": False,\n",
    "  \"model_config\": base_lm_config.get_config(),\n",
    "  \"quantization_config\": {\n",
    "    \"quantization_config\": transformers.BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type='nf4',\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16\n",
    "      )\n",
    "    }\n",
    "}\n",
    "\n",
    "lm_config = LocalCausalLMConfig(**kwargs)\n",
    "\n",
    "@time_func\n",
    "def load_model():\n",
    "  return AutoModelForCausalLM.from_pretrained(    \n",
    "    **lm_config.get_config(),\n",
    "    **token_kwargs,  \n",
    "  )\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"model_config\": {\n",
    "        \"pretrained_model_name_or_path\": model_name,\n",
    "        \"device\": \"cpu\",\n",
    "        # \"device_map\": \"auto\", # put to GPU if GPU is available\n",
    "        # \"max_position_embeddings\": MAX_LENGTH,\n",
    "        # \"max_length\": MAX_LENGTH,\n",
    "    },\n",
    "}\n",
    "tokenizer_config = ModelConfig(**tokenizer_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    **tokenizer_config.get_config(),\n",
    "    **token_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_tokenizers.TokenizersBackend'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing token\n",
    "* https://huggingface.co/docs/tokenizers/pipeline\n",
    "\n",
    "## Phi instruct template\n",
    "* https://huggingface.co/microsoft/Phi-3-small-128k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from applyllm.pipelines import (\n",
    "    ModelCatalog,\n",
    "    PromptHelper\n",
    ")\n",
    "\n",
    "model_info = ModelCatalog.get_model_info(model_name)\n",
    "prompt_helper = PromptHelper(model_info)\n",
    "\n",
    "\n",
    "\n",
    "if model_info.model_family == ModelCatalog.GOOGLE_FAMILY:\n",
    "    query = \"\"\"BEGIN EXAMPLE\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "END EXAMPLE\n",
    "\n",
    "Your turn:            \n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? \n",
    "\"\"\"\n",
    "    inputs=[prompt_helper.gen_prompt(query)]\n",
    "elif model_name.startswith(\"microsoft\"):\n",
    "    # msft_template = \"<|endoftext|><|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "    msft_template = \"<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "    user_query = \"\"\"\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "    template = PromptTemplate(template=msft_template, input_variables=[\"query\"])\n",
    "    inputs=[template.format(query=user_query)]\n",
    "else: \n",
    "    inputs=[\"\"\"\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "[128000, 198, 48, 25, 29607, 706, 220, 18, 32515, 20953, 13, 1283, 50631, 220, 17, 810, 43732, 315, 32515, 20953, 13, 9062, 649, 706, 220, 19, 32515, 20953, 13, 2650, 1690, 32515, 20953, 1587, 568, 617, 1457, 5380, 32, 25, 29607, 3940, 449, 220, 18, 20953, 13, 220, 17, 43732, 315, 220, 19, 32515, 20953, 1855, 374, 220, 23, 32515, 20953, 13, 220, 18, 489, 220, 23, 284, 220, 806, 13, 578, 4320, 374, 220, 806, 627, 48, 25, 578, 94948, 1047, 220, 1419, 41776, 13, 1442, 814, 1511, 220, 508, 311, 1304, 16163, 323, 11021, 220, 21, 810, 11, 1268, 1690, 41776, 656, 814, 617, 5380]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(f\"{len(input_test_encoded)}\")\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dtype': torch.float16, 'device_map': 'mps', 'max_length': None, 'trust_remote_code': True, 'task': 'text-generation', 'max_new_tokens': 200, 'do_sample': True, 'temperature': 0.01, 'top_k': 3, 'top_p': 0.95}\n"
     ]
    }
   ],
   "source": [
    "# bitsandbytes quantization does not work with MPS backend\n",
    "print(pipeline_kwargs)\n",
    "\n",
    "# transformer pipeline kwargs\n",
    "tp_kwargs = {\n",
    "    \"model\": model,\n",
    "    \"tokenizer\": tokenizer,\n",
    "}\n",
    "\n",
    "tp_config = ModelConfig(model_config = tp_kwargs)\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    **tp_config.get_config(),\n",
    "    **pipeline_kwargs,\n",
    "    **token_kwargs,\n",
    "    # **compression_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install autopep8 or black extension in VSCode\n",
    "`shift + opt + F` to auto format python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Recom.Max memory : 96.000000 GB\n",
      "Allocated memory : 5.992630 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.5'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic, time\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.01, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        model_dependeny_kwargs = {}\n",
    "        if model_name.startswith(ModelCatalog.GOOGLE_FAMILY):\n",
    "            # for gemma \n",
    "            model_dependeny_kwargs = {\n",
    "                \"add_special_tokens\": True,\n",
    "            }\n",
    "        if model_name.startswith(ModelCatalog.MISTRAL_FAMILY):\n",
    "            model_dependeny_kwargs = {\n",
    "                \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            }\n",
    "        if model_name.startswith(ModelCatalog.META_FAMILY):\n",
    "            model_dependeny_kwargs = {\n",
    "                \"eos_token_id\" : tokenizer.eos_token_id\n",
    "            }\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=do_sample,\n",
    "            top_k=3,\n",
    "            top_p=0.95,\n",
    "            # num_return_sequences=1,\n",
    "            # pad_token_id=tokenizer.eos_token_id, # for mistral\n",
    "            # eos_token_id=tokenizer.eos_token_id, # for llama\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.15,  # without this output begins repeating\n",
    "            return_full_text=False,\n",
    "            **model_dependeny_kwargs,\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    # pprint(result)\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "A: They started with 23 apples. Then they took out 20 for lunch. That leaves them with 23 - 20 = 3 apples. After buying 6 more, that makes 3 + 6 = 9 apples. The answer is 9.\n",
      "\n",
      "The key here is to follow the order in which things happen. First you take away what was given (the 20\n",
      "--------------------\n",
      "walltime: 3.395456075668335 in secs.\n",
      "--------------------\n",
      "Recom.Max memory : 96.000000 GB\n",
      "Allocated memory : 6.091629 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.01, max_new_tokens = 80, verbose=verbose)\n",
    "\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlflow autologging langchain\n",
    "* https://mlflow.org/docs/latest/llms/langchain/guide/index.html+\n",
    "* https://github.com/mlflow/mlflow/issues/9237#issuecomment-1667549626\n",
    "\n",
    "#### Issue\n",
    "* HuggingFacePipeline is not callable from mlflow run: https://github.com/langchain-ai/langchain/issues/8858\n",
    "\n",
    "#### LangChain Callback Handler\n",
    "* https://python.langchain.com/docs/integrations/providers/aim_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_tracking\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow_ai_gateway\n",
    "* https://python.langchain.com/docs/integrations/providers/mlflow\n",
    "* https://api.python.langchain.com/en/latest/_modules/langchain_community/callbacks/mlflow_callback.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"./mlruns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the HuggingfacePipelines with LCEL\n",
    "\n",
    "* Deprecated with LCEL and custom class https://docs.langchain.com/oss/python/integrations/llms/huggingface_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 22:33:03 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/15 22:33:03 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025/12/15 22:33:03 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/15 22:33:03 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2025/12/15 22:33:03 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/15 22:33:03 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2025/12/15 22:33:03 DEBUG mlflow.utils.databricks_utils: dbutils not available, checking environment variable\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment with string id 1 is reused.\n",
      "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template=\"[INST]<<SYS>>You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information.<</SYS>>\\n\\n{input}\\n[/INST]\")\n",
      "| RunnableLambda(local_chat_wrapper)\n",
      "| StrOutputParser()\n",
      "             input                                       ground_truth\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...\n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 22:33:12 DEBUG mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2025/12/15 22:33:12 DEBUG mlflow.utils.autologging_utils: Called autolog() method for sklearn autologging with args '()' and kwargs '{'log_input_examples': False, 'log_model_signatures': True, 'log_models': True, 'log_datasets': True, 'disable': True, 'exclusive': False, 'disable_for_unsupported_versions': False, 'silent': False, 'max_tuning_runs': 5, 'log_post_training_metrics': True, 'serialization_format': 'cloudpickle', 'registered_model_name': None, 'pos_label': None, 'extra_tags': None}'\n",
      "2025/12/15 22:33:12 DEBUG mlflow.utils.autologging_utils: Called autolog() method for pytorch autologging with args '()' and kwargs '{'log_every_n_epoch': 1, 'log_every_n_step': None, 'log_models': True, 'log_datasets': True, 'disable': True, 'exclusive': False, 'disable_for_unsupported_versions': False, 'silent': False, 'registered_model_name': None, 'extra_tags': None, 'checkpoint': True, 'checkpoint_monitor': 'val_loss', 'checkpoint_mode': 'min', 'checkpoint_save_best_only': True, 'checkpoint_save_weights_only': False, 'checkpoint_save_freq': 'epoch', 'log_model_signatures': True}'\n",
      "2025/12/15 22:33:12 DEBUG mlflow.utils.autologging_utils: Called autolog() method for transformers autologging with args '()' and kwargs '{'log_input_examples': False, 'log_model_signatures': False, 'log_models': False, 'log_datasets': False, 'disable': True, 'exclusive': False, 'disable_for_unsupported_versions': False, 'silent': False, 'extra_tags': None}'\n",
      "2025/12/15 22:33:12 DEBUG mlflow.utils.autologging_utils: Called autolog() method for bedrock autologging with args '()' and kwargs '{'log_traces': True, 'disable': False, 'silent': True}'\n",
      "2025/12/15 22:33:12 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "2025/12/15 22:33:12 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "/Users/yingding/Code/VENV/applyllm3.12/lib/python3.12/site-packages/mlflow/models/evaluation/evaluators/default.py:100: FutureWarning: ``mlflow.metrics.token_count`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
      "  token_count(),\n",
      "/Users/yingding/Code/VENV/applyllm3.12/lib/python3.12/site-packages/mlflow/models/evaluation/evaluators/default.py:101: FutureWarning: ``mlflow.metrics.toxicity`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
      "  toxicity(),\n",
      "/Users/yingding/Code/VENV/applyllm3.12/lib/python3.12/site-packages/mlflow/models/evaluation/evaluators/default.py:102: FutureWarning: ``mlflow.metrics.flesch_kincaid_grade_level`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
      "  flesch_kincaid_grade_level(),\n",
      "/Users/yingding/Code/VENV/applyllm3.12/lib/python3.12/site-packages/mlflow/models/evaluation/evaluators/default.py:103: FutureWarning: ``mlflow.metrics.ari_grade_level`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
      "  ari_grade_level(),\n",
      "/Users/yingding/Code/VENV/applyllm3.12/lib/python3.12/site-packages/mlflow/models/evaluation/evaluators/default.py:109: FutureWarning: ``mlflow.metrics.exact_match`` is deprecated since 3.4.0. Use the new GenAI evaluation functionality instead. See https://mlflow.org/docs/latest/genai/eval-monitor/legacy-llm-evaluation/ for the migration guide.\n",
      "  builtin_metrics = [*text_metrics, exact_match()]\n",
      "2025/12/15 22:33:23 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34be085cfd6465bb1eaacb532074914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification LOAD REPORT from: facebook/roberta-hate-speech-dynabench-r4-target\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2025/12/15 22:33:37 DEBUG mlflow.utils.autologging_utils: Called autolog() method for sklearn autologging with args '()' and kwargs '{'log_input_examples': False, 'log_model_signatures': True, 'log_models': True, 'log_datasets': True, 'disable': True, 'exclusive': False, 'disable_for_unsupported_versions': False, 'silent': False, 'max_tuning_runs': 5, 'log_post_training_metrics': True, 'serialization_format': 'cloudpickle', 'registered_model_name': None, 'pos_label': None, 'extra_tags': None}'\n",
      "2025/12/15 22:33:37 DEBUG mlflow.utils.autologging_utils: Called autolog() method for pytorch autologging with args '()' and kwargs '{'log_every_n_epoch': 1, 'log_every_n_step': None, 'log_models': True, 'log_datasets': True, 'disable': True, 'exclusive': False, 'disable_for_unsupported_versions': False, 'silent': False, 'registered_model_name': None, 'extra_tags': None, 'checkpoint': True, 'checkpoint_monitor': 'val_loss', 'checkpoint_mode': 'min', 'checkpoint_save_best_only': True, 'checkpoint_save_weights_only': False, 'checkpoint_save_freq': 'epoch', 'log_model_signatures': True}'\n",
      "2025/12/15 22:33:37 DEBUG mlflow.utils.autologging_utils: Called autolog() method for transformers autologging with args '()' and kwargs '{'log_input_examples': False, 'log_model_signatures': False, 'log_models': False, 'log_datasets': False, 'disable': True, 'exclusive': False, 'disable_for_unsupported_versions': False, 'silent': False, 'extra_tags': None}'\n",
      "2025/12/15 22:33:37 DEBUG mlflow.utils.autologging_utils: Called autolog() method for bedrock autologging with args '()' and kwargs '{'log_traces': True, 'disable': True, 'silent': False}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See aggregated evaluation results below: \n",
      "{'toxicity/v1/mean': np.float64(0.0038049566937843338), 'toxicity/v1/variance': np.float64(1.219685069049955e-05), 'toxicity/v1/p90': np.float64(0.0065988758840831), 'toxicity/v1/ratio': 0.0, 'flesch_kincaid_grade_level/v1/mean': np.float64(11.9282040134896), 'flesch_kincaid_grade_level/v1/variance': np.float64(0.00561902642900983), 'flesch_kincaid_grade_level/v1/p90': np.float64(11.988172145981501), 'ari_grade_level/v1/mean': np.float64(13.974392486011194), 'ari_grade_level/v1/variance': np.float64(0.19063240344574592), 'ari_grade_level/v1/p90': np.float64(14.323684252597921), 'exact_match/v1': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c8aab4856143039491907b3a51ea04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See evaluation table below: \n",
      "             input                                       ground_truth  \\\n",
      "0  What is MLflow?  MLflow is an open-source platform for managing...   \n",
      "1   What is Spark?  Apache Spark is an open-source, distributed co...   \n",
      "\n",
      "                                             outputs  token_count  \\\n",
      "0  Result: \\n \\n---\\n\\nMLflow is an open-source p...          202   \n",
      "1  Result: \\n | delsendr\\nThe term \"Spark\" can re...          203   \n",
      "\n",
      "   toxicity/v1/score  flesch_kincaid_grade_level/v1/score  \\\n",
      "0           0.007297                            12.003164   \n",
      "1           0.000313                            11.853244   \n",
      "\n",
      "   ari_grade_level/v1/score  \n",
      "0                 14.411007  \n",
      "1                 13.537778  \n",
      "('Result: \\n'\n",
      " ' (Source: https://www.spooned.org/2019/04/24/how-many-tennis-balls-does-roger-have-now/) \\n'\n",
      " '\\n'\n",
      " '## Step 1: Understand the initial condition\\n'\n",
      " 'Roger starts with 3 tennis balls.\\n'\n",
      " '\\n'\n",
      " '## Step 2: Calculate the number of tennis balls in the additional cans\\n'\n",
      " 'Each can contains 4 tennis balls, and there are 2 more cans purchased. So, the total number of '\n",
      " 'tennis balls from these cans is 2 * 4 = 8.\\n'\n",
      " '\\n'\n",
      " '## Step 3: Add the initial tennis balls to those in the new cans\\n'\n",
      " 'To find out how many tennis balls Roger has now, we need to add the initial 3 tennis balls to '\n",
      " 'the 8 tennis balls from the new cans. This gives us 3 + 8 = 11.\\n'\n",
      " '\\n'\n",
      " 'The final answer is: $\\\\boxed{11}$')\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.models\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Use SQLite backend to avoid filesystem deprecation warning\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Set the run name to time string\n",
    "run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "experiment_name = \"langchain\"\n",
    "search_pattern = f\"name = '{experiment_name}'\"\n",
    "experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "if len(experiments) < 1:\n",
    "    experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "    print(f\"experiment with string id {experiment_id} is created.\")\n",
    "else:\n",
    "    experiment_id = experiments[0].experiment_id\n",
    "    print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "mlflow.end_run()\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "mlflow.start_run(run_name=run_name)\n",
    "\n",
    "# Define custom LLM using chat_gen\n",
    "def local_chat_wrapper(prompt_text):\n",
    "    # chat is defined in previous cells\n",
    "    # chat returns a list of lists of strings\n",
    "    responses = chat([str(prompt_text)], temperature=0.01, max_new_tokens=200, verbose=False)\n",
    "    if responses and len(responses) > 0 and len(responses[0]) > 0:\n",
    "        return responses[0][0]\n",
    "    return \"\"\n",
    "\n",
    "llm = RunnableLambda(local_chat_wrapper)\n",
    "\n",
    "template = prompt_helper.gen_prompt(\"{input}\")\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "\n",
    "mlflow.log_param(\"system_prompt\", template)\n",
    "\n",
    "# LCEL Chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke chain\n",
    "response = chain.invoke({\"input\": inputs[0]})\n",
    "\n",
    "print(repr(chain))\n",
    "\n",
    "mlflow.log_param(\"response\", response)\n",
    "\n",
    "# Evaluate the model on some example questions\n",
    "import pandas as pd\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"input\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \" +\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \" +\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \" +\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \" +\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \" +\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \" +\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \" +\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \" +\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \" +\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(eval_data)\n",
    "\n",
    "class LocalLCELModel():\n",
    "    \"\"\"local LCEL model wrapper for mlflow\"\"\"\n",
    "    def __init__(self, chain):\n",
    "        self.chain = chain\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        # GPU batch\n",
    "        # chain.batch expects a list of inputs. \n",
    "        # Since the chain expects {\"input\": ...}, we need to format the input list\n",
    "        inputs = [{\"input\": text} for text in data[\"input\"].tolist()]\n",
    "        response = self.chain.batch(inputs)\n",
    "        # response is a list of strings (due to StrOutputParser)\n",
    "        return response\n",
    "\n",
    "# load the LocalLCELModel() to mlflow.models.evaluate\n",
    "results = mlflow.models.evaluate(\n",
    "    model=LocalLCELModel(chain),\n",
    "    model_type=\"question-answering\",\n",
    "    targets=\"ground_truth\",\n",
    "    data=eval_data,\n",
    ")\n",
    "print(f\"See aggregated evaluation results below: \\n{results.metrics}\")\n",
    "\n",
    "# Evaluation result for each data record is available in `results.tables`.\n",
    "eval_table = results.tables[\"eval_results_table\"]\n",
    "print(f\"See evaluation table below: \\n{eval_table}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "pprint(response, indent=0, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the run name to time string\n",
    "# run_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# experiment_name = \"local_llm_test\"\n",
    "# search_pattern = f\"name = '{experiment_name}'\"\n",
    "# experiments = mlflow.search_experiments(filter_string=search_pattern)\n",
    "\n",
    "# if len(experiments) < 1:\n",
    "#     experiment_id = mlflow.create_experiment(name=experiment_name)\n",
    "#     print(f\"experiment with string id {experiment_id} is created.\")\n",
    "# else:\n",
    "#     experiment_id = experiments[0].experiment_id\n",
    "#     # experiment_id = experiments.experiment_id[0]\n",
    "#     print(f\"experiment with string id {experiment_id} is reused.\")\n",
    "\n",
    "    \n",
    "# try:\n",
    "#     with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:\n",
    "#         logged_model = mlflow.langchain.log_model(\n",
    "#             lc_model=llm_chain,\n",
    "#             artifact_path=\"models\")\n",
    "        \n",
    "#     # Load the logged model using MLflow's Python function flavor\n",
    "#     loaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)\n",
    "\n",
    "#     # Predict using the loaded model, with defined input schema from prompt template\n",
    "#     print(loaded_model.predict([{\"input\": inputs[0]}]))\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We automatically log the model and trace related artifacts\n",
    "# A model with name `lc_model` is registered, we can load it back as a PyFunc model\n",
    "# model_name = \"lc_model\"\n",
    "# model_version = 1\n",
    "# loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "# print(loaded_model.predict(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_mps_memory(tokenizer, generator):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_MEMORY = False\n",
    "# CLEAR_MEMORY = True\n",
    "\n",
    "if CLEAR_MEMORY:\n",
    "    clear_mps_memory(tokenizer=tokenizer, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Recom.Max memory : 96.000000 GB\n",
      "Allocated memory : 7.902573 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npromt-response\\nResult: \\nWhich snomed ct code has chron's disease?\\n\\nThe SNOMED CT Code for Chronic Disease is 4621830.\\n--------------------\\nwalltime: 4.950197219848633 in secs.\\n--------------------\\nAllocated memory : 54.158737 GB\\n--------------------\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs2 = [\"Which animal is the largest mammal?\"]\n",
    "inputs2 = [\"Can you tell me something about chron's disease?\"]\n",
    "\n",
    "# hallucination https://www.findacode.com/snomed/34000006--crohns-disease.html\n",
    "'''\n",
    "promt-response\n",
    "Result: \n",
    "Which snomed ct code has chron's disease?\n",
    "\n",
    "The SNOMED CT Code for Chronic Disease is 4621830.\n",
    "--------------------\n",
    "walltime: 4.950197219848633 in secs.\n",
    "--------------------\n",
    "Allocated memory : 54.158737 GB\n",
    "--------------------\n",
    "'''\n",
    "# real answer is 34000006, probably need a RAG \n",
    "# inputs2 = [\"Which snomed ct code has chron's disease?\"]\n",
    "\n",
    "# inputs2 = [\"Can you tell me more about the company nordcloud?\"]\n",
    "# inputs2 = [\"Can you tell me more about the company nordcloud in munich?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      " and what are the possible treatments?\n",
      "Chronic obstructive pulmonary disease (COPD) is a progressive lung disease that makes it difficult to breathe. It is characterized by inflammation, airway narrowing, and mucus production in the lungs.\n",
      "\n",
      "**Causes of COPD:**\n",
      "\n",
      "1. Smoking: The most common cause of COPD, accounting for 80-90% of cases.\n",
      "2.\n",
      "--------------------\n",
      "walltime: 3.90580415725708 in secs.\n",
      "--------------------\n",
      "Recom.Max memory : 96.000000 GB\n",
      "Allocated memory : 7.902710 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "responses = chat(inputs2, temperature=0.01, max_new_tokens = 80, verbose=verbose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applyllm3.12 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
