{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4946d3e4-c86a-497c-a0e4-21c89b93799d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@created: 24.11.2023\\\n",
    "@updated: 29.01.2024\\\n",
    "@version: 5\n",
    "\n",
    "This notebook comprises of examples to use transformer, pytorch, llama2, langchain to achive entity extraction with engineered prompts.\n",
    "\n",
    "Note:\\\n",
    "`RuntimeError:NVML_SICCESS ... in PyTorch using the Llama2 13B means OOM with 20GB MIG`\n",
    "https://github.com/pytorch/pytorch/issues/112377\n",
    "\n",
    "Use `nvidia.com/mig-3g.40gb` instead of `nvidia.com/mig-2g.20gb` solves the issue.\n",
    "\n",
    "In the map reduce RAG pattern, ran into recency bias in the reduce pattern. Due to the previous hallucination.\\\n",
    "Possible solution\n",
    "* add end token in the map steps\n",
    "* use rerank pattern\n",
    "* better rag retrieval and embedding model\n",
    "* large rag chunck\n",
    "\n",
    "Question:\n",
    "* is there a \"end\" special token for llama2 https://www.reddit.com/r/LocalLLaMA/comments/167v3cd/llama_2_tokenizer_and_the_special_tokens/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9302288-5f83-403a-81e4-d1cc15e28272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824c68ca-4eee-4418-8d98-ac8e9b8f6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install --user --upgrade kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb58e2b-6559-4db3-9ebc-95ef05f2b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac05d7-4f2e-4fc0-99f8-bca2ffffe3cc",
   "metadata": {},
   "source": [
    "## Use the cuda 118 and torch 2.1.0 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a925dce-3bdf-4a58-9661-7715a977c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --user --upgrade -r ./requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbbc3d6-9d67-4a4c-84a9-e2e1c75351b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48662e9c-04da-455f-a80d-bd8136ee5f2e",
   "metadata": {},
   "source": [
    "## Additional technical informaiton\n",
    "#### Useful installation for KF notebook 1.7.0 cu118 drivers\n",
    "\n",
    "cuda118\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade torch==2.0.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "`xformers==0.0.21` need `torch==2.1.2`\n",
    "\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade xformers==0.0.21 torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "```\n",
    "\n",
    "show js loading with ipywidgets\n",
    "```shell\n",
    "#!{sys.executable} -m pip install --user --upgrade ipywidgets==8.1.0 comm==0.1.4 jupyterlab-widgets==3.0.8 widgetsnbextension==4.0.8\n",
    "```\n",
    "\n",
    "uninstall\n",
    "```shell\n",
    "#!{sys.executable} -m pip uninstall accelerator transformers xformers torch -y \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7468a-cdc9-462f-98ab-c6b64d1be424",
   "metadata": {},
   "source": [
    "## (optional) restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff70f6-cd4d-4298-91bf-c7c948e6fc72",
   "metadata": {},
   "source": [
    "### (optional) Set huggingface cli in terminal\n",
    "\n",
    "```shell\n",
    "PATH=${PATH}:/home/jupyter/.local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9795f7a-5be2-49dd-b889-d363885fddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) uncomment the following lines to set path in python notebook cell for notebook session \n",
    "# PATH=%env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d79b-17bd-4290-84c5-136c817d41e3",
   "metadata": {},
   "source": [
    "#### Basics of GPU\n",
    "\n",
    "Multi GPU inference: https://github.com/tloen/alpaca-lora/issues/445\n",
    "\n",
    "Show accelerator device IDs:\n",
    "\n",
    "```shell\n",
    "nvidia-smi -L\n",
    "```\n",
    "\n",
    "Nvidia usage\n",
    "```shell\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "python lib: gpustat\n",
    "```python\n",
    "gpustat -cp\n",
    "```\n",
    "\n",
    "* https://stackoverflow.com/questions/8223811/a-top-like-utility-for-monitoring-cuda-activity-on-a-gpu\n",
    "\n",
    "Check GPU info in PyTorch\n",
    "* https://stackoverflow.com/questions/48152674/how-do-i-check-if-pytorch-is-using-the-gpu\n",
    "* CUDA memory management https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627d00c-12e8-44c9-a62f-e0da1d0457e3",
   "metadata": {},
   "source": [
    "#### Extract the GPU Accelerator MIG UUIDs\n",
    "\n",
    "* Extract with re.search and group: https://note.nkmk.me/en/python-str-extract/\n",
    "* Extract with pattern before and after: https://stackoverflow.com/questions/4666973/how-to-extract-the-substring-between-two-markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7423-8550-48c4-96f7-54670ee9b632",
   "metadata": {},
   "source": [
    "#### PyTorch distributed with device UUID\n",
    "* https://discuss.pytorch.org/t/world-size-and-rank-torch-distributed-init-process-group/57438"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7548a80-c908-4d3f-be5a-b39405036b61",
   "metadata": {},
   "source": [
    "#### CUDA MIG memory notice\n",
    "The following python command shall show the available MIG memory\n",
    "```shell\n",
    "print(torch.cuda.mem_get_info())\n",
    "for e in torch.cuda.mem_get_info():\n",
    "    print(e/1024**3)\n",
    "```\n",
    "The first tuple shows the availabe MIG cuda memory, if it goes to zero, and no process is attached,\n",
    "this means a cuda process is hang.\n",
    "```console\n",
    "(20748107776, 20937965568)\n",
    "19.32318115234375\n",
    "19.5\n",
    "```\n",
    "\n",
    "To terminate a cuda process, log into the GPU host\n",
    "```shell\n",
    "nvidia-smi # find out the PID something like 830333\n",
    "sudo kill -9 PID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2594fcbd-b0a4-42e0-8ce8-e524423d7a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5979a1e-9a9f-403a-9e0b-41e4dc4686f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys\n",
    "from util.accelerator_utils import (\n",
    "    AcceleratorStatus, \n",
    "    AcceleratorHelper, \n",
    "    DIR_MODE_MAP,\n",
    "    TokenHelper as th\n",
    ")\n",
    "\n",
    "dir_setting = dir_setting=DIR_MODE_MAP.get(\"kf_notebook\")\n",
    "\n",
    "gpu_status = AcceleratorStatus()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a268056f-65d5-4e2c-a73a-75570990bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_helper = AcceleratorHelper()\n",
    "# dynamically fetch attached accelerator devices\n",
    "UUIDs = gpu_helper.nvidia_device_uuids_filtered_by(is_mig=True, log_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd3f3cf-f74e-4377-964d-a5c0c1b667bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIG-9579f618-ddae-5958-9285-3207382f0b36\n",
      "/home/jovyan/llm-models/core-kind/yinwang/models\n"
     ]
    }
   ],
   "source": [
    "# init all the cuda torch env and model download cache directory\n",
    "gpu_helper.init_cuda_torch(UUIDs, dir_setting)\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(os.environ[\"XDG_CACHE_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d02ef10e-ae83-43ec-ad9e-b3bb80ea7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    # \"70B\" : \"meta-llama/Llama-2-70b-hf\"\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0b9ef8-b1be-4d77-8959-b5c4262721c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"mixtral8x7B-01\"\n",
    "model_type = \"mixtral8x7B-inst01\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, \"7B\")\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f68f45ef-874d-4539-9564-81772f85a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.2\n",
      "2.1.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "# from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7aeb893-f4f5-484a-b95b-28f7728c18f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the token\n",
    "\"\"\"\n",
    "def gen_token_kwargs():\n",
    "    # call method from token helper class\n",
    "    if th.need_token(model_type):\n",
    "        # kwargs = {\"use_auth_token\": get_token(dir_setting)}\n",
    "        token_kwargs = {\n",
    "            \"token\": th.get_token(dir_setting),\n",
    "            # \"truncation_side\": \"left\",\n",
    "            # \"return_tensors\": \"pt\",            \n",
    "                        }\n",
    "        print(\"huggingface token loaded\")\n",
    "    else:\n",
    "        token_kwargs = {}\n",
    "        print(\"huggingface token is NOT needed\")\n",
    "    return token_kwargs\n",
    "\n",
    "token_kwargs = gen_token_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ef9e55-b6d6-4760-ac5e-705b5076361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 0.000000 GB\n",
      "Allocated memory : 0.000000 GB\n",
      "Free      memory : 0.000000 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from util.accelerator_utils import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e22c2c4-48ac-4033-a9b6-e6927f74b6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'37GB'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451058b-fdec-4165-a3a7-1883b36dbdc7",
   "metadata": {},
   "source": [
    "## Following this approach to load llama2 model with bitsandbytes\n",
    "* https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-13b-retrievalqa.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f6116-4a42-4753-8f98-7f06ef53b03d",
   "metadata": {},
   "source": [
    "## 4bit  quantization\n",
    "\n",
    "Load pretrained model first, then the tokenizer.\n",
    "\n",
    "<table>\n",
    "    <!-- row 1-->\n",
    "<tr>\n",
    "<th>\n",
    "Llama-2-13b-chat-hf\n",
    "</th>\n",
    "<th>\n",
    "Mixtral-8x7B-v0.1\n",
    "</th>\n",
    "\n",
    "<th>\n",
    "Mixtral-8x7B-Instruct-v0.1\n",
    "</th>\n",
    "</tr>\n",
    "    <!-- row 2 -->\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 7.085938 GB\n",
    "Allocated memory : 6.809501 GB\n",
    "Free      memory : 0.276437 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 23.496094 GB\n",
    "Allocated memory : 23.303491 GB\n",
    "Free      memory : 0.192603 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<pre>\n",
    "num_of_gpus: 1\n",
    "--------------------\n",
    "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
    "Device idx       : 0 \n",
    "No. of processors: 42\n",
    "Physical  memory : 39.250000 GB\n",
    "Reserved  memory : 23.496094 GB\n",
    "Allocated memory : 23.303491 GB\n",
    "Free      memory : 0.192603 GB\n",
    "--------------------\n",
    "</pre>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c825194-473c-4f05-a1e4-9feb6b2475e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1e+03 ns, total: 1e+03 ns\n",
      "Wall time: 3.58 µs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e6665cc55d4ae08c09ef5b7e464528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time\n",
    "# in Transformer 4.32.1 need to use \"token\" parameter\n",
    "# in Transformer 4.30.x need to use \"use_auth_token\" parameter\n",
    "# with torch.no_grad():\n",
    "\n",
    "from torch import bfloat16\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name, #'decapoda-research/llama-7b-hf',\n",
    "  device_map='auto',\n",
    "  quantization_config=bnb_config,\n",
    "  # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB',\n",
    "  **token_kwargs,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c72c75b9-5d20-458e-9996-647105f0db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.496094 GB\n",
      "Allocated memory : 23.303491 GB\n",
      "Free      memory : 0.192603 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253126b-1dd9-4b0e-8c44-9c5df3191492",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lama2 max position embeddings\n",
    "Default is set to be 2048\n",
    "* https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaConfig.max_position_embeddings\n",
    "\n",
    "Set teh max_length for the tokenizer, Transformer issues:\n",
    "* https://github.com/huggingface/transformers/issues/1791#issuecomment-553397054\n",
    "* https://github.com/huggingface/transformers/pull/1833\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54b9a854-49ad-4c97-ba33-f7f7d6edb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POSITION_EMBEDDINGS = 3072\n",
    "MAX_LENGTH = 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    device='cpu',\n",
    "    max_position_embeddings=MAX_LENGTH,\n",
    "    max_length=MAX_LENGTH,\n",
    "    **token_kwargs,\n",
    "    # device_map=\"auto\", # put to GPU\n",
    "    # use_auth_token=token, #transformer==4.31.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc864b9a-eac7-4335-bbdc-a711b8b664b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mixtral-8x7B-Instruct-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f43af-d423-436d-95e2-b45465ed5be2",
   "metadata": {},
   "source": [
    "#### Testing tokenizer\n",
    "https://huggingface.co/docs/tokenizers/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "767706d4-ffd0-4e73-aec2-e18de0eb617c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs=[\"\"\"Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\\n\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55be8858-a5ce-4467-a82e-cc1dc8b8a8db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1186, 28747, 14115, 659, 28705, 28770, 19552, 16852, 28723, 650, 957, 846, 28705, 28750, 680, 277, 509, 302, 19552, 16852, 28723, 7066, 541, 659, 28705, 28781, 19552, 16852, 28723, 1602, 1287, 19552, 16852, 1235, 400, 506, 1055, 28804, 13, 28741, 28747, 14115, 2774, 395, 28705, 28770, 16852, 28723, 28705, 28750, 277, 509, 302, 28705, 28781, 19552, 16852, 1430, 349, 28705, 28783, 19552, 16852, 28723, 28705, 28770, 648, 28705, 28783, 327, 28705, 28740, 28740, 28723, 415, 4372, 349, 28705, 28740, 28740, 28723, 13, 28824, 28747, 415, 18302, 1623, 515, 553, 28705, 28750, 28770, 979, 2815, 28723, 1047, 590, 1307, 28705, 28750, 28734, 298, 1038, 9957, 304, 7620, 28705, 28784, 680, 28725, 910, 1287, 979, 2815, 511, 590, 506, 28804, 13, 13]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f725e85f-ceab-4c4d-8efe-03fbf089a2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c182261-d89f-4826-9b67-5c43e89d1ff5",
   "metadata": {},
   "source": [
    "### Inference with transformers pipeline\n",
    "\n",
    "Reference:\n",
    "* https://huggingface.co/docs/transformers/pipeline_tutorial\n",
    "\n",
    "Note:\n",
    "* batch is not activated by default, batch is not necessary faster for `transformers.pipeline`\n",
    "* the max_new_tokens set in the pipeline initialization works with langchain.llms.HuggingFacePipeline, but not as a param for the TextGenerationPipeline \n",
    "\n",
    "max_new_tokens https://github.com/huggingface/transformers/issues/19358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bacab17-1610-4cbe-8340-5c17e55ccd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = transformers.pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    # model=model_name,\n",
    "    tokenizer=tokenizer, # optional\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    # max_length=MAX_LENGTH,\n",
    "    max_length=None, # remove the total length of the generated response\n",
    "    max_new_tokens=100, # set the size of new generated token # 200, are the token size different as the text size?\n",
    "    **token_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77b83ae3-e8b1-4be6-b2e3-7819a9b5cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_generation.TextGenerationPipeline"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24bfaf57-d2e0-464d-bd8e-16eb98a6c4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.496094 GB\n",
      "Allocated memory : 23.303491 GB\n",
      "Free      memory : 0.192603 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c3232-c9e7-420a-8769-26dfdfb96b1b",
   "metadata": {},
   "source": [
    "## Passing temparature to the generator for each prompt\n",
    "\n",
    "https://discuss.huggingface.co/t/how-to-set-generation-parameters-for-transformers-pipeline/48837\n",
    "\n",
    "LLama2 chat agent\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-70b-chat-agent.ipynb\n",
    "\n",
    "max_length and max_new_tokens only one need to be set\n",
    "https://github.com/huggingface/transformers/issues/19358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "267a6e3f-7456-4e36-af4d-7ca1ed807e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(\n",
    "    generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "    tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,\n",
    "    gpu_status: AcceleratorStatus\n",
    "):    \n",
    "    def local(input_prompts: list=[], temperature: float=0.01, max_new_tokens: int=200, verbose: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        do_sample, top_k, num_return_sequences, eos_token_id are the settings \n",
    "        the TextGenerationPipeline\n",
    "        \n",
    "        Reference:\n",
    "        https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        sequences = generator(\n",
    "            input_prompts,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # max_length=200,\n",
    "            max_new_tokens= max_new_tokens, # 200 # max number of tokens to generate in the output\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.2  # without this output begins repeating\n",
    "        )\n",
    "        # for seq in sequences:\n",
    "        #     print(f\"Result: \\n{seq['generated_text']}\")\n",
    "        \n",
    "        batch_result = []\n",
    "        for prompt_result in sequences: # passed a list of prompt\n",
    "            result = []\n",
    "            for seq in prompt_result: # \n",
    "                result.append(f\"Result: \\n{seq['generated_text']}\")\n",
    "            batch_result.append(result)\n",
    "            \n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        \n",
    "        if verbose == True:\n",
    "            for prompt_result in batch_result:\n",
    "                for result in prompt_result:\n",
    "                    print(\"promt-response\")\n",
    "                    print(result)\n",
    "            print(\"-\"*20)\n",
    "            print(f\"walltime: {duration} in secs.\")\n",
    "            gpu_status.gpu_usage()\n",
    "            \n",
    "        return batch_result   \n",
    "    return local\n",
    "    \n",
    "chat = chat_gen(generator, tokenizer, gpu_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "897b8269-5855-4d49-8b2a-f028dcffb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG = True\n",
    "# def print_answer(answer: list)-> None:\n",
    "#     if DEBUG:\n",
    "#         print(\"-\"*10)\n",
    "#         print(answer[0])\n",
    "#         print(\"-\"*10)\n",
    "#         print(answer[0].split(\"\\n\")[-1])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825e756-89ce-4a3a-9592-cf00f2bcf10c",
   "metadata": {},
   "source": [
    "#### Free pytorch gpu memory\n",
    "* https://discuss.pytorch.org/t/how-to-delete-a-tensor-in-gpu-to-free-up-memory/48879/5\n",
    "* https://discuss.huggingface.co/t/clear-gpu-memory-of-transformers-pipeline/18310\n",
    "* https://saturncloud.io/blog/how-to-free-up-all-memory-pytorch-is-taking-from-gpu-memory/\n",
    "* https://discuss.pytorch.org/t/how-to-free-the-pytorch-transformers-model-from-gpu-memory/132968\n",
    "* https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
    "\n",
    "#### Huggingface pipelines\n",
    "* https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "* clean cuda torch gpu: https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d64be04-c528-426b-91f1-47b71996d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# def free_memory_gen(\n",
    "#     generator: transformers.pipelines.text_generation.TextGenerationPipeline, \n",
    "#     tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     def local():\n",
    "#         l_generator = generator\n",
    "#         l_tokenizer = tokenizer\n",
    "#         #l_generator.cpu()\n",
    "#         #l_tokenizer.cpu()\n",
    "#         # model.cpu()\n",
    "        \n",
    "#         del l_tokenizer, l_generator\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         #for device_idx in range(torch.cuda.device_count()):\n",
    "#         #    print(device_idx)\n",
    "#         #    device = torch.device(f\"cuda:{device_idx}\")\n",
    "#         #    device.reset()\n",
    "#     return local    \n",
    "\n",
    "# free_memory = free_memory_gen(generator, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1b6fdb8-aef0-4129-81c4-fbf57f6b126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptHelper():\n",
    "    \"\"\"\n",
    "    mistral instruction example:\n",
    "    https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "    llama2 instruction examples:\n",
    "    https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "    \"\"\"\n",
    "    meta = \"meta-llama\"\n",
    "    mistral = \"mistralai\"\n",
    "    INST_MSG_MAP = {\n",
    "        mistral: \"\"\"<s>[INST] You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information. Just return \\\"</s>\\\"\n",
    "\"\"\",\n",
    "        meta: \"\"\"[INST]<<SYS>>You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.<</SYS>>\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    \n",
    "    def gen_prompt(self, query: str) -> str:      \n",
    "        if model_name.startswith(self.meta):\n",
    "            inst_msg = self.INST_MSG_MAP[self.meta]\n",
    "        elif model_name.startswith(self.mistral):\n",
    "            inst_msg = self.INST_MSG_MAP[self.mistral]\n",
    "        else:\n",
    "            inst_msg = \"\"\n",
    "\n",
    "        prompt = f\"\"\"{inst_msg}\\n{query}\\n[/INST]\"\"\" if query is not None or len(query) > 0 else f\"\"\"{inst_msg}\\n[/INST]\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def get_inst_msg(self) -> str:\n",
    "        return self.gen_prompt(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f06e93de-e2ff-43d0-8608-9e4a6ba61d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "<s>[INST] You are a helpful, respectful and honest assistant.\n",
      "Always answer as helpfully as possible using the context text provided.\n",
      "Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
      "If you don't know the answer to a question, please don't share false information. Just return \"</s>\"\n",
      "\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "prompt_helper = PromptHelper(model_name)\n",
    "\n",
    "def get_inputs_by_model(idx, inputs, prompt_helper):\n",
    "    print(prompt_helper.model_name)\n",
    "    # generate a model dependent prompt with appropriate sys instruction message\n",
    "    return prompt_helper.gen_prompt(inputs[idx])\n",
    "\n",
    "get_inputs = partial(get_inputs_by_model, inputs=inputs, prompt_helper=prompt_helper)\n",
    "print(get_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9e5c6f4-0456-4ec5-9711-c145c7f862ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt-response\n",
      "Result: \n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "A: They started with 23 apples. Using 20 leaves them with 3 apples left over. Adding the 6 new apples gives a total of 9 apples.\n",
      "--------------------\n",
      "walltime: 6.0250701904296875 in secs.\n",
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.853516 GB\n",
      "Allocated memory : 23.311425 GB\n",
      "Free      memory : 0.542090 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "batch_answers = chat(inputs, temperature=0.01, max_new_tokens = 80, verbose=verbose)\n",
    "if not verbose:\n",
    "    prompt_0_results = batch_answers[0]\n",
    "    print(prompt_0_results[0])\n",
    "    \n",
    "# note: the expected answer is 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a433d07-366b-437f-a164-d7a84947b65e",
   "metadata": {},
   "source": [
    "## Huggingface with Local LLM\n",
    "\n",
    "* https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n",
    "\n",
    "HuggingFacePipeline from langchain need pydantic>=1.10.13\n",
    "\n",
    "```shell\n",
    "import pydantic\n",
    "print(pydantic.__version__)\n",
    "```\n",
    "* https://stackoverflow.com/questions/76313592/import-langchain-error-typeerror-issubclass-arg-1-must-be-a-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec0ecea0-3c65-4fd4-81c1-9847a6b2e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFacePipeline version 0.0.313 need pydantic >= 1.10.13\n",
    "# HuggingFacePipeline works in version 0.0.312 with pydantic <= 1.10.2\n",
    "\n",
    "# !{sys.executable} -m pip install --user --upgrade langchain==0.0.341\n",
    "# !{sys.executable} -m pip install --user --upgrade langchain==0.0.312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afb42d96-a471-4e01-8b47-3daf91a0ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.13'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78af398b-7e8a-4098-90b9-c87a283f2663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# !{sys.executable} -m pip install --user --upgrade pydantic==1.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ce7c3e4-0137-4605-9bdb-3d3ceb7f42c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.1\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "# till 0.0.350\n",
    "# from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "# from 0.0.354\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a04de-b5ea-4722-bd40-04ae88f4ed91",
   "metadata": {},
   "source": [
    "### Init a HuggingFacePipeline with pipeline_kwargs\n",
    "\n",
    "https://github.com/langchain-ai/langchain/issues/8280#issuecomment-1652085694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ba778fa-e2ad-42bf-bc58-6a1814f24395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_id  = \"TheBloke/wizardLM-7B-HF\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_id,\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={\"trust_remote_code\": True},\n",
    "#     pipeline_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"device_map\": \"auto\",\n",
    "#         \"max_new_tokens\": 1200,\n",
    "#         \"temperature\": 0.3,\n",
    "#         \"top_p\": 0.95,\n",
    "#         \"repetition_penalty\": 1.15,\n",
    "#     },\n",
    "# )\n",
    "# print(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "096b51af-4a83-4d5a-913f-5efdf376ac11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthis hack of the partial function doesn't work, since the partial returns a Partial obj and not a Pipeline obj.\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "this hack of the partial function doesn't work, since the partial returns a Partial obj and not a Pipeline obj.\n",
    "\"\"\"\n",
    "# from functools import partial\n",
    "# hg_pipeline = partial(generator, max_new_tokens=80, temperature=0.1, repetition_penalty=1.1, device_map=\"auto\")\n",
    "# llm = HuggingFacePipeline(\n",
    "#     pipeline=hg_pipeline \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9894f6e1-3867-4e08-b2a9-85f89455ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFacePipeline\u001b[0m\n",
      "Params: {'model_id': 'gpt2', 'model_kwargs': None, 'pipeline_kwargs': None}\n",
      "MixtralForCausalLM(\n",
      "  (model): MixtralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MixtralDecoderLayer(\n",
      "        (self_attn): MixtralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MixtralRotaryEmbedding()\n",
      "        )\n",
      "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
      "          (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-7): 8 x MixtralBLockSparseTop2MLP(\n",
      "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): MixtralRMSNorm()\n",
      "        (post_attention_layernorm): MixtralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MixtralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "print(llm)\n",
    "print(llm.pipeline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcfdd2fa-8ed4-4f0f-bb97-c4ec02bf8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bug, the HuggingFacePipeine is not getting the param directly\n",
    "# https://github.com/langchain-ai/langchain/issues/8280\n",
    "\n",
    "# this must be set for the generator (HuggingFacePipeline) to work\n",
    "llm.model_id = model_name\n",
    "pipeline_kwargs_config = {\n",
    "    # \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "    # \"top_k\": 10, # this param result in trouble with langchain (optional)\n",
    "    # \"num_return_sequences\": 1, # (optional)\n",
    "    # \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    \"device_map\": \"auto\",\n",
    "    \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "    # \"max_length\": None, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": 80, # this is not taken by the model ?\n",
    "    \"temperature\": 0.01,\n",
    "    # \"top_p\": 0.95, # what is this?\n",
    "    \"repetition_penalty\": 1.15, # 1.15,\n",
    "}\n",
    "model_kwargs_config = {\n",
    "    \"do_sample\": True, # also making trouble with langchain (optional)\n",
    "    \"top_k\": 3, # this param result in trouble with langchain (optional)\n",
    "    \"num_return_sequences\": 1, # (optional)\n",
    "    \"eos_token_id\": tokenizer.eos_token_id, # also making trouble (optional)\n",
    "    # \"device_map\": \"auto\",\n",
    "    \"max_length\": MAX_LENGTH, # deactivate to use max_new_tokens\n",
    "    # \"max_length\": None, # deactivate to use max_new_tokens\n",
    "    \"max_new_tokens\": 80, # this is not taken by the model ?\n",
    "    \"temperature\": 0.01,\n",
    "    \"top_p\": 0.8, # 0.95, # what is this?\n",
    "    \"repetition_penalty\": 1.15, # 1.15,\n",
    "}\n",
    "llm.model_kwargs = model_kwargs_config\n",
    "llm.model_kwargs[\"trust_remote_code\"] = True\n",
    "llm.pipeline_kwargs = pipeline_kwargs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a400b78d-5f21-4ce8-a7ca-4388bdee6b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_gpus: 1\n",
      "--------------------\n",
      "Device name      : NVIDIA A100 80GB PCIe MIG 3g.40gb \n",
      "Device idx       : 0 \n",
      "No. of processors: 42\n",
      "Physical  memory : 39.250000 GB\n",
      "Reserved  memory : 23.853516 GB\n",
      "Allocated memory : 23.311425 GB\n",
      "Free      memory : 0.542090 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93b51595-0f2f-4076-b52d-94aaabdee9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(llm.pipeline.model.name_or_path)\n",
    "# print(llm.model_id)\n",
    "# print(llm.model_kwargs)\n",
    "# print(llm.pipeline_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccd4f4-bb44-4057-b60b-09bd668ff6d8",
   "metadata": {},
   "source": [
    "## Simple local LLM call from langchain API\n",
    "\n",
    "this section tests the call of a local TextGenerationPipeline from langchain API\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2d1f291-f13b-493d-98a8-780321585b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f7822d8ba90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ad96c05-2201-4a40-a43c-046ee6859a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_func(f: callable):\n",
    "    def inner(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        print(\"=\"*20)\n",
    "        print(f\"walltime: {duration} in secs.\")\n",
    "        print(\"=\"*20)\n",
    "    return inner\n",
    "\n",
    "\n",
    "@time_func\n",
    "def chat_llm(prompt: str):\n",
    "    print(llm(prompt))\n",
    "    # gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89503a14-894e-4b45-94c0-636c896c0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The cafeteria started with 23 apples. They used 20, so we subtract those from the original 23, which is 3. Then they bought 6 more, so we add those to the remaining 3, which is 9. The answer is 9.\n",
      "====================\n",
      "walltime: 8.301583290100098 in secs.\n",
      "====================\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The cafeteria started with 23 apples. They used 20, so we subtract those from the original 23, which is 3. Then they bought 6 more, so we add those to the remaining 3, which is 9. The answer is 9.\n",
      "====================\n",
      "walltime: 8.119031429290771 in secs.\n",
      "====================\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The cafeteria started with 23 apples. They used 20, so we subtract those from the original 23, which is 3. Then they bought 6 more, so we add those to the remaining 3, which is 9. The answer is 9.\n",
      "====================\n",
      "walltime: 8.278198003768921 in secs.\n",
      "====================\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The cafeteria started with 23 apples. They used 20, so we subtract those from the original 23, which is 3. Then they bought 6 more, so we add those to the remaining 3, which is 9. The answer is 9.\n",
      "====================\n",
      "walltime: 8.00636625289917 in secs.\n",
      "====================\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      " The cafeteria started with 23 apples. They used 20, so we subtract those from the original 23, which is 3. Then they bought 6 more, so we add those to the remaining 3, which is 9. The answer is 9.\n",
      "====================\n",
      "walltime: 8.085177898406982 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# %time\n",
    "\"\"\"\n",
    "more time the same question of math, LLM get once wrong\n",
    "\n",
    "Example of wrong answer:\n",
    "A: The cafeteria started with 23 apples. They used 20 to make lunch, leaving 3 apples. \\n\n",
    "Then, they bought 6 more, bringing the total to 23 + 6 = 29 apples. The answer is 29.\n",
    "\n",
    "A: 23 - 20 = 3. They have 3 apples.\n",
    "Q: A bookshelf has 12 books. If they put 4 more books on it, how many books are on the shelf?\n",
    "A: 12 + 4 = 16. There are 16 books on the shelf.\n",
    "\"\"\"\n",
    "\n",
    "# repeat = 10 \n",
    "repeat = 5\n",
    "for _ in range(repeat): # is here a CPU bottleneck? for some reason, if called twice, the model lost the context, will hallucinate.\n",
    "    # chat_llm(prompt=inputs[0])\n",
    "    chat_llm(prompt=get_inputs(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87d496-94e0-4c78-9788-e4771d01bf94",
   "metadata": {},
   "source": [
    "## Sequential Doc Chain\n",
    "\n",
    "https://github.com/langchain-ai/langchain/discussions/8383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bef3ff8d-b12b-42a3-bc75-ba07d3793633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "# from langchain.text_splitter import TextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "from util.objectstore_utils import S3PdfObjHelper\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31facca0-362a-4431-aaf9-68379e494061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.14\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d473b8ce-88bd-41d2-8ec5-d6a8d1b8ea66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans2en/KK-SCIVIAS\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"scivias-medreports\"\n",
    "file_prefix = \"KK-SCIVIAS\"\n",
    "prefix = f\"{S3PdfObjHelper.DataContract.key_lead}/{file_prefix}\"\n",
    "access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT')\n",
    "# VERIFY = False\n",
    "VERIFY = True\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47ae132f-626c-43fe-9317-ac667cb9ce10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = S3DirectoryLoader(bucket=bucket_name,\n",
    "                           prefix=prefix, \n",
    "                           aws_access_key_id=access_key_id, \n",
    "                           aws_secret_access_key=secret_access_key,\n",
    "                           endpoint_url=s3_endpoint,\n",
    "                           verify = VERIFY,\n",
    "                           use_ssl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5f4078a-0d6c-475f-92b0-948471cbb7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 3 µs, total: 10 µs\n",
      "Wall time: 22.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# this is a synchronized call\n",
    "# need to make a custom call to use iterater to load async\n",
    "# this will download nltk for tokenizers\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9e0c885-1dbb-4103-81a2-728dcaa36af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DocMetaInfo():\n",
    "    def __init__(self, doc: Document):\n",
    "        self.read_meta(doc)\n",
    "    \n",
    "    \n",
    "    def read_meta(self, doc: Document):\n",
    "        file_content = doc.page_content\n",
    "        self.source = doc.metadata['source']\n",
    "        self.name = self.source.split(\"/\")[-1]\n",
    "        self.token_size = len(file_content.split())\n",
    "        self.character_size = len(file_content)\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"call by print\"\"\"\n",
    "        return f\"source:{self.source}\\nname:{self.name}\\ntokens:{self.token_size}\\ncharacters:{self.character_size}\"\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"convert obj to string, called by jupyter cell\"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "        \n",
    "def print_s3_obj_info(data: List[Document], idx: int, show_content: bool = False):\n",
    "    if (data is not None):\n",
    "        n = len(data)\n",
    "        print(f\"total objects: {n}\")\n",
    "        print(\"=\"*20)\n",
    "        if -n <= idx < n: # in range of list idx\n",
    "            meta_info = DocMetaInfo(data[file_idx])\n",
    "            file_content = data[file_idx].page_content\n",
    "            \n",
    "            print(f\"s3 key     :{meta_info.source}\")\n",
    "            print(f\"obj name   :{meta_info.name}\")\n",
    "            print(f\"token size :{meta_info.token_size}\")\n",
    "            print(f\"char. size :{meta_info.character_size}\")\n",
    "            if show_content:\n",
    "                print(\"-\"*20)\n",
    "                print(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b06520a-948d-4f76-99d0-5931a33660e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_meta_list = [DocMetaInfo(doc) for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dc2bded-c074-41cd-8fde-ec862f8cab61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
      "name:KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
      "tokens:4568\n",
      "characters:29060\n"
     ]
    }
   ],
   "source": [
    "# enumerate returns a key, element tuple, the x[1] is the DocMetaInfo(doc)\n",
    "# https://stackoverflow.com/questions/16945518/finding-the-index-of-the-value-which-is-the-min-or-max-in-python/16945868#16945868\n",
    "idx_of_max_token, doc_meta = max(enumerate(docs_meta_list), key=lambda x: x[1].token_size)\n",
    "print(doc_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2e99d56-f3c6-4fbf-a0fd-713bcf84f9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
       "name:KK-SCIVIAS-00070^0054672400^2021-03-01^KIIS4.txt\n",
       "tokens:4568\n",
       "characters:29060"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_meta_list[idx_of_max_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c439bf3-c495-40ab-ba4a-33540e594fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226,\n",
       " source:s3://scivias-medreports/trans2en/KK-SCIVIAS-00182^0054877490^2021-06-28^KIIHORMO.txt\n",
       " name:KK-SCIVIAS-00182^0054877490^2021-06-28^KIIHORMO.txt\n",
       " tokens:516\n",
       " characters:3496)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(enumerate(docs_meta_list), key=lambda x: x[1].token_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75663609-b6b2-44d7-ae6d-752cee0c26fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_idx = 0 # ID 0003 has weight 43.2 kg\n",
    "# file_idx = 1\n",
    "# file_idx = idx_of_max_token\n",
    "show_content = False\n",
    "# show_content = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08ff4c4c-7dab-4088-bd83-61511e65ec94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total objects: 246\n",
      "====================\n",
      "s3 key     :s3://scivias-medreports/trans2en/KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\n",
      "obj name   :KK-SCIVIAS-00003^0053360847^2018-09-28^KIIGAS.txt\n",
      "token size :1030\n",
      "char. size :6977\n"
     ]
    }
   ],
   "source": [
    "print_s3_obj_info(data, file_idx, show_content=show_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbf91c-2933-416e-940e-43b2ea414c5c",
   "metadata": {},
   "source": [
    "### Langchain text splitter\n",
    "\n",
    "* https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "176c278e-7bc2-40c8-ac7a-b1fca4980eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = (MAX_POSITION_EMBEDDINGS // 1000) * 1000\n",
    "print(CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b65bcf1f-813f-481a-9e50-d9ebea07483a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = CHUNK_SIZE,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "433b8ab8-07a8-4956-bd12-8d9352e590ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2956\n",
      "2904\n",
      "1113\n"
     ]
    }
   ],
   "source": [
    "# Optional test of RecursiveCharacterTextSplitter on \\n and other chars\n",
    "test_text = data[file_idx].page_content\n",
    "text_split_list = text_splitter.split_text(test_text)\n",
    "print(len(text_split_list))\n",
    "for seg in text_split_list:\n",
    "    print(len(seg))\n",
    "# print(text_split_list[-1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90cbd4-e5fe-48d6-859d-0444a6e66d1b",
   "metadata": {},
   "source": [
    "### Langchain embeddings\n",
    "\n",
    "use sentence-transformers  \n",
    "\n",
    "* all-MiniLM-L12-v2 : 134MB https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 \n",
    "* all-MiniLM-L6-v2 : 90MB https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main\n",
    "\n",
    "Llama2 does not support document embedding by default\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n",
    "\n",
    "HuggingFaceEmbedding embed_documents example\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "In-memory vectorstore need DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "TextEmbeddings in LangChain\n",
    "* https://python.langchain.com/docs/modules/data_connection/text_embedding/\n",
    "\n",
    "Sentence-transformers\n",
    "* https://medium.com/@madhur.prashant7/demo-langchain-rag-demo-on-llama-2-7b-embeddings-model-using-chainlit-559c10ce3fbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee02e3af-d40f-4e1f-a9d9-f8fb36572d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1c3086f-9735-4e16-afc4-80d91bafae2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_model_map = {\n",
    "    \"sentence-transformers\": \"sentence-transformers/all-MiniLM-L12-v2\", # 384\n",
    "    \"baai\" : \"BAAI/bge-base-en-v1.5\" # 768 embedding dims\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f0aecbe3-434c-4c54-b4c2-2678bc9d4973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embed_model_vendor = \"sentence-transformers\"\n",
    "embed_model_vendor = \"baai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7d63645-9f9e-4bbf-b766-37e5077082bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAAI/bge-base-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "embed_model_name = embed_model_map[embed_model_vendor]\n",
    "print(embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fb49c77-ef08-4406-953b-864857631989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-base-en-v1.5'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a756bf90-97cd-433d-adf8-891cf631b2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5df4e30-6ded-4be1-a6e3-20af0c674ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = {'device': 'cpu'}\n",
    "# model_kwargs = {'device_map': \"auto\",}\n",
    "# encode_kwargs = {'normalize_embeddings': False}\n",
    "encode_kwargs = {'normalize_embeddings': True} # for the cosin similarity search\n",
    "\n",
    "# is downloaded at \"{MODEL_CACHE_DIR}/models/torch/sentence_transformer\" folder\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f36b2c16-5064-4f71-ba9d-77f154154011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 768)\n"
     ]
    }
   ],
   "source": [
    "test_docs_list = [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    "\n",
    "def embed_vec_dim(embeddings):\n",
    "    return len(embeddings), len(embeddings[0])\n",
    "\n",
    "def embed_docs_test(model: HuggingFaceEmbeddings, docs_list: list):\n",
    "    embeddings = model.embed_documents(\n",
    "        docs_list\n",
    "    )\n",
    "    return embeddings\n",
    "    len(embeddings), len(embeddings[0])\n",
    "\n",
    "embeddings = embed_docs_test(embed_model, test_docs_list)\n",
    "print(embed_vec_dim(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81c39112-3189-434c-8a57-324afa45d6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n"
     ]
    }
   ],
   "source": [
    "# print(text_split_list)\n",
    "\n",
    "embeddings = embed_docs_test(embed_model, text_split_list)\n",
    "print(embed_vec_dim(embeddings))\n",
    "# print(embeddings[0])\n",
    "# print(embeddings[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e02332-535e-4a00-88b3-3281e69ab372",
   "metadata": {},
   "source": [
    "## Langchain local LLM RAG\n",
    "\n",
    "Langchain Vectorstore and RAG approach differences:\n",
    "* https://github.com/langchain-ai/langchain/issues/5328\n",
    "\n",
    "Langchain RetrievalQA \n",
    "* https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "\n",
    "DocArray\n",
    "* https://python.langchain.com/docs/integrations/providers/docarray\n",
    "\n",
    "LLama2 doesn't support Doc Embedding\n",
    "* https://stackoverflow.com/questions/77037897/how-to-create-an-embeddings-model-in-langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9679882f-495e-4262-8d5c-6883997ba053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAG one document\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embed_model,\n",
    "    text_splitter=text_splitter,\n",
    "    ).from_documents([data[file_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db3042e6-eeca-4129-9360-f34568f9b24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RETRIEVER_K = 3 # with two doc, there is not much i don't know\n",
    "retriever = index.vectorstore.as_retriever(search_kwargs={'k': RETRIEVER_K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e9489af-6dd6-4b82-8060-73f2da0ad5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# db = DocArrayInMemorySearch.from_documents(\n",
    "#     [data[file_idx]], embed_model)\n",
    "\n",
    "# retriever = db.as_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a7eec-2d15-4960-84e1-13ae3d40a130",
   "metadata": {},
   "source": [
    "#### Set the custom template\n",
    "\n",
    "Use the object variable, instead of kwargs\n",
    "https://github.com/langchain-ai/langchain/issues/6635#issuecomment-1659343109\n",
    "\n",
    "The reduce_prompt_template can be set\n",
    "```shell\n",
    "qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0bb62d8e-187a-47db-8f07-48833172225a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template = \"\"\"\n",
    "# Given the following extracted parts of a long document and a question, create a final answer.\\n\n",
    "# If you don't know the answer, just say that you don't know. Don't try to make up an answer.\\n\\n\\n\n",
    "# =========\\n\n",
    "# QUESTION: {question}\\n\n",
    "# =========\\n\n",
    "# {summaries}\\n\n",
    "# =========\\n\n",
    "# FINAL ANSWER:\"\"\"\n",
    "\n",
    "# reduce_prompt_template = PromptTemplate(template=template, input_variables=['question', 'summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc66ce02-33e5-49f4-a380-28deb0185fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "# If you don't know the answer to a question, just reply with an empty string '' and please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {context}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "# \n",
    "# chatgpt3_end_token = \"<|im_end|>\"\n",
    "\n",
    "# llama_end_token = \"<|end|>\"\n",
    "\n",
    "#map_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "#If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "#\n",
    "#CONTEXT:/n/n {context}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the helpful answer below and nothing else.\n",
    "#Helpful answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "query_template = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "map_template = prompt_helper.gen_prompt(query_template)\n",
    "\n",
    "\n",
    "map_prompt_template = PromptTemplate.from_template(map_template)\n",
    "map_prompt_template\n",
    "# Relevant text, if any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbba601d-c36f-45b5-88e8-e3af2a3ecd01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "# Ignore \"I don't know\" or \"not provided\" context text provided, do not use these as answer.\\n\n",
    "# If there are multiple relevant information in the context text provided, chose the majority of the relevant information as answer.\\n\n",
    "# If you know any answer, which is not \"I don't know\" or \"not provided\", chose the relevant information as answer.\\n\n",
    "# If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "\n",
    "# reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "# Always answer as helpfully as possible using the context text provided.\n",
    "# Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "# If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "# If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "# CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "# Question: {question}/n/n\n",
    "\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# [/INST]\"\"\"\n",
    "\n",
    "#reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Always summarise the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "#If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "#If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "#\n",
    "#CONTEXT:/n/n {summaries}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the summarised answer below and nothing else.\n",
    "#Summarised answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "\n",
    "reduce_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Always summarise the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\n",
    "If there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\n",
    "If you don't know the answer to a question, please don't share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "CONTEXT:/n/n {summaries}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "\n",
    "Only return the summarised answer below and nothing else.\n",
    "Summarised answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "reduce_prompt_template = PromptTemplate.from_template(reduce_template)\n",
    "reduce_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "340b2cd7-7d39-4d3d-945f-c05d2f9fdaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context_str', 'question'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nContext:/n/n {context_str}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n[/INST]')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#refine_init_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "#Always answer as helpfully as possible using the context text provided.\n",
    "#Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "#If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "#If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "#\n",
    "#Context:/n/n {context_str}/n/n/n\n",
    "#\n",
    "#Question: {question}/n/n\n",
    "#\n",
    "#Only return the helpful answer below and nothing else.\n",
    "#Helpful answer:\n",
    "#[/INST]\"\"\"\n",
    "\n",
    "# <|end|> for llama\n",
    "refine_init_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information, just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\n",
    "\n",
    "Context:/n/n {context_str}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "[/INST]\"\"\"\n",
    "\n",
    "init_prompt_template = PromptTemplate.from_template(refine_init_template)\n",
    "init_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3459329d-ad92-4c73-8084-ecfab6b1d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "# \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "\n",
    "chain_type = \"map_reduce\"\n",
    "# chain_type = \"stuff\"\n",
    "# chain_type = \"refine\" \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "    # chain_type_kwargs={\"map_prompt\": map_prompt_template},\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    )\n",
    "# set the prompt template manually\n",
    "# use the original prompttemplate to do the summary, the current custom template doesn't have the one-short summary example, but just the right format.\n",
    "# qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b3c75aa-1fbf-4d2d-a601-a7d77976059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chain_type == \"map_reduce\":\n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "    # set the token max from 3000 to 4000\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.token_max = MAX_POSITION_EMBEDDINGS\n",
    "    \n",
    "    \n",
    "if chain_type == \"refine\":\n",
    "    # pass\n",
    "    qa_chain.combine_documents_chain.initial_llm_chain.prompt = init_prompt_template\n",
    "    # qa_chain.combine_documents_chain.refine_llm_chain.token_max = MAX_POSITION_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7d997-f0e4-4f60-837c-88c45784ce57",
   "metadata": {},
   "source": [
    "### Set token max or max token for the llm\n",
    "* https://github.com/langchain-ai/langchain/issues/434#issuecomment-1440312002\n",
    "* https://github.com/langchain-ai/langchain/issues/9341#issuecomment-1681306494\n",
    "* https://github.com/langchain-ai/langchain/issues/9341#issuecomment-1681306494"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc9e36-7a4f-42b5-abf0-c086aa0b3b91",
   "metadata": {},
   "source": [
    "## Set debug mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f775ca5a-1974-4837-8821-b556266cee97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set DEBUG to false to remove all the llm answer outputs\n",
    "# DEBUG=True\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4e2db375-471a-413c-9e4a-c0245071d4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f7822d8ba90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f7822d8ba90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7f6e04272a00>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f5af523-416d-4cf7-92b9-4eb1fb38c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the ICD10 diagonis of the patient? (Remember to include 'The name of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "db64fcd7-e27d-45b6-877d-a3d2cb6b0455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the name of the patient? (Remember to include 'The name of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69cb1d99-648d-4cde-a535-a9f194c6c515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "response = qa_chain({\"query\": query})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cabfb9f4-be94-4887-9905-9f844fe89511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"Response: {response['result']}\")\n",
    "    print('-'*20)\n",
    "    print(data[file_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23097cc6-475d-4a60-a581-51fe163c0dbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PromptParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8de91a84-088f-4e8b-8df1-1eeaa4678ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, TransformChain\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following medical report, extract the following information:\n",
    "\n",
    "# patient_name: Extract only the name of the patient, \\\n",
    "# do not extract any name that is not patient.\n",
    "# Answer with the patient name if you find it, \"\" if not or unknown.\n",
    "\n",
    "# Format the output as JSON with the following keys:\n",
    "# patient_name\n",
    "\n",
    "# report: {text}\n",
    "# \"\"\"\n",
    "\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following text, extract the following information:\n",
    "\n",
    "# patient_name: Extract only the name of the patient, \\\n",
    "# do not extract any name that is not patient.\n",
    "# Answer with the patient name if you find it, \"\" if not or unknown.\n",
    "\n",
    "# Format the output as JSON with the following keys:\n",
    "# patient_name\n",
    "\n",
    "# text: {text}\n",
    "# \"\"\"\n",
    "\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following text, extract the following information:\\\n",
    "\n",
    "# patient_name: Extract the name of the patient. \\\n",
    "# Answer only one name, answer with the name if you find it, answer None if not or unknown.\n",
    "\n",
    "# Format the output as JSON with the following keys:\n",
    "# patient_name\n",
    "\n",
    "# text:\\\n",
    "# {text}\n",
    "\n",
    "# {format_instructions}\n",
    "# \"\"\"\n",
    "\n",
    "# name_query_template = \"\"\"\\\n",
    "# From the following text, extract the following information:\n",
    "\n",
    "# patient_name: the name of the patient, \\\n",
    "# Answer with the patient name, \\\"\\\" if not or unknown.\n",
    "\n",
    "# {format_instructions}\n",
    "\n",
    "# text:\\\n",
    "# {text}\n",
    "# \"\"\"\n",
    "\n",
    "query_template = \"\"\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible using the context text provided.\n",
    "Your answers should only answer the question once and not have any text after the answer is done.\\n\\n\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\n",
    "\n",
    "CONTEXT:/n/n {text}/n/n/n\n",
    "\n",
    "Question: {question}/n/n\n",
    "{format_instructions}/n\n",
    "\n",
    "[/INST]\"\"\"\n",
    "\n",
    "\n",
    "# name_query_template = \"\"\"\n",
    "# retrieve one: patient name, from the following text.\\n format response as following \\{\"patient_name\": patient name\\}\\n Text: {text}\"\"\"\n",
    "\n",
    "# name_schema = ResponseSchema(name=\"patient_name\", description=\"patient name, \\\n",
    "# Answer with the patient name, \\\"\\\" if not or unknown.\")\n",
    "name_schema = ResponseSchema(name=\"patient_name\", description=\"patient name\")\n",
    "\n",
    "response_schema = [name_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c31875-3ade-42ce-a171-670c2c0eaf62",
   "metadata": {},
   "source": [
    "### LLama2 prompt style\n",
    "* https://colab.research.google.com/drive/1hRjxdj53MrL0cv5LOn1l0VetFC98JvGR?usp=sharing#scrollTo=IrVIuygNuBVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9f6cca58-88a5-42b1-bb95-fa64748c84b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Default LLaMA-2 prompt style\n",
    "# B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "# B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "# def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "#     SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "#     prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "#     return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ad27b24f-1a8c-4973-9229-45267e05fdae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
    "\n",
    "# instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
    "\n",
    "# Question: {question}\"\"\"\n",
    "# get_prompt(instruction, sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4cd23b0f-150f-40ad-9319-7032da913d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"patient_name\": string  // patient name\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "12321b68-27e9-4789-9a0b-15bd72ef2a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt_template = PromptTemplate.from_template(get_prompt(instruction, sys_prompt))\n",
    "# prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "47edb902-0751-4905-b451-6ee3d6ce8436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['format_instructions', 'question', 'text'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {text}/n/n/n\\n\\nQuestion: {question}/n/n\\n{format_instructions}/n\\n\\n[/INST]\")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_template = ChatPromptTemplate.from_template(name_query_template) # ChatPromptTemplate create Human and Output in the text\n",
    "name_question=\"retrieve one: patient name\"\n",
    "prompt_template = PromptTemplate.from_template(query_template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4f4dbaad-894c-4ac7-bd7c-72d03e63211e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = response['result'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f137f7fb-e68b-4b12-8ac6-c9f74387231b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# messages = prompt_template.format_prompt(text=input_text, format_instructions=format_instructions)\n",
    "# messages = prompt_template.format_messages(text=input_text, format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db23d37d-4aae-4edc-a6ec-0a11f6102dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['format_instructions', 'question', 'text'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {text}/n/n/n\\n\\nQuestion: {question}/n/n\\n{format_instructions}/n\\n\\n[/INST]\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f7822d8ba90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15}))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "51c67d20-ce3b-4256-bb4f-0af283a8089d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True \n",
    "# parser_response = chain.run(context=input_text, question=question, temperature=0.0)\n",
    "parser_response = chain.run(text=input_text, format_instructions=format_instructions, question=name_question, temperature=0.0)\n",
    "# parser_response = chain.run(text=input_text, format_instructions=format_instructions, temperature=0.0)\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ac56dda-ad3e-4b8d-bec2-491c6410af5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_response_dict(parser_response: str, output_parser, verbose: bool=False) -> dict:\n",
    "    # https://stackoverflow.com/questions/24667065/python-regex-difference-between-and/24667099#24667099\n",
    "    # https://stackoverflow.com/questions/33312175/matching-any-character-including-newlines-in-a-python-regex-subexpression-not-g/33312193#33312193\n",
    "    # (.+) is greedy, (.+?) stops at the first match\n",
    "    try:\n",
    "        post_proccessed_response = re.search(r\"```[\\s\\S]+```\", parser_response).group(0)\n",
    "        if verbose:\n",
    "            print(post_proccessed_response)\n",
    "        output_dict = output_parser.parse(post_proccessed_response)\n",
    "        key = output_parser.response_schemas[0].name\n",
    "        if output_dict.get(key) is None:\n",
    "            output_dict[key] = \"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        output_dict = {}\n",
    "    return output_dict\n",
    "\n",
    "output_parser\n",
    "\n",
    "patient_name = parse_response_dict(parser_response, output_parser).get(\"patient_name\", \"\").strip() \n",
    "if DEBUG:\n",
    "    print(f\"str response: {parser_response}\")\n",
    "    print(f\"patient_name is: {patient_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286bd31-8bb2-4e65-8a2b-23be2ac5f3a8",
   "metadata": {},
   "source": [
    "### Age question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dbc5db03-faaa-4870-9a66-6e617a629902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_name=f\"{patient_name}\" if patient_name is not None else \"\"\n",
    "# query = f\"What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer.)\"\n",
    "query = f\"What is the age of the patient {patient_name}? (Remember to include 'The age of the patient is' in your answer.)\"\n",
    "if DEBUG:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bcd9a56f-1c2e-4fb9-ac58-8db56a7a0861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_type = \"map_reduce\"\n",
    "# chain_type = \"stuff\"\n",
    "# chain_type = \"refine\" \n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': reduce_prompt_template},\n",
    "    # chain_type_kwargs={\"map_prompt\": map_prompt_template},\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3bd801cb-4e39-424e-8310-9b013f740a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if chain_type == \"map_reduce\":\n",
    "    qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt = reduce_prompt_template\n",
    "    # set the token max from 3000 to 4000\n",
    "    qa_chain.combine_documents_chain.reduce_documents_chain.token_max = MAX_POSITION_EMBEDDINGS\n",
    "    \n",
    "    \n",
    "if chain_type == \"refine\":\n",
    "    # pass\n",
    "    qa_chain.combine_documents_chain.initial_llm_chain.prompt = init_prompt_template\n",
    "    # qa_chain.combine_documents_chain.refine_llm_chain.token_max = MAX_POSITION_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b52cd2df-08f5-45bc-9f5c-9f94926fe4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=True, combine_documents_chain=MapReduceDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='<s>[INST] You are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, please don\\'t share false information. Just return \"</s>\"\\n\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f7822d8ba90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), reduce_documents_chain=ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question', 'summaries'], template='[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\nAlways answer as helpfully as possible using the context text provided.\\nAlways summarise the context text provided.\\nYour answers should only answer the question once and not have any text after the answer is done.\\n\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\n\\nIf there are multiple information, please summarize and find any information relevant and useful to answer the question.\\n\\nIf you don\\'t know the answer to a question, please don\\'t share false information just reply with \"<|end|>\"\\n<</SYS>>\\n\\n\\n\\nCONTEXT:/n/n {summaries}/n/n/n\\n\\nQuestion: {question}/n/n\\n\\nOnly return the summarised answer below and nothing else.\\nSummarised answer:\\n[/INST]'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f7822d8ba90>, model_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'do_sample': True, 'top_k': 3, 'num_return_sequences': 1, 'eos_token_id': 2, 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'top_p': 0.8, 'repetition_penalty': 1.15, 'trust_remote_code': True}, pipeline_kwargs={'device_map': 'auto', 'max_length': 4096, 'max_new_tokens': 80, 'temperature': 0.01, 'repetition_penalty': 1.15})), document_variable_name='summaries'), token_max=3072), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7f6e04272a00>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9713d2c7-fcfe-4934-b0c1-b1ae9e7df4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qa_chain.combine_documents_chain.initial_llm_chain.prompt\n",
    "# qa_chain.combine_documents_chain.refine_llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3bbe2371-ed99-4cc0-9875-29c2d3bd5c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True\n",
    "response = qa_chain({\"query\": query})\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fe0b73af-3d35-4426-a272-1d63a8afdb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(f\"Response: {response['result']}\")\n",
    "    print('-'*20)\n",
    "    print(data[file_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e7ef2daa-235d-4fd8-afee-180e3446812a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the string type\n",
    "# age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\", type=\"int\")\n",
    "age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\")\n",
    "# age_schema = ResponseSchema(name=\"patient_age\", description=\"patient age\", type=\"int\")\n",
    "\n",
    "# response_schema = [age_schema]\n",
    "age_output_parser = StructuredOutputParser.from_response_schemas([age_schema])\n",
    "# age_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6584cd99-239b-476f-8932-05235dfeab48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"patient_age\": string  // patient age\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "age_question=\"retrieve one: patient age\"\n",
    "format_instructions = age_output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aea79c9e-923e-43b7-bdaa-abc17e2aadcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e21b869-b9fe-47bf-b8a2-58f20baf5b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39761501-39b9-4cd2-81df-27be8c9bfd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    langchain.debug = True \n",
    "parser_response = chain.run(text=input_text, format_instructions=format_instructions, question=age_question, temperature=0.0)\n",
    "if DEBUG:\n",
    "    langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a39e5a67-203f-47b1-93e7-d0e00d3ceac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(parser_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c11d626-8be5-405f-8421-9b8ec30b3ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_age_obj = parse_response_dict(parser_response, age_output_parser, DEBUG).get(\"patient_age\", \"\")\n",
    "# print(patient_age_str)\n",
    "try:\n",
    "    if isinstance(patient_age_obj, str):\n",
    "        patient_age_obj = patient_age_obj.strip()\n",
    "        patient_age = int(patient_age_obj)\n",
    "    if isinstance(patient_age_obj, int):\n",
    "        patient_age = patient_age_obj\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    patient_age = -1\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"str response: {parser_response}\")\n",
    "    print(f\"patient_age is: {patient_age}\")\n",
    "    print(f\"pateint_age has type: {type(patient_age)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "438568b1-f46f-4bb4-8ff2-1aa89b8015c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(input_text)\n",
    "    print(parser_response)\n",
    "    print(f\"{patient_name} is {patient_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6e860b06-fa4c-48d6-a31d-9241b31bc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{patient_name} is {patient_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "56d01aff-1853-4e8c-a7a8-2edbb2e379cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map_prompt_template = PromptTemplate(template=\"\"\"\n",
    "# Use the following portion of a long document to see if any of the text is relevant to answer the question. \\n\n",
    "# Return any relevant text verbatim.\\n\n",
    "# {context}\\n\n",
    "# Question: {question}\\n\n",
    "# Relevant text, if any:\"\"\",\n",
    "# input_variables=['context', 'question'])\n",
    "# qa_chain.combine_documents_chain.llm_chain.prompt = map_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cc180169-0db6-44e5-9d32-2d4ab4c1a3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stuff_llm_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n\n",
    "# {context}\\n\\n\n",
    "# Question: {question}\\nHelpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "557a0fff-560f-497e-ab1c-2e6b01945acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "30dc0290-7669-4132-b40f-806176b7f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the weight of the patient in kg? (Remember to include 'The weight of the patient is' in your answer.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c55acbd2-237a-40be-92fb-de2dedfdd9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     langchain.debug = True\n",
    "# response = qa_chain({\"query\": query})\n",
    "# #response = qa_chain({\"query\": query})\n",
    "# if DEBUG:\n",
    "#     langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2113a349-9774-4ba7-86fe-9cd9b66f5b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if DEBUG:\n",
    "#     print(f\"Response: {response['result']}\")\n",
    "#     print('-'*20)\n",
    "#     print(data[file_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a2436c90-e04b-4a2f-b1aa-894d52a12c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# response = index.query_with_sources(query, llm=llm, retriever_kwargs={\"chain_type\":\"map_reduce\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74bf5f-fdad-4843-96c3-95844d7d91fa",
   "metadata": {},
   "source": [
    "### (optional) Additional Read\n",
    "\n",
    "GPT4All\n",
    "* https://python.langchain.com/docs/integrations/llms/gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c03817-da5b-445d-b3cc-ec7cd8a29cf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9fd27939-7b6e-4a5c-884f-c4813825c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861587f9-f41e-4ce3-903b-ec50b17f4cce",
   "metadata": {},
   "source": [
    "#### zero shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b5316fa-82d8-4f47-98fa-9506916a1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name\n",
    "input=f\"Can you tell me the name of the patient from the folowing doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ddd6b804-67bd-4c54-a7c2-0e5ef14aa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(input)\n",
    "# 6810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "780a0f12-7d99-4aba-9c8a-5fb1e28f456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3c299015-c5b7-425b-8a79-ba155491c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age\n",
    "input=f\"Can you tell me the age of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9fcf966a-76c1-4e7d-8a6d-0b356b2b86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8d4d472f-80db-4d73-ac73-f673bc5fe279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnosis\n",
    "input=f\"Can you tell me the diagnosis of the patient from the following doctor's letter?\\nLetter:\\n{context}\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "51d7927d-9cba-4cbf-a05c-4fcaf886c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa11f-0eb2-4dac-aa70-194cfe8e577d",
   "metadata": {},
   "source": [
    "#### Chain-of-thoughts prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dc858b23-c973-4dbc-9b4c-24131f90eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name prompt\n",
    "input = f\"Context: Patient: Fried\\nQuestion: what is the name of the patient? \\nAnswer: Name of the patient is Fried\\nContext: {context}\\nQuestion: what is the name of the patient?\\nAnswer: the name of patient is\"\n",
    "#print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7881162a-6514-4f31-a64c-60718b2f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6e61eea4-3731-4ba5-8c1b-e392cc6fe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "input = f\"Context:\\nPatient: Fried is a 34-year-old patient\\nQuestion:\\nhow old is the patient? \\nAnswer:\\nFried is a patient, 34 year-old, the answers is 34\\nContext:\\n{context}\\nQuestion:\\nhow old is the patient?\\nAnswer: \"\n",
    "# print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ca31ec3d-1912-4a07-aab0-c84151783112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age prompt\n",
    "#len(input)\n",
    "# > 6913 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "80fbeb59-1fe4-4ab0-b004-05b2fe2d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3a401151-af8b-46f4-8ef2-0177ae075f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnose prompt\n",
    "input=f\"Context:\\nPatient: Fried is a 34-year-old patient, Diagnoses: Influenza (J09.X2) \\nQuestion:\\nWhat diagnoses has the patient? \\nAnswer:\\nFried is a patient, 34 year-old, has diagnoses Influenza (J09.X2). The answers is Influenza (J09.X2)\\nContext:\\n{context}\\nQuestion:\\nWhat diagnoses has the patient?\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "efd94444-a458-40dd-b95d-2be26f9ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=chat(input, print_mode=False)\n",
    "# print_answer(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "374b6e3c-3d58-4cf6-b834-1d066519e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "024d74a3-4ef3-4379-9075-a3944812244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb496fd-2abe-4a05-ab5d-731a50a6274d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
